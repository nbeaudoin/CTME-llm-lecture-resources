{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nbeaudoin/CTME-llm-lecture-resources/blob/main/01_getting_started_with_llms_nbeaudoin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HDOGmN6OmF0"
      },
      "source": [
        "# Making our first LLM API call\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZDWgZfZQGcU"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JYQyB8QUIhOm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b22bee1d-4ec7-45d8-e169-9f7de09a78ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. Instal ling dependencies...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.8/389.8 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.2/320.2 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m339.7/339.7 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.4/340.4 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.8/195.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.6/442.6 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import userdata\n",
        "    print(\"Colab notebook detected. Instal ling dependencies...\")\n",
        "    !pip install -Uqqqq \\\n",
        "        openai \\\n",
        "        chromadb \\\n",
        "        sentence-transformers \\\n",
        "        llama-index \\\n",
        "        llama-index-llms-openai \\\n",
        "        gradio \\\n",
        "        datasets \\\n",
        "        dspy-ai\n",
        "\n",
        "except:\n",
        "    print(\"Not in Colab. Skipping installation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z7bXGANBPZk-"
      },
      "outputs": [],
      "source": [
        "# Basic imports\n",
        "from rich import print\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report, cohen_kappa_score, mean_absolute_error\n",
        "\n",
        "# OpenAI\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfqCJ9bmLDG2"
      },
      "source": [
        "## Setting our `OPENAI_API_KEY` environment variable\n",
        "\n",
        "When we use any LLM provider like OpenAI, Anthropic, or Google, we need some way to tell them who we are making the request.\n",
        "Today, we'll be using OpenAI.\n",
        "The most straightforward way to provide this key is through the environment variable `OPENAI_API_KEY`.\n",
        "The OpenAI python client looks for this environment variable to use in authentication.\n",
        "\n",
        "In the cell below, we load it from the Google Colab secrets manager on the left 👈.\n",
        "Before runnign this code, make sure your API key is set as shown below:\n",
        "\n",
        "![](https://github.com/mgfrantz/CTME-llm-lecture-resources/blob/main/images/colabSecrets.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "N2nPVwzrIpPQ"
      },
      "outputs": [],
      "source": [
        "# Set the OPENAI_API_KEY environment variable\n",
        "try:\n",
        "    from google.colab import userdata # import the environment variables from secrets\n",
        "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY') # Set the OPENAI_API_KEY environmnet variable\n",
        "except:\n",
        "    import dotenv\n",
        "    env_loaded = dotenv.load_dotenv('../.env')\n",
        "    if not env_loaded:\n",
        "        raise ValueError(\"Failed to load environment variables from .env file.\")\n",
        "    else:\n",
        "        print(\"Loaded environment variables from .env file.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-_7WDTwPyZo"
      },
      "source": [
        "## Under the hood: `curl`\n",
        "\n",
        "Almost all of the interactions we will have with LLMs are through API calls.\n",
        "Below is one of the most low-level ways we can call an LLM, using the `curl` command.\n",
        "This command gives us a lot of information about how the API request is structured.\n",
        "We pass a JSON with an authorization header containing our `OPENAI_API_KEY`.\n",
        "We also pass the model we want to call, the chat messages, and hyperparameters such as `temperature` that help control how text is generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "s47r5FtcOyLG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23e29633-dd29-40be-9bfe-077d64b22694"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   901    0   765  100   136   1297    230 --:--:-- --:--:-- --:--:--  1527\n",
            "{\n",
            "    \"id\": \"chatcmpl-AchZbpIOixL9mRhQ84VHmmfESMfRu\",\n",
            "    \"object\": \"chat.completion\",\n",
            "    \"created\": 1733787323,\n",
            "    \"model\": \"gpt-4o-mini-2024-07-18\",\n",
            "    \"choices\": [\n",
            "        {\n",
            "            \"index\": 0,\n",
            "            \"message\": {\n",
            "                \"role\": \"assistant\",\n",
            "                \"content\": \"This is a test!\",\n",
            "                \"refusal\": null\n",
            "            },\n",
            "            \"logprobs\": null,\n",
            "            \"finish_reason\": \"stop\"\n",
            "        }\n",
            "    ],\n",
            "    \"usage\": {\n",
            "        \"prompt_tokens\": 14,\n",
            "        \"completion_tokens\": 5,\n",
            "        \"total_tokens\": 19,\n",
            "        \"prompt_tokens_details\": {\n",
            "            \"cached_tokens\": 0,\n",
            "            \"audio_tokens\": 0\n",
            "        },\n",
            "        \"completion_tokens_details\": {\n",
            "            \"reasoning_tokens\": 0,\n",
            "            \"audio_tokens\": 0,\n",
            "            \"accepted_prediction_tokens\": 0,\n",
            "            \"rejected_prediction_tokens\": 0\n",
            "        }\n",
            "    },\n",
            "    \"system_fingerprint\": \"fp_bba3c8e70b\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Make the API call to OpenAI and store the response in test.json\n",
        "!curl https://api.openai.com/v1/chat/completions \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "  -d '{ \\\n",
        "     \"model\": \"gpt-4o-mini\", \\\n",
        "     \"messages\": [{\"role\": \"user\", \"content\": \"Say: This is a test!\"}], \\\n",
        "     \"temperature\": 0.7 \\\n",
        "    }' > test.json\n",
        "# Show the output of test.json formatted nicely\n",
        "!cat test.json | python -m json.tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPRgn2_IP3w6"
      },
      "source": [
        "## Using the OpenAI Python client\n",
        "\n",
        "While the `curl` command shows us how the API call is made, it's not something that we can easily use in more complex applications.\n",
        "One thing we can use is the OpenAI python client.\n",
        "We can do the exact same thing, but the API call is a bit more abstracted from the developer.\n",
        "Let's see how to perform the exact same API call using the OpenAI clinet 👇:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fPRvearuIz86"
      },
      "outputs": [],
      "source": [
        "client = OpenAI() # Create the OpenAI client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vYbGuKC8I3na"
      },
      "outputs": [],
      "source": [
        "# Create the messages (same as above)\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Say: This is a test!\"},\n",
        "]\n",
        "\n",
        "# Make the API call\n",
        "chat_completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages,\n",
        "    # stream=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2gd5AitgI9kU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "1dff02c8-17a0-4061-bfe3-31af2b8346b2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mChatCompletion\u001b[0m\u001b[1m(\u001b[0m\n",
              "    \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-AchZbtzlljVSMCjZqcfgt1uGqTJX5'\u001b[0m,\n",
              "    \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\n",
              "        \u001b[1;35mChoice\u001b[0m\u001b[1m(\u001b[0m\n",
              "            \u001b[33mfinish_reason\u001b[0m=\u001b[32m'stop'\u001b[0m,\n",
              "            \u001b[33mindex\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
              "            \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "            \u001b[33mmessage\u001b[0m=\u001b[1;35mChatCompletionMessage\u001b[0m\u001b[1m(\u001b[0m\n",
              "                \u001b[33mcontent\u001b[0m=\u001b[32m'This is a test!'\u001b[0m,\n",
              "                \u001b[33mrefusal\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "                \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
              "                \u001b[33maudio\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "                \u001b[33mfunction_call\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "                \u001b[33mtool_calls\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
              "            \u001b[1m)\u001b[0m\n",
              "        \u001b[1m)\u001b[0m\n",
              "    \u001b[1m]\u001b[0m,\n",
              "    \u001b[33mcreated\u001b[0m=\u001b[1;36m1733787323\u001b[0m,\n",
              "    \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini-2024-07-18'\u001b[0m,\n",
              "    \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,\n",
              "    \u001b[33mservice_tier\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33msystem_fingerprint\u001b[0m=\u001b[32m'fp_818c284075'\u001b[0m,\n",
              "    \u001b[33musage\u001b[0m=\u001b[1;35mCompletionUsage\u001b[0m\u001b[1m(\u001b[0m\n",
              "        \u001b[33mcompletion_tokens\u001b[0m=\u001b[1;36m5\u001b[0m,\n",
              "        \u001b[33mprompt_tokens\u001b[0m=\u001b[1;36m14\u001b[0m,\n",
              "        \u001b[33mtotal_tokens\u001b[0m=\u001b[1;36m19\u001b[0m,\n",
              "        \u001b[33mcompletion_tokens_details\u001b[0m=\u001b[1;35mCompletionTokensDetails\u001b[0m\u001b[1m(\u001b[0m\n",
              "            \u001b[33maccepted_prediction_tokens\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
              "            \u001b[33maudio_tokens\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
              "            \u001b[33mreasoning_tokens\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
              "            \u001b[33mrejected_prediction_tokens\u001b[0m=\u001b[1;36m0\u001b[0m\n",
              "        \u001b[1m)\u001b[0m,\n",
              "        \u001b[33mprompt_tokens_details\u001b[0m=\u001b[1;35mPromptTokensDetails\u001b[0m\u001b[1m(\u001b[0m\u001b[33maudio_tokens\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mcached_tokens\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\n",
              "    \u001b[1m)\u001b[0m\n",
              "\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletion</span><span style=\"font-weight: bold\">(</span>\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-AchZbtzlljVSMCjZqcfgt1uGqTJX5'</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
              "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Choice</span><span style=\"font-weight: bold\">(</span>\n",
              "            <span style=\"color: #808000; text-decoration-color: #808000\">finish_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
              "            <span style=\"color: #808000; text-decoration-color: #808000\">index</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
              "            <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "            <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionMessage</span><span style=\"font-weight: bold\">(</span>\n",
              "                <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'This is a test!'</span>,\n",
              "                <span style=\"color: #808000; text-decoration-color: #808000\">refusal</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "                <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
              "                <span style=\"color: #808000; text-decoration-color: #808000\">audio</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "                <span style=\"color: #808000; text-decoration-color: #808000\">function_call</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "                <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
              "            <span style=\"font-weight: bold\">)</span>\n",
              "        <span style=\"font-weight: bold\">)</span>\n",
              "    <span style=\"font-weight: bold\">]</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1733787323</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini-2024-07-18'</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">service_tier</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'fp_818c284075'</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionUsage</span><span style=\"font-weight: bold\">(</span>\n",
              "        <span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,\n",
              "        <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>,\n",
              "        <span style=\"color: #808000; text-decoration-color: #808000\">total_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>,\n",
              "        <span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionTokensDetails</span><span style=\"font-weight: bold\">(</span>\n",
              "            <span style=\"color: #808000; text-decoration-color: #808000\">accepted_prediction_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
              "            <span style=\"color: #808000; text-decoration-color: #808000\">audio_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
              "            <span style=\"color: #808000; text-decoration-color: #808000\">reasoning_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
              "            <span style=\"color: #808000; text-decoration-color: #808000\">rejected_prediction_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
              "        <span style=\"font-weight: bold\">)</span>,\n",
              "        <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTokensDetails</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">audio_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">cached_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>\n",
              "    <span style=\"font-weight: bold\">)</span>\n",
              "<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Display the output\n",
        "print(chat_completion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ0qYsS0Y4DZ"
      },
      "source": [
        "# Build our first chatbots\n",
        "\n",
        "In this section, we'll go through several demos.\n",
        "By the end of this section, you should be able to:\n",
        "- Build a basic chatbot with the popular `gradio` Python library\n",
        "- Understand key hyperparameters like `temperature`, `top_p`, and `top_k`\n",
        "- Build an advanced chatbot with hyperparameter controls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTQ-yAmFQVOM"
      },
      "source": [
        "## Exercise: Building a basic chatbot with `gradio`\n",
        "\n",
        "User interfaces (UIs) are a great way to demo work in AI.\n",
        "In the next several lessons, we will be using the `gradio` framework to demonstrate our growing skillset.\n",
        "In this exercise, we will get a gentle introduction to creating chatbots with `gradio`.\n",
        "\n",
        "Please follow the [ChatInterface](https://www.gradio.app/docs/gradio/chatinterface) documentation and the [Creating a chatbot fast](https://www.gradio.app/guides/creating-a-chatbot-fast) guide to make your first AI chatbot.\n",
        "Your chatbot must:\n",
        "- respond to messages\n",
        "\n",
        "If this too easy, try to:\n",
        "- add a system prompt\n",
        "- use `stream=True` in your chat function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zy5lPNy9R7ur",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ff9bc4d-1f35-40c7-991c-7eebed83c3c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.8.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.6)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.5.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.1)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.19)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.8.2)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.41.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.32.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.1->gradio) (2024.9.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.1->gradio) (14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from time import sleep\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "xceKeunn8UyT"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def chat(message:str, history=list):\n",
        "  if len(history) == 0:\n",
        "\n",
        "    history.append({\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful assistant.\"})\n",
        "\n",
        "    history.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": message})\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=history,\n",
        "        stream=True\n",
        "    )\n",
        "\n",
        "    resp = \"\"\n",
        "    for chunk in response:\n",
        "      sleep(np.random.random())\n",
        "      resp += chunk.choices[0].delta.content\n",
        "      yield resp\n"
      ],
      "metadata": {
        "id": "IWJUbyR5yJ3I"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo = gr.ChatInterface(fn=chat, type=\"messages\")\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "-6Tkzpg60nQP",
        "outputId": "08657817-7b83-47f7-ecf9-60ff0ede1324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://73a2cb623648dcf1f2.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://73a2cb623648dcf1f2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bHAfnK9YizZ"
      },
      "source": [
        "## Text generation hyperparameters\n",
        "\n",
        "There are several hyperparameters we can play with that determine how text is generated.\n",
        "For each token, the model outputs a score distribution over words, and that distribution is normalized using the softmax function to sum to 1.0.\n",
        "We have several options to modify this probability distribution in ways that affect the way text is generated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pibw6nM-YncC"
      },
      "source": [
        "### `temperature`\n",
        "\n",
        "The softmax function is shown below:\n",
        "\n",
        "$$\n",
        "\\text{softmax}(p) = \\frac{e^{x_i}}{\\sum_{j=0}^ne^{x_j}}\n",
        "$$\n",
        "\n",
        "The softmax function is defined in python below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIskN3MLMdmx"
      },
      "outputs": [],
      "source": [
        "def softmax(p):\n",
        "    return np.exp(p) / np.sum(np.exp(p))\n",
        "\n",
        "# Example usage\n",
        "p = np.array([1, 2, 3, 4, 5])\n",
        "print(softmax(p))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kdVu2JRp1Yi"
      },
      "source": [
        "The `temperature` paramater allows us to make the most probable words more probable (temperature < 1) or less probable (temperature > 1) than vanilla softmax (temperature = 1).\n",
        "The formula for softmax with temperature is show below:\n",
        "\n",
        "$$\n",
        "\\text{softmax}(p, T) = \\frac{e^{\\frac{x_i}{T}}}{\\sum_{j=0}^ne^{\\frac{x_j}{T}}}\n",
        "$$\n",
        "\n",
        "All you do is divide everything by T before taking the exponent; larger values of $T$ flatten the distribution, while smaller values of $T$ skew the distribution towards the most probable tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNo9GVX_r9g_"
      },
      "source": [
        "### Exercise: Softmax with temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wt5DWn9psCqD"
      },
      "outputs": [],
      "source": [
        "# Softmax function with temperature parameter\n",
        "def softmax_with_temperature(probs, temperature):\n",
        "    raise NotImplementedError(\"You need to implement this function!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pz-eytfxLA6I"
      },
      "outputs": [],
      "source": [
        "# Define a small probability distribution\n",
        "probs = np.array([0.5, 0.3, 0.1, 0.05, 0.05])\n",
        "\n",
        "# Plot the distribution with numbers on top of each bar\n",
        "def plot_distribution(temperature):\n",
        "    adjusted_probs = softmax_with_temperature(probs, temperature)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    bars = plt.bar(range(len(probs)), adjusted_probs, tick_label=['A', 'B', 'C', 'D', 'E'])\n",
        "    plt.ylim(0, 1)\n",
        "    plt.title(f'Softmax with Temperature = {temperature:.2f}')\n",
        "    plt.ylabel('Probability')\n",
        "    plt.xlabel('Tokens')\n",
        "\n",
        "    # Add numbers on top of each bar\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Interactive widget\n",
        "interact(plot_distribution, temperature=(0.1, 2.0, 0.1));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiAOWzFosPZ0"
      },
      "source": [
        "### Demo: Alter the `temperature` parameter\n",
        "\n",
        "In this demo, we generate several messages from the same prompt.\n",
        "If we lower the temperature to 0, what do you notice about the results?\n",
        "What if we raise it above 1.0?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOnm4SUoKSFu"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Write a python function that reverses as tring. Tell a joke in the docstring!\"},\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages,\n",
        "    temperature=1.0, # change this number between 0 and 2 to see the outcome\n",
        "    n=3 # generate 3 choices\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJNXwiCmKhJW"
      },
      "outputs": [],
      "source": [
        "for choice in response.choices:\n",
        "    print(choice.message.content)\n",
        "    print('\\n\\n' + '='*50 + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxkYh5ZkYrb-"
      },
      "source": [
        "### Demo: `top_k`\n",
        "\n",
        "In top k sampling, we define the number of tokens we want to consider to sample from.\n",
        "For example if `top_k = 3`, we will take the scores of the top 3 tokens and apply the softmax to only those 3 scores.\n",
        "Run the code block below to see how `top_k` normalizes the scores at different values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqptc8mIr0hq"
      },
      "outputs": [],
      "source": [
        "# Define a small probability distribution\n",
        "probs = np.array([0.5, 0.3, 0.1, 0.05, 0.05])\n",
        "\n",
        "# Plot the distribution with numbers on top of each bar\n",
        "def plot_top_k(top_k):\n",
        "    ticks = ['A', 'B', 'C', 'D', 'E']\n",
        "    sorted_probs = np.sort(probs)[::-1]\n",
        "    top_k_probs = sorted_probs[:top_k]\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    adjusted_probs = softmax(probs)\n",
        "    ax = axes[0]\n",
        "    bars = ax.bar(range(len(probs)), adjusted_probs, tick_label=ticks)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_title('Original Probabilities')\n",
        "    ax.set_ylabel('Probability')\n",
        "    ax.set_xlabel('Token')\n",
        "\n",
        "    # Add numbers on top of each bar\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    ax = axes[1]\n",
        "    if top_k < len(probs):\n",
        "        updated_probs = softmax(probs[:top_k])\n",
        "        bars = ax.bar(range(top_k), updated_probs, tick_label=ticks[:top_k])\n",
        "    else:\n",
        "        bars = ax.bar(range(len(probs)), softmax(probs), tick_label=ticks)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_title(f'Top {top_k} Probabilities')\n",
        "    ax.set_ylabel('Probability')\n",
        "    ax.set_xlabel('Token')\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Interactive widget\n",
        "interact(plot_top_k, top_k=(1, len(probs), 1));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-x9f33bYvPN"
      },
      "source": [
        "### Demo: `top_p` (aka nucleus sampling)\n",
        "\n",
        "`top_p` is similar to `top_k`, but instead of defining the number of tokens to consider, you define a cutoff for cumulative probability.\n",
        "For example, if you have a top_p of 0.6 and score of ('cat', 0.4), ('dog', 0.15), ('llama', 0.1), and ('parakeet', 0.01), you would cut only consider 'cat', 'dog', and 'llama' because 0.4 + 0.15 is less than 0.6, but 0.4 + 0.15 + 0.1 is greater.\n",
        "Because you have a probability cutoff instead of number of tokens, this may mean you have different numbers of tokens considered at each decoding step.\n",
        "\n",
        "Play around with the `top_p` slider below 👇 to get some intuition for how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nUXAXiPsGlA"
      },
      "outputs": [],
      "source": [
        "# Define a small probability distribution (can simulate a language model's logits)\n",
        "probs = np.array([0.4, 0.2, 0.15, 0.1, 0.08, 0.05, 0.02])\n",
        "\n",
        "# Function to apply top-p filtering with a minimum of one token selected\n",
        "def top_p_filter(probs, p):\n",
        "    sorted_probs = np.sort(probs)[::-1]\n",
        "    cumulative_probs = np.cumsum(sorted_probs)\n",
        "\n",
        "    # Ensure at least one token is selected\n",
        "    if p < sorted_probs[0]:\n",
        "        cutoff = 1\n",
        "    else:\n",
        "        cutoff = np.argmax(cumulative_probs >= p) + 1\n",
        "\n",
        "    filtered_probs = sorted_probs[:cutoff]\n",
        "    return filtered_probs, cutoff\n",
        "\n",
        "# Plot the distribution with top-p filtering\n",
        "def plot_top_p(p):\n",
        "    filtered_probs, cutoff = top_p_filter(probs, p)\n",
        "    normalized_probs = filtered_probs / np.sum(filtered_probs)  # Normalize the selected probabilities\n",
        "\n",
        "    # Create two subplots\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
        "\n",
        "    # Plot 1: Original distribution with top-p filtering\n",
        "    bars1 = axs[0].bar(range(len(probs)), np.sort(probs)[::-1], tick_label=labels)\n",
        "    axs[0].set_ylim(0, 1)\n",
        "    axs[0].set_title(f'Top-p Sampling (p = {p:.2f}) - Original Probabilities')\n",
        "    axs[0].set_ylabel('Probability')\n",
        "    axs[0].set_xlabel('Tokens')\n",
        "\n",
        "    # Highlight selected and unselected probabilities\n",
        "    for i, bar in enumerate(bars1):\n",
        "        if i >= cutoff:\n",
        "            bar.set_color('gray')  # Color the bars outside top-p as gray\n",
        "        else:\n",
        "            bar.set_color('blue')  # Highlight the selected probabilities\n",
        "\n",
        "    # Add numbers on top of each bar for original distribution\n",
        "    for bar in bars1:\n",
        "        yval = bar.get_height()\n",
        "        axs[0].text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    # Plot 2: Normalized probabilities of the selected tokens\n",
        "    bars2 = axs[1].bar(range(len(filtered_probs)), normalized_probs, tick_label=labels[:len(filtered_probs)])\n",
        "    axs[1].set_ylim(0, 1)\n",
        "    axs[1].set_title(f'Normalized Probabilities of Selected Tokens (p = {p:.2f})')\n",
        "    axs[1].set_ylabel('Normalized Probability')\n",
        "    axs[1].set_xlabel('Selected Tokens')\n",
        "\n",
        "    # Add numbers on top of each bar for normalized probabilities\n",
        "    for bar in bars2:\n",
        "        yval = bar.get_height()\n",
        "        axs[1].text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Interactive widget\n",
        "interact(plot_top_p, p=(0.01, 1.0, 0.05));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktB3q7OrYW72"
      },
      "source": [
        "## Exercise: Advanced chatbot with hyperparameter controls\n",
        "\n",
        "Now that you've learned about roles and generation hyperparameters, let's create a new chatbot that allows you to contol at them.\n",
        "Your chatbot must:\n",
        "- allow for control of at least 1 generation hyperparameter (ex: `temperature`)\n",
        "- allow for user input of a system message\n",
        "\n",
        "If this is too easy, try to:\n",
        "- allow for control over `temperature` and `top_p`\n",
        "- improve the UI by putting all the controls in a sidebar\n",
        "- have a `Clear` button that restarts the conversation\n",
        "- add documentation with markdown\n",
        "- implement streaming responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_qTM-_-UkG6"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koHpg9S2jthm"
      },
      "source": [
        "# Prompt engineering 1: zero-shot prompting\n",
        "\n",
        "To really benchmark how each of these techniques do, we need a baseline.\n",
        "We will use zero-shot prompting to get a base level of performance on our task.\n",
        "\n",
        "So far, we've been using the low-level `openai` library.\n",
        "However, there are several very competent higher-level libraries that provide great abstractions such as `langchain` and `llama-index`.\n",
        "Today, we'll be using `llama-index` to make our LLM calls a bit easier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cagMwLN4kKVP"
      },
      "source": [
        "## Prepare our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IslHF3_ak71j"
      },
      "outputs": [],
      "source": [
        "# Prompt engineering imports\n",
        "from datasets import load_dataset, Dataset\n",
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.core.prompts import ChatMessage\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "from IPython.display import display\n",
        "import asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAkjCmM4yP7T"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset('SetFit/amazon_reviews_multi_en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRlkTBciyz7S"
      },
      "outputs": [],
      "source": [
        "train_samples_per_class = 50\n",
        "eval_test_samples_per_class = 10\n",
        "train = Dataset.from_pandas(ds['train'].to_pandas().groupby('label').sample(train_samples_per_class, random_state=1234).reset_index(drop=True))\n",
        "valid = Dataset.from_pandas(ds['validation'].to_pandas().groupby('label').sample(eval_test_samples_per_class, random_state=1234).reset_index(drop=True))\n",
        "test = Dataset.from_pandas(ds['test'].to_pandas().groupby('label').sample(eval_test_samples_per_class, random_state=1234).reset_index(drop=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uf6l_lA33OV-"
      },
      "outputs": [],
      "source": [
        "train.to_pandas().sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocJ1AjZ2mMTx"
      },
      "outputs": [],
      "source": [
        "async def predict_and_evaluate(predict_fn):\n",
        "    labels = [int(x) for x in valid['label']]\n",
        "    tasks = [\n",
        "        predict_fn(text)\n",
        "        for text in valid['text']\n",
        "    ]\n",
        "    predictions = await asyncio.gather(*tasks)\n",
        "    cm = ConfusionMatrixDisplay.from_predictions(labels, predictions, normalize='true')\n",
        "    cr = classification_report(labels, predictions)\n",
        "    kappa = cohen_kappa_score(labels, predictions, weights='quadratic')\n",
        "    mae = mean_absolute_error(labels, predictions)\n",
        "    return labels, predictions, kappa, mae, cm, cr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMAydzy6kT2k"
      },
      "source": [
        "## Zero-shot prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KNSnSPikdh1"
      },
      "outputs": [],
      "source": [
        "prompt_tmpl_str = \"\"\"\\\n",
        "The review text is below.\n",
        "---------------------\n",
        "{review}\n",
        "---------------------\n",
        "Given the review text and not prior knowledge, \\\n",
        "please attempt to predict the review score of the context.\n",
        "\n",
        "Query: What is the rating of this review?\n",
        "Answer: \\\n",
        "\"\"\"\n",
        "\n",
        "prompt_tmpl = PromptTemplate(\n",
        "    prompt_tmpl_str,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rJk61JskmHa"
      },
      "outputs": [],
      "source": [
        "class Rating(BaseModel):\n",
        "    rating: int = Field(..., description=\"Rating of the review\", enum=[0, 1, 2, 3, 4])\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "zero_shot_structured_llm = llm.as_structured_llm(Rating)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idqg4yf-lPDi"
      },
      "outputs": [],
      "source": [
        "async def zero_shot_predict(text):\n",
        "    messages = [\n",
        "        ChatMessage.from_str(prompt_tmpl.format(review=text))\n",
        "    ]\n",
        "    response = await zero_shot_structured_llm.achat(messages)\n",
        "    return response.raw.rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bOBOElala6d"
      },
      "outputs": [],
      "source": [
        "zero_shot_labels, zero_shot_predictions, zero_shot_kappa, zero_shot_mae, zero_shot_cm, zero_shot_cr = await predict_and_evaluate(zero_shot_predict)\n",
        "print(f\"Cohen's Kappa: {zero_shot_kappa:.04f}, MAE: {zero_shot_mae}\")\n",
        "print(zero_shot_cr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vnpPQUOZpPX"
      },
      "source": [
        "# Prompt engineering 2: few-shot promting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-_rhqOV7lzY"
      },
      "source": [
        "In the following cell, we load and parse the data.\n",
        "The data here is uber reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dozUZ9OS433K"
      },
      "outputs": [],
      "source": [
        "train.shuffle()[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGLdG-Hz3a4h"
      },
      "outputs": [],
      "source": [
        "rng = np.random.Generator(np.random.PCG64(1234))\n",
        "\n",
        "def random_few_shot_examples_fn(**kwargs):\n",
        "    random_examples = train.shuffle(generator=rng)[:5]\n",
        "    result_strs = []\n",
        "    for text, rating in zip(random_examples['text'], random_examples['label']):\n",
        "        result_strs.append(f\"Text: {text}\\nRating: {rating}\")\n",
        "    return \"\\n\\n\".join(result_strs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYCRyZFT48eM"
      },
      "outputs": [],
      "source": [
        "print(random_few_shot_examples_fn())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGk-w7fL5XW5"
      },
      "outputs": [],
      "source": [
        "few_shot_prompt_tmpl_str = \"\"\"\\\n",
        "The review text is below.\n",
        "---------------------\n",
        "{review}\n",
        "---------------------\n",
        "Given the review text and not prior knowledge, \\\n",
        "please attempt to predict the review score of the context. \\\n",
        "Here are several examples of reviews and their ratings:\n",
        "\n",
        "{random_few_shot_examples}\n",
        "\n",
        "Query: What is the rating of this review?\n",
        "Answer: \\\n",
        "\"\"\"\n",
        "\n",
        "few_shot_prompt_tmpl = PromptTemplate(\n",
        "    few_shot_prompt_tmpl_str,\n",
        "    function_mappings={\"random_few_shot_examples\": random_few_shot_examples_fn},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apaZeBI35qcL"
      },
      "outputs": [],
      "source": [
        "print(few_shot_prompt_tmpl.format(review='I loved this product!'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSGk69O33Hfo"
      },
      "outputs": [],
      "source": [
        "class Rating(BaseModel):\n",
        "    rating: int = Field(..., description=\"Rating of the review\", enum=[0, 1, 2, 3, 4])\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "rand_few_shot_structured_llm = llm.as_structured_llm(Rating)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBZuZeBK440q"
      },
      "outputs": [],
      "source": [
        "async def random_few_shot_predict(text):\n",
        "    messages = [\n",
        "        ChatMessage.from_str(few_shot_prompt_tmpl.format(review=text))\n",
        "    ]\n",
        "    response = await rand_few_shot_structured_llm.achat(messages)\n",
        "    return response.raw.rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRAMqSGKre1j"
      },
      "outputs": [],
      "source": [
        "random_few_shot_labels, random_few_shot_predictions, random_few_shot_kappa, random_few_shot_mae, random_few_shot_cm, random_few_shot_cr = await predict_and_evaluate(random_few_shot_predict)\n",
        "print(f\"Cohen's Kappa: {random_few_shot_kappa:.04f}, MAE: {random_few_shot_mae}\")\n",
        "print(random_few_shot_cr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfedNl_UZyWj"
      },
      "source": [
        "# Demo: Embeddings and vector stores\n",
        "\n",
        "In the previous demonstration, we saw that providing several randomly-selected examples to the LLM at inference time does decently well - it's decently good at predicting the review score, especially within 1 point of the actual review.\n",
        "In a little bit, we'll see that providing better examples to the model at inference time helps improve these scores.\n",
        "But we need efficient ways of searching over our `train` examples to determine which one to use.\n",
        "\n",
        "This is when you want to use a vector store.\n",
        "Vector stores can be in-memory stors, on-disk stores, database extensions like pgvector for Postgres, or even external APIs like Pinecone.\n",
        "\n",
        "Today, we'll use a popular open-source vectore database called `chromadb`.\n",
        "This tool allows us to ingest our documents and search over them effectively to determine which examples to use.\n",
        "\n",
        "In this demo, we'll go over the basics of how to use ChromaDB.\n",
        "We will also use `sentence-transformers` for embeddings as an example of how to use open-weights embedding models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ij1wKj_P8hbl"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from chromadb import Client\n",
        "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89FH9DtpcI3r"
      },
      "source": [
        "In this cell, we declare our embedding function.\n",
        "We will use the small but powerful [BGE-small](https://huggingface.co/BAAI/bge-small-en-v1.5) model to embed our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpQdhTRP92MM"
      },
      "outputs": [],
      "source": [
        "embed_fn = SentenceTransformerEmbeddingFunction('BAAI/bge-small-en-v1.5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQtyevhXceZ9"
      },
      "source": [
        "Next, we can create our `chromadb` client and use it to create our collection (think table).\n",
        "Notice that we pass our embedding function.\n",
        "That way, when we add documents to the table, the the text is automatically embedded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjN4Byls-sl4"
      },
      "outputs": [],
      "source": [
        "chroma_client = Client()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU6DBPIP-tyS"
      },
      "outputs": [],
      "source": [
        "reviews = chroma_client.create_collection(\n",
        "    name='reviews',\n",
        "    embedding_function=embed_fn,\n",
        "    get_or_create=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMu6Dqs4_3Bm"
      },
      "outputs": [],
      "source": [
        "reviews.add(documents=train['text'], metadatas=[{'rating': x} for x in train['label']], ids=train['id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJuCCU5pcvvU"
      },
      "source": [
        "Once we have created our vector store, we can search over it using plain text.\n",
        "Here are 3 queries - good review, a neutral review, and a bad review.\n",
        "Let's search our train documents and observe the average rating for the closest 5 documents to each fake review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHhwQkgSZmQW"
      },
      "outputs": [],
      "source": [
        "queries = [\n",
        "    \"This product is great!\",\n",
        "    \"This product was pretty typical - not good or bad.\",\n",
        "    \"This product was awful\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNW6jiGX-1kg"
      },
      "outputs": [],
      "source": [
        "retrievals = reviews.query(\n",
        "    query_texts=queries,\n",
        "    n_results=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "226vy3W6Y9kz"
      },
      "outputs": [],
      "source": [
        "for query, metadatas in zip(queries, retrievals['metadatas']):\n",
        "    ratings = [float(i.get('rating')) for i in metadatas]\n",
        "    print(f\"Review {query}\")\n",
        "    print(f\"Avg rating of retrieved passages: {np.mean(ratings)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQOW_7KZ30XH"
      },
      "outputs": [],
      "source": [
        "reviews.query(query_texts = 'hello!')['documents']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIRfHOCFdBY_"
      },
      "source": [
        "## Discussion: Using vector stores\n",
        "\n",
        "Now that we have our data encoded this way, can anyone tell me how we might use this object to improve the way we classify reviews?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whSrknBwZ1hA"
      },
      "source": [
        "# Prompt engineering 3: dynamic few-shot prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdM9igtyZ4ZA"
      },
      "outputs": [],
      "source": [
        "def dynamic_few_shot_examples_fn(**kwargs):\n",
        "    n_examples = kwargs.get('n_examples', 5)\n",
        "    retrievals = reviews.query(\n",
        "        query_texts=[kwargs['review']],\n",
        "        n_results=n_examples\n",
        "    )\n",
        "    result_strs = []\n",
        "    documents = retrievals['documents'][0]\n",
        "    metadatas = retrievals['metadatas'][0]\n",
        "    for document, metadata in zip(documents, metadatas):\n",
        "        result_strs.append(f\"Text: {document}\\nRating: {metadata.get('rating')}\")\n",
        "    return \"\\n\\n\".join(result_strs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSjfOxUP1V69"
      },
      "outputs": [],
      "source": [
        "print(dynamic_few_shot_examples_fn(review=\"This is the best uber ride of my life!\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n47biWMBqt2t"
      },
      "outputs": [],
      "source": [
        "print(dynamic_few_shot_examples_fn(review=\"This is the worst uber ride of my life!\", n_examples=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFqomajG4YyU"
      },
      "outputs": [],
      "source": [
        "dynamic_few_shot_prompt_tmpl_str = \"\"\"\\\n",
        "The review text is below.\n",
        "---------------------\n",
        "{review}\n",
        "---------------------\n",
        "Given the review text and not prior knowledge, \\\n",
        "please attempt to predict the review score of the context. \\\n",
        "Here are several examples of reviews and their ratings:\n",
        "\n",
        "{dynamic_few_shot_examples}\n",
        "\n",
        "Query: What is the rating of this review?\n",
        "Answer: \\\n",
        "\"\"\"\n",
        "\n",
        "dynamic_few_shot_prompt_tmpl = PromptTemplate(\n",
        "    dynamic_few_shot_prompt_tmpl_str,\n",
        "    function_mappings={\"dynamic_few_shot_examples\": dynamic_few_shot_examples_fn},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UJsdk9Ypknd"
      },
      "outputs": [],
      "source": [
        "print(dynamic_few_shot_prompt_tmpl.format(review='I loved this product!', n_examples=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Sg2NzM-1ZXY"
      },
      "outputs": [],
      "source": [
        "class Rating(BaseModel):\n",
        "    rating: int = Field(..., description=\"Rating of the review\", enum=[0, 1, 2, 3, 4])\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "dynamic_few_shot_structured_llm = llm.as_structured_llm(Rating)\n",
        "\n",
        "async def dynamic_few_shot_predict(text):\n",
        "    messages = [\n",
        "        ChatMessage.from_str(dynamic_few_shot_prompt_tmpl.format(review=text))\n",
        "    ]\n",
        "    response = await dynamic_few_shot_structured_llm.achat(messages)\n",
        "    return response.raw.rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IU7jjKtk4s28"
      },
      "outputs": [],
      "source": [
        "dynamic_few_shot_labels, dynamic_few_shot_predictions, dynamic_few_shot_kappa, dynamic_few_shot_mae, dynamic_few_shot_cm, dynamic_few_shot_cr = await predict_and_evaluate(dynamic_few_shot_predict)\n",
        "print(f\"Cohen's Kappa: {dynamic_few_shot_kappa:.04f}, MAE: {dynamic_few_shot_mae}\")\n",
        "print(dynamic_few_shot_cr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA84jzIihOaw"
      },
      "source": [
        "# Exercise: Rating Reviews\n",
        "\n",
        "In today's session, we've learned about:\n",
        "- Chat models, interfaces, and `gradio`\n",
        "- Zero shot prompting\n",
        "- Few shot learning\n",
        "- Embeddings and vector stores\n",
        "- Dynamic few shot learning\n",
        "\n",
        "It's time to combine these principles into our final exercise of the day.\n",
        "Your task is to create a `gradio` app where a user can paste a review from Amazon and the app displays the predicted number of ⭐stars⭐.\n",
        "To complete this task, please:\n",
        "- Create a GradIO app with...\n",
        "  - an input field where a user can submit text\n",
        "  - a submit button and/or functionality to submit the text to the app when the user hits the return key\n",
        "  - an output field to display the predicted result\n",
        "\n",
        "If this is too easy, try to:\n",
        "- Add hyperparameters like the number of examples retrieved\n",
        "- Add details in markdown for how to use the app\n",
        "- Display the prompt and response for inspection\n",
        "- Install the [`gradio-client`](https://pypi.org/project/gradio-client/) library and make requests to your app from this notebook\n",
        "\n",
        "If you're done, and **really** want to challenge yourslef, add a dropdown for a different model.\n",
        "You can follow the `Gemini API keys` button in the 👈secrets🔑 tab of Colab, or follow [this notebook guide](https://github.com/mgfrantz/CTME-llm-lecture-resources/blob/main/resources/ollama.ipynb) on a GPU colab to try doing inference with local LLMs like llama3 (Recommended to restart and use a gpu runtime, runtime > change runtime type. May not work with `.as_structured_llm`, check out [this low-level guide on structured outputs](https://docs.llamaindex.ai/en/stable/examples/output_parsing/llm_program/)).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfBvued0nApC"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAmFGXzWnApC"
      },
      "source": [
        "# Bonus: Prompt optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOWYJoP9nApC"
      },
      "outputs": [],
      "source": [
        "import dspy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_9NNzcinApC"
      },
      "outputs": [],
      "source": [
        "lm = dspy.LM(model='openai/gpt-4o-mini')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdLG_srwnApD"
      },
      "outputs": [],
      "source": [
        "dspy.configure(lm=lm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyrUXAbbnApD"
      },
      "outputs": [],
      "source": [
        "train_examples = [\n",
        "    dspy.Example(\n",
        "        review=e['text'],\n",
        "        rating=e['label'],\n",
        "    ).with_inputs('review')\n",
        "    for e in train\n",
        "]\n",
        "\n",
        "valid_examples = [\n",
        "    dspy.Example(\n",
        "        review=e['text'],\n",
        "        rating=e['label'],\n",
        "    ).with_inputs('review')\n",
        "    for e in valid\n",
        "]\n",
        "\n",
        "test_examples = [\n",
        "    dspy.Example(\n",
        "        review=e['text'],\n",
        "        rating=e['label'],\n",
        "    ).with_inputs('review')\n",
        "    for e in test\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WoPYCq4nApD"
      },
      "outputs": [],
      "source": [
        "class FewShotResponse(dspy.Signature):\n",
        "    \"A rating for a review\"\n",
        "    review: str = dspy.InputField(description=\"Review text\")\n",
        "    examples: str = dspy.InputField(description=\"Examples of reviews and their ratings\")\n",
        "    rating: int = dspy.OutputField(description=\"Rating of the review. Should be 0, 1, 2, 3, or 4.\", ge=0, le=4)\n",
        "\n",
        "class FewShotLearning(dspy.Module):\n",
        "    def __init__(self, collection=reviews, k=5):\n",
        "        super().__init__()\n",
        "        self.cot = dspy.ChainOfThought(FewShotResponse)\n",
        "        self.collection = collection\n",
        "        self.k = k\n",
        "\n",
        "\n",
        "    def search(self, query):\n",
        "        results = self.collection.query(query_texts=query, n_results=self.k)\n",
        "        examples = \"\\n\\n\".join([f\"Review: {doc}\\nRating: {meta.get('rating')}\" for doc, meta in zip(results['documents'][0], results['metadatas'][0])])\n",
        "        return examples\n",
        "\n",
        "    def forward(self, review):\n",
        "        examples = self.search(review)\n",
        "        return self.cot(review=review, examples=examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNsdbd9-nApD"
      },
      "outputs": [],
      "source": [
        "# calculate metrics (not async)\n",
        "def dspy_predict_and_evaluate(predict_fn):\n",
        "    labels = [int(x) for x in valid['label']]\n",
        "    predictions = [\n",
        "        predict_fn(text).rating\n",
        "        for text in valid['text']\n",
        "    ]\n",
        "    cm = ConfusionMatrixDisplay.from_predictions(labels, predictions, normalize='true')\n",
        "    cr = classification_report(labels, predictions)\n",
        "    kappa = cohen_kappa_score(labels, predictions, weights='quadratic')\n",
        "    mae = mean_absolute_error(labels, predictions)\n",
        "    return labels, predictions, kappa, mae, cm, cr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqDveD8anApD"
      },
      "outputs": [],
      "source": [
        "def score_func(example, pred, trace=None):\n",
        "    return float(example.rating == pred.rating)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuXaKwO4nApD"
      },
      "outputs": [],
      "source": [
        "eval_example = valid_examples[0]\n",
        "score_func(eval_example, FewShotLearning()(eval_example.review))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dIpW8vfnApD"
      },
      "outputs": [],
      "source": [
        "# If you want to run the optimizer, set this to True.\n",
        "# Otherwise, an optimized model will be downloaded for you.\n",
        "DO_OPTIMIZE = False\n",
        "if DO_OPTIMIZE:\n",
        "    optimizer = dspy.teleprompt.MIPROv2(metric=score_func)\n",
        "    optimized_few_shot = optimizer.compile(\n",
        "        FewShotLearning(),\n",
        "        trainset=train_examples,\n",
        "        valset=valid_examples,\n",
        "        max_bootstrapped_demos=2,\n",
        "        max_labeled_demos=2,\n",
        "        requires_permission_to_run=False\n",
        "    )\n",
        "else:\n",
        "    # If you don't want to run the optimizer, you can download the pre-trained model below\n",
        "    import os\n",
        "    import requests\n",
        "\n",
        "        # Download the pre-trained model if it doesn't exist locally\n",
        "    model_path = \"mipro_optimized_few_shot.json\"\n",
        "    if not os.path.exists(model_path):\n",
        "        print(\"Downloading pre-trained model...\")\n",
        "        url = \"https://raw.githubusercontent.com/mgfrantz/CTME-llm-lecture-resources/main/prototyping_ai/mipro_optimized_few_shot.json\"\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            with open(model_path, \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "            print(\"Model downloaded successfully\")\n",
        "        else:\n",
        "            print(f\"Failed to download model: {response.status_code}\")\n",
        "\n",
        "    optimized_few_shot = FewShotLearning()\n",
        "    optimized_few_shot.load(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mUSnC05nApD"
      },
      "outputs": [],
      "source": [
        "optimized_few_shot.save(\"mipro_optimized_few_shot.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNZH148OnApD"
      },
      "outputs": [],
      "source": [
        "# calculate metrics (not async, no async support for dspy yet)\n",
        "def dspy_predict_and_evaluate(predict_fn):\n",
        "    labels = [int(x) for x in valid['label']]\n",
        "    predictions = [\n",
        "        predict_fn(text).rating\n",
        "        for text in valid['text']\n",
        "    ]\n",
        "    cm = ConfusionMatrixDisplay.from_predictions(labels, predictions, normalize='true')\n",
        "    cr = classification_report(labels, predictions)\n",
        "    kappa = cohen_kappa_score(labels, predictions, weights='quadratic')\n",
        "    mae = mean_absolute_error(labels, predictions)\n",
        "    return labels, predictions, kappa, mae, cm, cr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xr9RIFcNnApD"
      },
      "outputs": [],
      "source": [
        "unoptimized_labels, unoptimized_predictions, unoptimized_kappa, unoptimized_mae, unoptimized_cm, unoptimized_cr = dspy_predict_and_evaluate(FewShotLearning())\n",
        "print(f\"Cohen's Kappa: {unoptimized_kappa:.04f}, MAE: {unoptimized_mae}\")\n",
        "print(unoptimized_cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_OdQbXonApD"
      },
      "outputs": [],
      "source": [
        "optimized_labels, optimized_predictions, optimized_kappa, optimized_mae, optimized_cm, optimized_cr = dspy_predict_and_evaluate(optimized_few_shot)\n",
        "print(f\"Cohen's Kappa: {optimized_kappa:.04f}, MAE: {optimized_mae}\")\n",
        "print(optimized_cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgHeHL6HnApD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}