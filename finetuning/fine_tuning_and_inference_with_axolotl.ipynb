{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgfrantz/CTME-llm-lecture-resources/blob/main/finetunint/fine_tuning_and_inference_with_axolotl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning with Axolotl\n",
        "\n",
        "[Axolotl](https://github.com/axolotl-ai-cloud/axolotl) is a convenient library that helps fine tune text generation models.\n",
        "In this notebook, we will use `axolotl` to fine tune a small LLM on a dataset we've created.\n",
        "Then, we'll run our evaluation suite on that model to see how it compares to other solutions we've explored in the past."
      ],
      "metadata": {
        "id": "Nnjlha4NSwDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment setup and imports"
      ],
      "metadata": {
        "id": "YSBVexvhTeoZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8OWX4huv9di",
        "outputId": "d45644f6-66c9-49ff-9e71-32f332fa5a4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'axolotl'...\n",
            "remote: Enumerating objects: 16083, done.\u001b[K\n",
            "remote: Counting objects: 100% (5171/5171), done.\u001b[K\n",
            "remote: Compressing objects: 100% (905/905), done.\u001b[K\n",
            "remote: Total 16083 (delta 4521), reused 4586 (delta 4107), pack-reused 10912 (from 1)\u001b[K\n",
            "Receiving objects: 100% (16083/16083), 6.02 MiB | 19.62 MiB/s, done.\n",
            "Resolving deltas: 100% (10481/10481), done.\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m25.0/25.0 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.1/58.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m402.5/402.5 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m395.2/395.2 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m116.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.3/84.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m606.3/606.3 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.5/44.5 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m769.2/769.2 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fschat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wavedrom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 1.4.15 requires pydantic>=2.7.0, but you have pydantic 2.6.3 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "mlflow 2.13.0 requires pyarrow<16,>=4.0.0, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if os.path.exists(\"axolotl\"):\n",
        "  !rm -rf axolotl\n",
        "!git clone https://github.com/axolotl-ai-cloud/axolotl\n",
        "# This handles a mismatch between xformers torch requirements and that of other dependencies\n",
        "with open('/content/axolotl/requirements.txt', 'r') as file:\n",
        "    requirements = file.read()\n",
        "    # replace xformers==0.0.27 with xformers\n",
        "    requirements = requirements.replace('xformers==0.0.27', 'xformers')\n",
        "with open('/content/axolotl/requirements.txt', 'w') as file:\n",
        "    file.write(requirements)\n",
        "!pip install -qqqq ninja packaging mlflow==\"2.13.0\"\n",
        "!cd axolotl && pip install -qqqq -e \".[flash-attn,deepspeed]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jSYMsvJR214f"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "token = userdata.get('HF_TOKEN')\n",
        "os.environ['HF_TOKEN'] = token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Axolotl configuration"
      ],
      "metadata": {
        "id": "s7BQ-DosToZW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWZ4a8_hu9yk",
        "outputId": "1447d5ba-4ef1-4ce5-d444-6443000cd6a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test_axolotl.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_axolotl.yaml\n",
        "\n",
        "# Model config\n",
        "adapter: qlora\n",
        "base_model: TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\n",
        "bf16: auto\n",
        "\n",
        "# HF hub config (push to huggingface)\n",
        "# requires HF_TOKEN api key to be set (ðŸ‘ˆðŸ”‘secrets)\n",
        "hf_use_auth_token: true\n",
        "hub_model_id: mgfrantz/axolotl-test\n",
        "mlflow_experiment_name: axolotl-test\n",
        "\n",
        "# Data config\n",
        "dataset_prepared_path: null\n",
        "datasets:\n",
        "- path: mhenrichsen/alpaca_2k_test\n",
        "  type: alpaca\n",
        "\n",
        "# Training config\n",
        "debug: null\n",
        "deepspeed: null\n",
        "early_stopping_patience: null\n",
        "eval_sample_packing: false\n",
        "evals_per_epoch: 4\n",
        "flash_attention: true\n",
        "fp16: null\n",
        "fsdp: null\n",
        "fsdp_config: null\n",
        "gradient_accumulation_steps: 4\n",
        "gradient_checkpointing: true\n",
        "group_by_length: false\n",
        "\n",
        "\n",
        "learning_rate: 0.0002\n",
        "load_in_4bit: true\n",
        "load_in_8bit: false\n",
        "local_rank: null\n",
        "logging_steps: 1\n",
        "lora_alpha: 16\n",
        "lora_dropout: 0.05\n",
        "lora_fan_in_fan_out: null\n",
        "lora_model_dir: null\n",
        "lora_r: 32\n",
        "lora_target_linear: true\n",
        "lora_target_modules: null\n",
        "lr_scheduler: cosine\n",
        "micro_batch_size: 8\n",
        "model_type: LlamaForCausalLM\n",
        "num_epochs: 4\n",
        "optimizer: paged_adamw_32bit\n",
        "output_dir: ./outputs/qlora-out\n",
        "pad_to_sequence_len: true\n",
        "resume_from_checkpoint: null\n",
        "sample_packing: true\n",
        "saves_per_epoch: 1\n",
        "sequence_len: 4096\n",
        "special_tokens: null\n",
        "strict: false\n",
        "tf32: false\n",
        "tokenizer_type: LlamaTokenizer\n",
        "train_on_inputs: false\n",
        "val_set_size: 0.05\n",
        "wandb_entity: null\n",
        "wandb_log_model: null\n",
        "wandb_name: null\n",
        "wandb_project: null\n",
        "wandb_watch: null\n",
        "warmup_steps: 10\n",
        "weight_decay: 0.0\n",
        "xformers_attention: null\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine tuning"
      ],
      "metadata": {
        "id": "N7PnQChITtHs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYHoF8rtdlCp",
        "outputId": "eee044a9-8e44-4d13-ad72-f4cf007db19f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2024-09-27 20:53:32.180645: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-09-27 20:53:32.197597: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-27 20:53:32.218657: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-27 20:53:32.225210: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-27 20:53:32.240716: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-27 20:53:33.398199: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/axolotl/src/axolotl/utils/gradient_checkpointing/unsloth.py:29: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, forward_function, hidden_states, *args):\n",
            "/content/axolotl/src/axolotl/utils/gradient_checkpointing/unsloth.py:40: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, dY):\n",
            "[2024-09-27 20:53:36,353] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-09-27 20:53:36,431] [INFO] [root.spawn:61] [PID:8730] x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -c /tmp/tmpn547w8n3/test.c -o /tmp/tmpn547w8n3/test.o\n",
            "[2024-09-27 20:53:36,449] [INFO] [root.spawn:61] [PID:8730] x86_64-linux-gnu-gcc /tmp/tmpn547w8n3/test.o -laio -o /tmp/tmpn547w8n3/a.out\n",
            "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
            "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
            "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
            "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
            "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
            "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n",
            "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, input, weight, bias=None):\n",
            "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, grad_output):\n",
            "/content/axolotl/src/axolotl/monkeypatch/relora.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
            "  from torch.distributed.optim import ZeroRedundancyOptimizer\n",
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:151: UserWarning: Field \"model_kwargs\" has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n",
            "[2024-09-27 20:53:39,671] [INFO] [axolotl.utils.config.models.input.check_eval_packing:993] [PID:8730] [RANK:0] setting `remove_unused_columns: false` for when sample_packing and eval_sample_packing don't match\u001b[39m\n",
            "[2024-09-27 20:53:39,672] [DEBUG] [axolotl.normalize_config:83] [PID:8730] [RANK:0] bf16 support detected, enabling for this configuration.\u001b[39m\n",
            "[2024-09-27 20:53:39,958] [INFO] [axolotl.normalize_config:186] [PID:8730] [RANK:0] GPU memory usage baseline: 0.000GB (+0.331GB misc)\u001b[39m\n",
            "                                 dP            dP   dP \n",
            "                                 88            88   88 \n",
            "      .d8888b. dP.  .dP .d8888b. 88 .d8888b. d8888P 88 \n",
            "      88'  `88  `8bd8'  88'  `88 88 88'  `88   88   88 \n",
            "      88.  .88  .d88b.  88.  .88 88 88.  .88   88   88 \n",
            "      `88888P8 dP'  `dP `88888P' dP `88888P'   dP   dP \n",
            "                                                       \n",
            "                                                       \n",
            "\n",
            "****************************************\n",
            "**** Axolotl Dependency Versions *****\n",
            "  accelerate: 0.34.2         \n",
            "        peft: 0.13.0         \n",
            "transformers: 4.45.0         \n",
            "         trl: 0.9.6          \n",
            "       torch: 2.4.1+cu121    \n",
            "bitsandbytes: 0.44.0         \n",
            "****************************************\n",
            "[2024-09-27 20:53:40,990] [DEBUG] [axolotl.load_tokenizer:282] [PID:8730] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
            "[2024-09-27 20:53:40,990] [DEBUG] [axolotl.load_tokenizer:283] [PID:8730] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
            "[2024-09-27 20:53:40,990] [DEBUG] [axolotl.load_tokenizer:284] [PID:8730] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
            "[2024-09-27 20:53:40,990] [DEBUG] [axolotl.load_tokenizer:285] [PID:8730] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
            "[2024-09-27 20:53:40,990] [INFO] [axolotl.load_tokenizer:296] [PID:8730] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
            "[2024-09-27 20:53:40,990] [INFO] [axolotl.load_tokenized_prepared_datasets:195] [PID:8730] [RANK:0] Unable to find prepared dataset in last_run_prepared/b679ea8e13fdec9db52fe0332ca58c81\u001b[39m\n",
            "[2024-09-27 20:53:40,990] [INFO] [axolotl.load_tokenized_prepared_datasets:196] [PID:8730] [RANK:0] Loading raw datasets...\u001b[39m\n",
            "\u001b[33m[2024-09-27 20:53:40,990] [WARNING] [axolotl.load_tokenized_prepared_datasets:198] [PID:8730] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset.\u001b[39m\n",
            "[2024-09-27 20:53:40,990] [INFO] [axolotl.load_tokenized_prepared_datasets:205] [PID:8730] [RANK:0] No seed provided, using default seed of 42\u001b[39m\n",
            "[2024-09-27 20:53:46,395] [INFO] [axolotl.get_dataset_wrapper:558] [PID:8730] [RANK:0] Loading dataset with base_type: alpaca and prompt_style: None\u001b[39m\n",
            "[2024-09-27 20:53:46,439] [INFO] [axolotl.load_tokenized_prepared_datasets:430] [PID:8730] [RANK:0] merging datasets\u001b[39m\n",
            "[2024-09-27 20:53:46,569] [INFO] [axolotl.load_tokenized_prepared_datasets:443] [PID:8730] [RANK:0] Saving merged prepared dataset to disk... last_run_prepared/b679ea8e13fdec9db52fe0332ca58c81\u001b[39m\n",
            "Saving the dataset (1/1 shards): 100% 2000/2000 [00:00<00:00, 77479.32 examples/s]\n",
            "[2024-09-27 20:53:46,612] [DEBUG] [axolotl.calculate_total_num_steps:316] [PID:8730] [RANK:0] total_num_tokens: 414_041\u001b[39m\n",
            "[2024-09-27 20:53:46,631] [DEBUG] [axolotl.calculate_total_num_steps:329] [PID:8730] [RANK:0] `total_supervised_tokens: 294_246`\u001b[39m\n",
            "[2024-09-27 20:53:52,536] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:195] [PID:8730] [RANK:0] gather_len_batches: [13]\u001b[39m\n",
            "[2024-09-27 20:53:52,536] [DEBUG] [axolotl.calculate_total_num_steps:381] [PID:8730] [RANK:0] data_loader_len: 3\u001b[39m\n",
            "[2024-09-27 20:53:52,536] [INFO] [axolotl.calc_sample_packing_eff_est:387] [PID:8730] [RANK:0] sample_packing_eff_est across ranks: [0.9719637357271634]\u001b[39m\n",
            "[2024-09-27 20:53:52,536] [DEBUG] [axolotl.calculate_total_num_steps:399] [PID:8730] [RANK:0] sample_packing_eff_est: 0.98\u001b[39m\n",
            "[2024-09-27 20:53:52,536] [DEBUG] [axolotl.calculate_total_num_steps:407] [PID:8730] [RANK:0] total_num_steps: 12\u001b[39m\n",
            "[2024-09-27 20:53:52,542] [DEBUG] [axolotl.train.train:67] [PID:8730] [RANK:0] loading tokenizer... TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\u001b[39m\n",
            "[2024-09-27 20:53:53,285] [DEBUG] [axolotl.load_tokenizer:282] [PID:8730] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
            "[2024-09-27 20:53:53,285] [DEBUG] [axolotl.load_tokenizer:283] [PID:8730] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
            "[2024-09-27 20:53:53,285] [DEBUG] [axolotl.load_tokenizer:284] [PID:8730] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
            "[2024-09-27 20:53:53,285] [DEBUG] [axolotl.load_tokenizer:285] [PID:8730] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
            "[2024-09-27 20:53:53,285] [INFO] [axolotl.load_tokenizer:296] [PID:8730] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
            "[2024-09-27 20:53:53,285] [DEBUG] [axolotl.train.train:96] [PID:8730] [RANK:0] loading model and peft_config...\u001b[39m\n",
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
            "\u001b[33m[2024-09-27 20:53:57,774] [WARNING] [axolotl.load_model:769] [PID:8730] [RANK:0] increasing model.config.max_position_embeddings from 2048 to 4096\u001b[39m\n",
            "[2024-09-27 20:53:57,774] [INFO] [axolotl.load_model:791] [PID:8730] [RANK:0] GPU memory usage after model load: 0.718GB (+0.043GB cache, +0.542GB misc)\u001b[39m\n",
            "[2024-09-27 20:53:57,787] [INFO] [axolotl.load_model:849] [PID:8730] [RANK:0] converting PEFT model w/ prepare_model_for_kbit_training\u001b[39m\n",
            "[2024-09-27 20:53:57,790] [INFO] [axolotl.load_model:858] [PID:8730] [RANK:0] converting modules to torch.bfloat16 for flash attention\u001b[39m\n",
            "[2024-09-27 20:53:57,793] [INFO] [axolotl.load_lora:1023] [PID:8730] [RANK:0] found linear modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[39m\n",
            "trainable params: 25,231,360 || all params: 1,125,279,744 || trainable%: 2.2422\n",
            "[2024-09-27 20:53:58,249] [INFO] [axolotl.load_model:906] [PID:8730] [RANK:0] GPU memory usage after adapters: 0.812GB (+0.533GB cache, +0.542GB misc)\u001b[39m\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "[2024-09-27 20:53:59,727] [INFO] [axolotl.train.train:137] [PID:8730] [RANK:0] Pre-saving adapter config to ./outputs/qlora-out\u001b[39m\n",
            "[2024-09-27 20:53:59,732] [INFO] [axolotl.train.train:174] [PID:8730] [RANK:0] Starting trainer...\u001b[39m\n",
            "[2024-09-27 20:54:00,132] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:195] [PID:8730] [RANK:0] gather_len_batches: [13]\u001b[39m\n",
            "[2024-09-27 20:54:00,134] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:195] [PID:8730] [RANK:0] gather_len_batches: [13]\u001b[39m\n",
            "[2024-09-27 20:54:00,233] [INFO] [axolotl.callbacks.on_train_begin:39] [PID:8730] [RANK:0] The Axolotl config has been saved to the MLflow artifacts.\u001b[39m\n",
            "  0% 0/12 [00:00<?, ?it/s][2024-09-27 20:54:00,235] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:195] [PID:8730] [RANK:0] gather_len_batches: [13]\u001b[39m\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "{'loss': 1.4095, 'grad_norm': 0.1798076033592224, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
            "  8% 1/12 [00:46<08:29, 46.32s/it]\n",
            "  0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 2/12 [00:02<00:14,  1.49s/it]\u001b[A\n",
            " 25% 3/12 [00:05<00:18,  2.11s/it]\u001b[A\n",
            " 33% 4/12 [00:08<00:19,  2.43s/it]\u001b[A\n",
            " 42% 5/12 [00:12<00:18,  2.68s/it]\u001b[A\n",
            " 50% 6/12 [00:15<00:16,  2.78s/it]\u001b[A\n",
            " 58% 7/12 [00:18<00:14,  2.85s/it]\u001b[A\n",
            " 67% 8/12 [00:21<00:11,  2.89s/it]\u001b[A\n",
            " 75% 9/12 [00:24<00:08,  2.96s/it]\u001b[A\n",
            " 83% 10/12 [00:27<00:05,  2.96s/it]\u001b[A\n",
            " 92% 11/12 [00:30<00:02,  2.96s/it]\u001b[A[2024-09-27 20:55:22,320] [INFO] [accelerate.accelerator.gather_for_metrics:2457] [PID:8730] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                  \n",
            "\u001b[A{'eval_loss': 1.4450758695602417, 'eval_runtime': 36.0754, 'eval_samples_per_second': 2.772, 'eval_steps_per_second': 0.36, 'epoch': 0.33}\n",
            "  8% 1/12 [01:22<08:29, 46.32s/it]\n",
            "100% 12/12 [00:33<00:00,  2.96s/it]\u001b[A\n",
            "                                   \u001b[A[2024-09-27 20:56:08,593] [INFO] [axolotl.callbacks.on_step_end:128] [PID:8730] [RANK:0] GPU memory usage while training: 0.929GB (+19.060GB cache, +0.816GB misc)\u001b[39m\n",
            "{'loss': 1.4009, 'grad_norm': 0.19046898186206818, 'learning_rate': 4e-05, 'epoch': 0.67}\n",
            " 17% 2/12 [02:08<11:13, 67.33s/it]\n",
            "  0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 2/12 [00:02<00:14,  1.49s/it]\u001b[A\n",
            " 25% 3/12 [00:05<00:19,  2.12s/it]\u001b[A\n",
            " 33% 4/12 [00:08<00:19,  2.44s/it]\u001b[A\n",
            " 42% 5/12 [00:12<00:18,  2.69s/it]\u001b[A\n",
            " 50% 6/12 [00:15<00:16,  2.78s/it]\u001b[A\n",
            " 58% 7/12 [00:18<00:14,  2.85s/it]\u001b[A\n",
            " 67% 8/12 [00:21<00:11,  2.89s/it]\u001b[A\n",
            " 75% 9/12 [00:24<00:08,  2.96s/it]\u001b[A\n",
            " 83% 10/12 [00:27<00:05,  2.96s/it]\u001b[A\n",
            " 92% 11/12 [00:30<00:02,  2.97s/it]\u001b[A[2024-09-27 20:56:44,436] [INFO] [accelerate.accelerator.gather_for_metrics:2457] [PID:8730] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                  \n",
            "\u001b[A{'eval_loss': 1.4446611404418945, 'eval_runtime': 36.1496, 'eval_samples_per_second': 2.766, 'eval_steps_per_second': 0.36, 'epoch': 0.67}\n",
            " 17% 2/12 [02:44<11:13, 67.33s/it]\n",
            "100% 12/12 [00:33<00:00,  2.97s/it]\u001b[A\n",
            "{'loss': 1.4018, 'grad_norm': 0.18244218826293945, 'learning_rate': 6e-05, 'epoch': 1.0}\n",
            " 25% 3/12 [03:30<11:08, 74.22s/it]\n",
            "  0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 2/12 [00:02<00:14,  1.50s/it]\u001b[A\n",
            " 25% 3/12 [00:05<00:19,  2.12s/it]\u001b[A\n",
            " 33% 4/12 [00:08<00:19,  2.44s/it]\u001b[A\n",
            " 42% 5/12 [00:12<00:18,  2.69s/it]\u001b[A\n",
            " 50% 6/12 [00:15<00:16,  2.78s/it]\u001b[A\n",
            " 58% 7/12 [00:18<00:14,  2.85s/it]\u001b[A\n",
            " 67% 8/12 [00:21<00:11,  2.89s/it]\u001b[A\n",
            " 75% 9/12 [00:24<00:08,  2.96s/it]\u001b[A\n",
            " 83% 10/12 [00:27<00:05,  2.96s/it]\u001b[A\n",
            " 92% 11/12 [00:30<00:02,  2.97s/it]\u001b[A[2024-09-27 20:58:06,849] [INFO] [accelerate.accelerator.gather_for_metrics:2457] [PID:8730] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                  \n",
            "\u001b[A{'eval_loss': 1.4426312446594238, 'eval_runtime': 36.1389, 'eval_samples_per_second': 2.767, 'eval_steps_per_second': 0.36, 'epoch': 1.0}\n",
            " 25% 3/12 [04:06<11:08, 74.22s/it]\n",
            "100% 12/12 [00:33<00:00,  2.97s/it]\u001b[A\n",
            "                                   \u001b[A/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "[2024-09-27 20:58:16,834] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:195] [PID:8730] [RANK:0] gather_len_batches: [13]\u001b[39m\n",
            "{'loss': 1.3928, 'grad_norm': 0.1904488205909729, 'learning_rate': 8e-05, 'epoch': 1.25}\n",
            " 33% 4/12 [04:51<10:14, 76.85s/it]\n",
            "  0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 2/12 [00:02<00:14,  1.50s/it]\u001b[A\n",
            " 25% 3/12 [00:05<00:19,  2.12s/it]\u001b[A\n",
            " 33% 4/12 [00:08<00:19,  2.45s/it]\u001b[A\n",
            " 42% 5/12 [00:12<00:18,  2.69s/it]\u001b[A\n",
            " 50% 6/12 [00:15<00:16,  2.79s/it]\u001b[A\n",
            " 58% 7/12 [00:18<00:14,  2.85s/it]\u001b[A\n",
            " 67% 8/12 [00:21<00:11,  2.90s/it]\u001b[A\n",
            " 75% 9/12 [00:24<00:08,  2.96s/it]\u001b[A\n",
            " 83% 10/12 [00:27<00:05,  2.96s/it]\u001b[A\n",
            " 92% 11/12 [00:30<00:02,  2.97s/it]\u001b[A[2024-09-27 20:59:27,783] [INFO] [accelerate.accelerator.gather_for_metrics:2457] [PID:8730] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                  \n",
            "\u001b[A{'eval_loss': 1.4368757009506226, 'eval_runtime': 36.184, 'eval_samples_per_second': 2.764, 'eval_steps_per_second': 0.359, 'epoch': 1.25}\n",
            " 33% 4/12 [05:27<10:14, 76.85s/it]\n",
            "100% 12/12 [00:33<00:00,  2.97s/it]\u001b[A\n",
            "{'loss': 1.3754, 'grad_norm': 0.18176476657390594, 'learning_rate': 0.0001, 'epoch': 1.58}\n",
            " 42% 5/12 [06:14<09:12, 78.88s/it]\n",
            "  0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 2/12 [00:02<00:14,  1.50s/it]\u001b[A\n",
            " 25% 3/12 [00:05<00:19,  2.12s/it]\u001b[A\n",
            " 33% 4/12 [00:08<00:19,  2.45s/it]\u001b[A\n",
            " 42% 5/12 [00:12<00:18,  2.70s/it]\u001b[A\n",
            " 50% 6/12 [00:15<00:16,  2.79s/it]\u001b[A\n",
            " 58% 7/12 [00:18<00:14,  2.86s/it]\u001b[A\n",
            " 67% 8/12 [00:21<00:11,  2.90s/it]\u001b[A\n",
            " 75% 9/12 [00:24<00:08,  2.96s/it]\u001b[A\n",
            " 83% 10/12 [00:27<00:05,  2.96s/it]\u001b[A\n",
            " 92% 11/12 [00:30<00:02,  2.97s/it]\u001b[A[2024-09-27 21:00:50,270] [INFO] [accelerate.accelerator.gather_for_metrics:2457] [PID:8730] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                  \n",
            "\u001b[A{'eval_loss': 1.4268478155136108, 'eval_runtime': 36.2028, 'eval_samples_per_second': 2.762, 'eval_steps_per_second': 0.359, 'epoch': 1.58}\n",
            " 42% 5/12 [06:50<09:12, 78.88s/it]\n",
            "100% 12/12 [00:33<00:00,  2.97s/it]\u001b[A\n",
            "{'loss': 1.399, 'grad_norm': 0.1898835152387619, 'learning_rate': 0.00012, 'epoch': 1.92}\n",
            " 50% 6/12 [07:36<08:00, 80.11s/it]\n",
            "  0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 2/12 [00:02<00:14,  1.50s/it]\u001b[A\n",
            " 25% 3/12 [00:05<00:19,  2.12s/it]\u001b[A\n",
            " 33% 4/12 [00:08<00:19,  2.45s/it]\u001b[A\n",
            " 42% 5/12 [00:12<00:18,  2.69s/it]\u001b[A\n",
            " 50% 6/12 [00:15<00:16,  2.79s/it]\u001b[A\n",
            " 58% 7/12 [00:18<00:14,  2.85s/it]\u001b[A\n",
            " 67% 8/12 [00:21<00:11,  2.90s/it]\u001b[A\n",
            " 75% 9/12 [00:24<00:08,  2.96s/it]\u001b[A\n",
            " 83% 10/12 [00:27<00:05,  2.96s/it]\u001b[A\n",
            " 92% 11/12 [00:30<00:02,  2.97s/it]\u001b[A[2024-09-27 21:02:12,737] [INFO] [accelerate.accelerator.gather_for_metrics:2457] [PID:8730] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                  \n",
            "\u001b[A{'eval_loss': 1.407961368560791, 'eval_runtime': 36.1789, 'eval_samples_per_second': 2.764, 'eval_steps_per_second': 0.359, 'epoch': 1.92}\n",
            " 50% 6/12 [08:12<08:00, 80.11s/it]\n",
            "100% 12/12 [00:33<00:00,  2.97s/it]\u001b[A\n",
            "                                   \u001b[A/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "[2024-09-27 21:02:34,306] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:195] [PID:8730] [RANK:0] gather_len_batches: [13]\u001b[39m\n",
            "{'loss': 1.3767, 'grad_norm': 0.18145377933979034, 'learning_rate': 0.00014, 'epoch': 2.17}\n",
            " 58% 7/12 [08:57<06:41, 80.30s/it]\n",
            "  0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 2/12 [00:02<00:14,  1.50s/it]\u001b[A\n",
            " 25% 3/12 [00:05<00:19,  2.12s/it]\u001b[A\n",
            " 33% 4/12 [00:08<00:19,  2.45s/it]\u001b[A\n",
            " 42% 5/12 [00:12<00:18,  2.69s/it]\u001b[A\n",
            " 50% 6/12 [00:15<00:16,  2.79s/it]\u001b[A\n",
            " 58% 7/12 [00:18<00:14,  2.85s/it]\u001b[A\n",
            " 67% 8/12 [00:21<00:11,  2.90s/it]\u001b[A\n",
            " 75% 9/12 [00:24<00:08,  2.96s/it]\u001b[A\n",
            " 83% 10/12 [00:27<00:05,  2.96s/it]\u001b[A\n",
            " 92% 11/12 [00:30<00:02,  2.97s/it]\u001b[A[2024-09-27 21:03:33,443] [INFO] [accelerate.accelerator.gather_for_metrics:2457] [PID:8730] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                  \n",
            "\u001b[A{'eval_loss': 1.3828611373901367, 'eval_runtime': 36.1933, 'eval_samples_per_second': 2.763, 'eval_steps_per_second': 0.359, 'epoch': 2.17}\n",
            " 58% 7/12 [09:33<06:41, 80.30s/it]\n",
            "100% 12/12 [00:33<00:00,  2.97s/it]\u001b[A\n",
            "{'loss': 1.3624, 'grad_norm': 0.15748918056488037, 'learning_rate': 0.00016, 'epoch': 2.5}\n",
            " 67% 8/12 [10:19<05:24, 81.01s/it]\n",
            "  0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 2/12 [00:02<00:14,  1.50s/it]\u001b[A\n",
            " 25% 3/12 [00:05<00:19,  2.12s/it]\u001b[A\n",
            " 33% 4/12 [00:08<00:19,  2.45s/it]\u001b[A\n",
            " 42% 5/12 [00:12<00:18,  2.69s/it]\u001b[A\n",
            " 50% 6/12 [00:15<00:16,  2.79s/it]\u001b[A\n",
            " 58% 7/12 [00:18<00:14,  2.85s/it]\u001b[A\n",
            " 67% 8/12 [00:21<00:11,  2.90s/it]\u001b[A\n",
            " 75% 9/12 [00:24<00:08,  2.96s/it]\u001b[A\n",
            " 83% 10/12 [00:27<00:05,  2.96s/it]\u001b[A\n",
            " 92% 11/12 [00:30<00:02,  2.97s/it]\u001b[A[2024-09-27 21:04:55,971] [INFO] [accelerate.accelerator.gather_for_metrics:2457] [PID:8730] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                  \n",
            "\u001b[A{'eval_loss': 1.3578858375549316, 'eval_runtime': 36.1765, 'eval_samples_per_second': 2.764, 'eval_steps_per_second': 0.359, 'epoch': 2.5}\n",
            " 67% 8/12 [10:56<05:24, 81.01s/it]\n",
            "100% 12/12 [00:33<00:00,  2.97s/it]\u001b[A\n",
            "{'loss': 1.3157, 'grad_norm': 0.11266450583934784, 'learning_rate': 0.00018, 'epoch': 2.83}\n",
            " 75% 9/12 [11:42<04:04, 81.48s/it]\n",
            "  0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 2/12 [00:02<00:14,  1.49s/it]\u001b[A\n",
            " 25% 3/12 [00:05<00:19,  2.12s/it]\u001b[A\n",
            " 33% 4/12 [00:08<00:19,  2.44s/it]\u001b[A\n",
            " 42% 5/12 [00:12<00:18,  2.69s/it]\u001b[A\n",
            " 50% 6/12 [00:15<00:16,  2.79s/it]\u001b[A\n",
            " 58% 7/12 [00:18<00:14,  2.85s/it]\u001b[A\n",
            " 67% 8/12 [00:21<00:11,  2.90s/it]\u001b[A\n",
            " 75% 9/12 [00:24<00:08,  2.96s/it]\u001b[A\n",
            " 83% 10/12 [00:27<00:05,  2.96s/it]\u001b[A\n",
            " 92% 11/12 [00:30<00:02,  2.97s/it]\u001b[A[2024-09-27 21:06:18,482] [INFO] [accelerate.accelerator.gather_for_metrics:2457] [PID:8730] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                  \n",
            "\u001b[A{'eval_loss': 1.3375614881515503, 'eval_runtime': 36.1909, 'eval_samples_per_second': 2.763, 'eval_steps_per_second': 0.359, 'epoch': 2.83}\n",
            " 75% 9/12 [12:18<04:04, 81.48s/it]\n",
            "100% 12/12 [00:33<00:00,  2.97s/it]\u001b[A\n",
            "                                   \u001b[A/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
            "[2024-09-27 21:06:51,629] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:195] [PID:8730] [RANK:0] gather_len_batches: [13]\u001b[39m\n",
            "{'loss': 1.2913, 'grad_norm': 0.10553040355443954, 'learning_rate': 0.0002, 'epoch': 3.08}\n",
            " 83% 10/12 [13:03<02:42, 81.24s/it]\n",
            "  0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 2/12 [00:02<00:14,  1.50s/it]\u001b[A\n",
            " 25% 3/12 [00:05<00:19,  2.12s/it]\u001b[A\n",
            " 33% 4/12 [00:08<00:19,  2.45s/it]\u001b[A\n",
            " 42% 5/12 [00:12<00:18,  2.70s/it]\u001b[A\n",
            " 50% 6/12 [00:15<00:16,  2.79s/it]\u001b[A\n",
            " 58% 7/12 [00:18<00:14,  2.85s/it]\u001b[A\n",
            " 67% 8/12 [00:21<00:11,  2.90s/it]\u001b[A\n",
            " 75% 9/12 [00:24<00:08,  2.96s/it]\u001b[A\n",
            " 83% 10/12 [00:27<00:05,  2.96s/it]\u001b[A\n",
            " 92% 11/12 [00:30<00:02,  2.97s/it]\u001b[A[2024-09-27 21:07:39,198] [INFO] [accelerate.accelerator.gather_for_metrics:2457] [PID:8730] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.3260382413864136, 'eval_runtime': 36.1864, 'eval_samples_per_second': 2.763, 'eval_steps_per_second': 0.359, 'epoch': 3.08}\n",
            " 83% 10/12 [13:39<02:42, 81.24s/it]\n",
            "100% 12/12 [00:33<00:00,  2.97s/it]\u001b[A\n",
            "{'loss': 1.2814, 'grad_norm': 0.14589698612689972, 'learning_rate': 0.0001, 'epoch': 3.42}\n",
            " 92% 11/12 [14:25<01:21, 81.63s/it]\n",
            "  0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 2/12 [00:02<00:14,  1.50s/it]\u001b[A\n",
            " 25% 3/12 [00:05<00:19,  2.12s/it]\u001b[A\n",
            " 33% 4/12 [00:08<00:19,  2.45s/it]\u001b[A\n",
            " 42% 5/12 [00:12<00:18,  2.70s/it]\u001b[A\n",
            " 50% 6/12 [00:15<00:16,  2.79s/it]\u001b[A\n",
            " 58% 7/12 [00:18<00:14,  2.85s/it]\u001b[A\n",
            " 67% 8/12 [00:21<00:11,  2.90s/it]\u001b[A\n",
            " 75% 9/12 [00:24<00:08,  2.96s/it]\u001b[A\n",
            " 83% 10/12 [00:27<00:05,  2.96s/it]\u001b[A\n",
            " 92% 11/12 [00:30<00:02,  2.97s/it]\u001b[A[2024-09-27 21:09:01,698] [INFO] [accelerate.accelerator.gather_for_metrics:2457] [PID:8730] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.316477656364441, 'eval_runtime': 36.1923, 'eval_samples_per_second': 2.763, 'eval_steps_per_second': 0.359, 'epoch': 3.42}\n",
            " 92% 11/12 [15:01<01:21, 81.63s/it]\n",
            "100% 12/12 [00:33<00:00,  2.97s/it]\u001b[A\n",
            "{'loss': 1.2778, 'grad_norm': 0.1442728191614151, 'learning_rate': 0.0, 'epoch': 3.75}\n",
            "100% 12/12 [15:48<00:00, 81.92s/it]\n",
            "  0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            " 17% 2/12 [00:02<00:14,  1.50s/it]\u001b[A\n",
            " 25% 3/12 [00:05<00:19,  2.12s/it]\u001b[A\n",
            " 33% 4/12 [00:08<00:19,  2.44s/it]\u001b[A\n",
            " 42% 5/12 [00:12<00:18,  2.69s/it]\u001b[A\n",
            " 50% 6/12 [00:15<00:16,  2.79s/it]\u001b[A\n",
            " 58% 7/12 [00:18<00:14,  2.85s/it]\u001b[A\n",
            " 67% 8/12 [00:21<00:11,  2.92s/it]\u001b[A\n",
            " 75% 9/12 [00:24<00:08,  2.97s/it]\u001b[A\n",
            " 83% 10/12 [00:27<00:05,  2.97s/it]\u001b[A\n",
            " 92% 11/12 [00:30<00:02,  2.97s/it]\u001b[A[2024-09-27 21:10:24,344] [INFO] [accelerate.accelerator.gather_for_metrics:2457] [PID:8730] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.3119059801101685, 'eval_runtime': 36.2509, 'eval_samples_per_second': 2.759, 'eval_steps_per_second': 0.359, 'epoch': 3.75}\n",
            "100% 12/12 [16:24<00:00, 81.92s/it]\n",
            "100% 12/12 [00:33<00:00,  2.97s/it]\u001b[A\n",
            "{'train_runtime': 985.7448, 'train_samples_per_second': 7.71, 'train_steps_per_second': 0.012, 'train_loss': 1.3570537169774373, 'epoch': 3.75}\n",
            "100% 12/12 [16:25<00:00, 82.14s/it]\n",
            "[2024-09-27 21:10:34,636] [INFO] [axolotl.train.train:191] [PID:8730] [RANK:0] Training Completed!!! Saving pre-trained model to ./outputs/qlora-out\u001b[39m\n",
            "adapter_model.bin: 100% 101M/101M [00:11<00:00, 8.98MB/s] \n",
            "(PeftModelForCausalLM(   (base_model): LoraModel(     (model): LlamaForCausalLM(       (model): LlamaModel(         (embed_tokens): Embedding(32000, 2048)         (layers): ModuleList(           (0-21): 22 x LlamaDecoderLayer(             (self_attn): LlamaFlashAttention2(               (q_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=2048, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=2048, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()                 (lora_magnitude_vector): ModuleDict()               )               (k_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=2048, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=256, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()                 (lora_magnitude_vector): ModuleDict()               )               (v_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=2048, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=256, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()                 (lora_magnitude_vector): ModuleDict()               )               (o_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=2048, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=2048, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()                 (lora_magnitude_vector): ModuleDict()               )               (rotary_emb): LlamaRotaryEmbedding()             )             (mlp): LlamaMLP(               (gate_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=2048, out_features=5632, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=2048, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=5632, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()                 (lora_magnitude_vector): ModuleDict()               )               (up_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=2048, out_features=5632, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=2048, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=5632, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()                 (lora_magnitude_vector): ModuleDict()               )               (down_proj): lora.Linear4bit(                 (base_layer): Linear4bit(in_features=5632, out_features=2048, bias=False)                 (lora_dropout): ModuleDict(                   (default): Dropout(p=0.05, inplace=False)                 )                 (lora_A): ModuleDict(                   (default): Linear(in_features=5632, out_features=32, bias=False)                 )                 (lora_B): ModuleDict(                   (default): Linear(in_features=32, out_features=2048, bias=False)                 )                 (lora_embedding_A): ParameterDict()                 (lora_embedding_B): ParameterDict()                 (lora_magnitude_vector): ModuleDict()               )               (act_fn): SiLU()             )             (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)             (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)           )         )         (norm): LlamaRMSNorm((2048,), eps=1e-05)         (rotary_emb): LlamaRotaryEmbedding()       )       (lm_head): Linear(in_features=2048, out_features=32000, bias=False)     )   ) ), LlamaTokenizer(name_or_path='TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={ \t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), \t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), \t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), })\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# By using the ! the comand will be executed as a bash command\n",
        "!accelerate launch -m axolotl.cli.train /content/test_axolotl.yaml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m axolotl.cli.merge_lora test_axolotl.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHlICPMmPzs5",
        "outputId": "517a10bb-7445-4164-c044-3b71c2e9ad47"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-27 21:15:40.083798: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-09-27 21:15:40.101025: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-27 21:15:40.121847: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-27 21:15:40.128114: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-27 21:15:40.142996: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-27 21:15:41.320928: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/axolotl/src/axolotl/utils/gradient_checkpointing/unsloth.py:29: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, forward_function, hidden_states, *args):\n",
            "/content/axolotl/src/axolotl/utils/gradient_checkpointing/unsloth.py:40: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, dY):\n",
            "[2024-09-27 21:15:44,335] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-09-27 21:15:44,414] [INFO] [root.spawn:61] [PID:14486] x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -c /tmp/tmp96nz0of_/test.c -o /tmp/tmp96nz0of_/test.o\n",
            "[2024-09-27 21:15:44,432] [INFO] [root.spawn:61] [PID:14486] x86_64-linux-gnu-gcc /tmp/tmp96nz0of_/test.o -laio -o /tmp/tmp96nz0of_/a.out\n",
            "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
            "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
            "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
            "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
            "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
            "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n",
            "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, input, weight, bias=None):\n",
            "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, grad_output):\n",
            "/content/axolotl/src/axolotl/monkeypatch/relora.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
            "  from torch.distributed.optim import ZeroRedundancyOptimizer\n",
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:151: UserWarning: Field \"model_kwargs\" has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n",
            "                                 dP            dP   dP \n",
            "                                 88            88   88 \n",
            "      .d8888b. dP.  .dP .d8888b. 88 .d8888b. d8888P 88 \n",
            "      88'  `88  `8bd8'  88'  `88 88 88'  `88   88   88 \n",
            "      88.  .88  .d88b.  88.  .88 88 88.  .88   88   88 \n",
            "      `88888P8 dP'  `dP `88888P' dP `88888P'   dP   dP \n",
            "                                                       \n",
            "                                                       \n",
            "\n",
            "****************************************\n",
            "**** Axolotl Dependency Versions *****\n",
            "  accelerate: 0.34.2         \n",
            "        peft: 0.13.0         \n",
            "transformers: 4.45.0         \n",
            "         trl: 0.9.6          \n",
            "       torch: 2.4.1+cu121    \n",
            "bitsandbytes: 0.44.0         \n",
            "****************************************\n",
            "[2024-09-27 21:15:47,693] [INFO] [axolotl.utils.config.models.input.check_eval_packing:993] [PID:14486] [RANK:0] setting `remove_unused_columns: false` for when sample_packing and eval_sample_packing don't match\u001b[39m\n",
            "\u001b[33m[2024-09-27 21:15:47,693] [WARNING] [axolotl.utils.config.models.input.check_sample_packing_wo_flash:808] [PID:14486] [RANK:0] sample_packing without flash_attention or sdp_attention does not handle cross-attention.\u001b[39m\n",
            "[2024-09-27 21:15:47,694] [DEBUG] [axolotl.normalize_config:83] [PID:14486] [RANK:0] bf16 support detected, enabling for this configuration.\u001b[39m\n",
            "[2024-09-27 21:15:47,980] [INFO] [axolotl.normalize_config:186] [PID:14486] [RANK:0] GPU memory usage baseline: 0.000GB (+0.331GB misc)\u001b[39m\n",
            "[2024-09-27 21:15:47,980] [INFO] [axolotl.common.cli.load_model_and_tokenizer:51] [PID:14486] [RANK:0] loading tokenizer... TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\u001b[39m\n",
            "[2024-09-27 21:15:48,709] [DEBUG] [axolotl.load_tokenizer:282] [PID:14486] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
            "[2024-09-27 21:15:48,709] [DEBUG] [axolotl.load_tokenizer:283] [PID:14486] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
            "[2024-09-27 21:15:48,709] [DEBUG] [axolotl.load_tokenizer:284] [PID:14486] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
            "[2024-09-27 21:15:48,709] [DEBUG] [axolotl.load_tokenizer:285] [PID:14486] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
            "[2024-09-27 21:15:48,709] [INFO] [axolotl.load_tokenizer:296] [PID:14486] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
            "[2024-09-27 21:15:48,709] [INFO] [axolotl.common.cli.load_model_and_tokenizer:53] [PID:14486] [RANK:0] loading model and (optionally) peft_config...\u001b[39m\n",
            "[2024-09-27 21:15:48,982] [INFO] [axolotl.load_model:430] [PID:14486] [RANK:0] patching llama _prepare_4d_causal_attention_mask*\u001b[39m\n",
            "[2024-09-27 21:15:49,127] [INFO] [accelerate.utils.modeling.get_balanced_memory:1086] [PID:14486] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
            "\u001b[33m[2024-09-27 21:15:50,775] [WARNING] [axolotl.load_model:769] [PID:14486] [RANK:0] increasing model.config.max_position_embeddings from 2048 to 4096\u001b[39m\n",
            "[2024-09-27 21:15:50,776] [INFO] [axolotl.load_model:791] [PID:14486] [RANK:0] GPU memory usage after model load: 2.051GB (+0.088GB cache, +0.513GB misc)\u001b[39m\n",
            "[2024-09-27 21:15:50,794] [INFO] [axolotl.load_model:858] [PID:14486] [RANK:0] converting modules to torch.bfloat16 for flash attention\u001b[39m\n",
            "[2024-09-27 21:15:50,797] [INFO] [axolotl.load_lora:1023] [PID:14486] [RANK:0] found linear modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[39m\n",
            "[2024-09-27 21:15:50,797] [DEBUG] [axolotl.load_lora:1065] [PID:14486] [RANK:0] Loading pretrained PEFT - LoRA\u001b[39m\n",
            "trainable params: 25,231,360 || all params: 1,125,279,744 || trainable%: 2.2422\n",
            "[2024-09-27 21:15:51,370] [INFO] [axolotl.load_model:906] [PID:14486] [RANK:0] GPU memory usage after adapters: 2.145GB (+0.675GB cache, +0.542GB misc)\u001b[39m\n",
            "[2024-09-27 21:15:52,479] [INFO] [axolotl.scripts.do_merge_lora:146] [PID:14486] [RANK:0] running merge of LoRA with base model\u001b[39m\n",
            "Unloading and merging model: 100% 469/469 [00:00<00:00, 3913.70it/s]\n",
            "[2024-09-27 21:15:52,606] [INFO] [axolotl.scripts.do_merge_lora:155] [PID:14486] [RANK:0] saving merged model to: outputs/qlora-out/merged\u001b[39m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token $HF_TOKEN --add-to-git-credential"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFwW8nIDWjS8",
        "outputId": "a4a6a4ef-7828-4b18-b308-9762d26749a3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'\n",
        "!huggingface-cli upload mgfrantz/axolotl-test outputs/qlora-out/merged/ ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBROYTQ2V98K",
        "outputId": "7839b025-249d-4ed3-cc5c-ecb01534d5de"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0% 0/1 [00:00<?, ?it/s]\n",
            "pytorch_model.bin:   0% 0.00/2.20G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model.bin:   1% 16.0M/2.20G [00:05<11:59, 3.03MB/s]\u001b[A\n",
            "pytorch_model.bin:   1% 32.0M/2.20G [00:07<07:44, 4.67MB/s]\u001b[A\n",
            "pytorch_model.bin:   2% 48.0M/2.20G [00:07<04:19, 8.28MB/s]\u001b[A\n",
            "pytorch_model.bin:   3% 64.0M/2.20G [00:08<03:01, 11.8MB/s]\u001b[A\n",
            "pytorch_model.bin:   4% 80.0M/2.20G [00:10<04:01, 8.76MB/s]\u001b[A\n",
            "pytorch_model.bin:   4% 96.0M/2.20G [00:10<02:50, 12.3MB/s]\u001b[A\n",
            "pytorch_model.bin:   5% 112M/2.20G [00:11<02:09, 16.1MB/s] \u001b[A\n",
            "pytorch_model.bin:   6% 128M/2.20G [00:11<01:53, 18.2MB/s]\u001b[A\n",
            "pytorch_model.bin:   8% 176M/2.20G [00:12<00:52, 38.2MB/s]\u001b[A\n",
            "pytorch_model.bin:   9% 192M/2.20G [00:12<00:47, 42.3MB/s]\u001b[A\n",
            "pytorch_model.bin:   9% 208M/2.20G [00:12<00:50, 39.8MB/s]\u001b[A\n",
            "pytorch_model.bin:  10% 224M/2.20G [00:13<00:42, 46.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  11% 240M/2.20G [00:13<00:35, 55.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  12% 272M/2.20G [00:13<00:26, 73.8MB/s]\u001b[A\n",
            "pytorch_model.bin:  15% 336M/2.20G [00:13<00:14, 132MB/s] \u001b[A\n",
            "pytorch_model.bin:  17% 384M/2.20G [00:13<00:10, 169MB/s]\u001b[A\n",
            "pytorch_model.bin:  19% 416M/2.20G [00:14<00:13, 132MB/s]\u001b[A\n",
            "pytorch_model.bin:  20% 448M/2.20G [00:14<00:11, 147MB/s]\u001b[A\n",
            "pytorch_model.bin:  23% 496M/2.20G [00:14<00:14, 114MB/s]\u001b[A\n",
            "pytorch_model.bin:  24% 528M/2.20G [00:15<00:13, 128MB/s]\u001b[A\n",
            "pytorch_model.bin:  26% 576M/2.20G [00:15<00:10, 149MB/s]\u001b[A\n",
            "pytorch_model.bin:  28% 608M/2.20G [00:15<00:11, 141MB/s]\u001b[A\n",
            "pytorch_model.bin:  30% 656M/2.20G [00:15<00:08, 187MB/s]\u001b[A\n",
            "pytorch_model.bin:  31% 688M/2.20G [00:15<00:07, 192MB/s]\u001b[A\n",
            "pytorch_model.bin:  33% 720M/2.20G [00:16<00:08, 166MB/s]\u001b[A\n",
            "pytorch_model.bin:  34% 752M/2.20G [00:16<00:08, 172MB/s]\u001b[A\n",
            "pytorch_model.bin:  38% 832M/2.20G [00:16<00:05, 268MB/s]\u001b[A\n",
            "pytorch_model.bin:  40% 880M/2.20G [00:16<00:04, 297MB/s]\u001b[A\n",
            "pytorch_model.bin:  42% 928M/2.20G [00:16<00:04, 314MB/s]\u001b[A\n",
            "pytorch_model.bin:  44% 976M/2.20G [00:16<00:03, 306MB/s]\u001b[A\n",
            "pytorch_model.bin:  47% 1.02G/2.20G [00:16<00:03, 319MB/s]\u001b[A\n",
            "pytorch_model.bin:  49% 1.07G/2.20G [00:17<00:03, 307MB/s]\u001b[A\n",
            "pytorch_model.bin:  52% 1.14G/2.20G [00:17<00:02, 376MB/s]\u001b[A\n",
            "pytorch_model.bin:  54% 1.18G/2.20G [00:17<00:02, 341MB/s]\u001b[A\n",
            "pytorch_model.bin:  57% 1.25G/2.20G [00:17<00:02, 387MB/s]\u001b[A\n",
            "pytorch_model.bin:  61% 1.34G/2.20G [00:17<00:01, 503MB/s]\u001b[A\n",
            "pytorch_model.bin:  64% 1.41G/2.20G [00:17<00:01, 416MB/s]\u001b[A\n",
            "pytorch_model.bin:  66% 1.46G/2.20G [00:18<00:02, 359MB/s]\u001b[A\n",
            "pytorch_model.bin:  73% 1.60G/2.20G [00:18<00:01, 556MB/s]\u001b[A\n",
            "pytorch_model.bin:  78% 1.71G/2.20G [00:18<00:00, 639MB/s]\u001b[A\n",
            "pytorch_model.bin:  81% 1.79G/2.20G [00:18<00:00, 600MB/s]\u001b[A\n",
            "pytorch_model.bin:  85% 1.87G/2.20G [00:18<00:00, 467MB/s]\u001b[A\n",
            "pytorch_model.bin:  88% 1.94G/2.20G [00:18<00:00, 476MB/s]\u001b[A\n",
            "pytorch_model.bin:  95% 2.08G/2.20G [00:19<00:00, 603MB/s]\u001b[A\n",
            "pytorch_model.bin: 2.21GB [00:19, 115MB/s]\n",
            "100% 1/1 [00:19<00:00, 19.59s/it]\n",
            "Removing 3 file(s) from commit that have not changed.\n",
            "https://huggingface.co/mgfrantz/axolotl-test/tree/main/.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QwVyqMoLdnqt"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "elKUHINDTyU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installs and imports"
      ],
      "metadata": {
        "id": "8ix_exAdUNcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqqq peft transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "9TWlCRYxMH9s"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "A7o15JZD29Z_"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from rich import print"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model"
      ],
      "metadata": {
        "id": "dpsVpe_rUQu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if model in vars(): del model;\n",
        "BASE_CKPT = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
        "ADAPTER_CKPT = \"mgfrantz/axolotl-test\"\n",
        "config = PeftConfig.from_pretrained(ADAPTER_CKPT)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(BASE_CKPT)\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_CKPT)\n",
        "model = PeftModel.from_pretrained(base_model, ADAPTER_CKPT).eval().cuda()"
      ],
      "metadata": {
        "id": "d68JOyYST3-b"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if model in vars(): del model;\n",
        "model = AutoModelForCausalLM.from_pretrained(ADAPTER_CKPT).eval().cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ff55f970e7434d0b8bebef5868366642",
            "740c8bd2bf1847cd848be53c61edee6e",
            "91c824713abb4b2290978aec9183b52f",
            "4342b1f3d81f4370a7af7b3baf387436",
            "23a328543e1b4b58b8b74f8a80fbd3c9",
            "56b5d16b0fb54767b86c3f6788783b69",
            "3206ffd0927a4117982e69b96154b7a9",
            "5af71d5231a64ca1b93fe18011cac7a6",
            "08114a9e49e946f2a1aa49d69168d8fe",
            "ed4c5d6deadc42f2bfff19c2e2db5414",
            "e11b377fb2104ecbb950b7bf08f06df8"
          ]
        },
        "id": "cGyHD0yPX9NW",
        "outputId": "9da9b577-983b-424c-f95b-f079bb5429af"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/750 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff55f970e7434d0b8bebef5868366642"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_instruction(instruction):\n",
        "    text = f\"\"\"\\\n",
        "Below is an instruction that describes a task. \\\n",
        "Write a response that appropriately completes the request. \\\n",
        "### Instruction: {instruction} \\\n",
        "### Response: \\\n",
        "\"\"\"\n",
        "    return text"
      ],
      "metadata": {
        "id": "IkEMSmD5UyQP"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_response(text):\n",
        "    return text.split(\"### Response:\")[1].strip()"
      ],
      "metadata": {
        "id": "i0zhDWrKYgFh"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def do_inference(instruction):\n",
        "    encoded = tokenizer(format_instruction(instruction), return_tensors=\"pt\")\n",
        "    encoded = {\n",
        "        k: v.to(\"cuda\") for k, v in encoded.items()\n",
        "    }\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(**encoded, max_new_tokens=500, top_k=3)\n",
        "\n",
        "    output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    return parse_response(output_text)\n"
      ],
      "metadata": {
        "id": "-zks0jCwUYBx"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = do_inference(\"Tell me how to bake a cake.\")"
      ],
      "metadata": {
        "id": "fQH7bsDwNOYl"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "CPJvaIuDYMD7",
        "outputId": "964a4c6f-243b-43d9-f052-b57100c8c048"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m1\u001b[0m. First, you need to gather all the ingredients. You will need flour, baking powder, baking soda, salt, sugar, \n",
              "butter, eggs, and vanilla extract. You will also need a cake pan or tube pan. \u001b[1;36m2\u001b[0m. Next, you will need to preheat the\n",
              "oven to \u001b[1;36m350\u001b[0m degrees Fahrenheit. You will also need to grease the pan or tube pan with butter or oil. \u001b[1;36m3\u001b[0m. Once the \n",
              "oven is preheated, you will need to place the ingredients in the order listed above. You will need to mix them \n",
              "together until they are well-combined. \u001b[1;36m4\u001b[0m. Once the ingredients are mixed, you will need to pour the mixture into \n",
              "the pan or tube pan. You will need to bake the cake for about \u001b[1;36m45\u001b[0m minutes, or until it is golden brown and a \n",
              "toothpick inserted into the center of the cake comes out clean. \u001b[1;36m5\u001b[0m. Once the cake is baked, you will need to let it \n",
              "cool for at least \u001b[1;36m10\u001b[0m minutes before serving. You can also serve it with a drizzle of icing or sprinkles. Enjoy!\n",
              "How to Make a Cake in the Oven?\n",
              "How to Make a Cake in the Oven? The process of making a cake in the oven is simple and straightforward. First, you \n",
              "need to prepare the ingredients and then bake the cake in the oven. The process of making a cake in the oven is \n",
              "similar to that of making a cake in a stand mixer. You will need to combine the ingredients, mix them together, and\n",
              "then bake them in the oven. Once the cake is baked, you can enjoy it!\n",
              "How to Make a Cake in the Microwave?\n",
              "How to Make a Cake in the Microwave? The process of making a cake in the microwave is simple and easy. All you need\n",
              "to do is follow these steps: \u001b[1;36m1\u001b[0m. Place the cake mix in the microwave and microwave for \u001b[1;36m30\u001b[0m seconds. \u001b[1;36m2\u001b[0m. Remove the \n",
              "cake from the microwave and let it cool for \u001b[1;36m10\u001b[0m minutes. \u001b[1;36m3\u001b[0m. Spread the icing on the cake and enjoy!\n",
              "How to Make\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. First, you need to gather all the ingredients. You will need flour, baking powder, baking soda, salt, sugar, \n",
              "butter, eggs, and vanilla extract. You will also need a cake pan or tube pan. <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Next, you will need to preheat the\n",
              "oven to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">350</span> degrees Fahrenheit. You will also need to grease the pan or tube pan with butter or oil. <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Once the \n",
              "oven is preheated, you will need to place the ingredients in the order listed above. You will need to mix them \n",
              "together until they are well-combined. <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. Once the ingredients are mixed, you will need to pour the mixture into \n",
              "the pan or tube pan. You will need to bake the cake for about <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">45</span> minutes, or until it is golden brown and a \n",
              "toothpick inserted into the center of the cake comes out clean. <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>. Once the cake is baked, you will need to let it \n",
              "cool for at least <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> minutes before serving. You can also serve it with a drizzle of icing or sprinkles. Enjoy!\n",
              "How to Make a Cake in the Oven?\n",
              "How to Make a Cake in the Oven? The process of making a cake in the oven is simple and straightforward. First, you \n",
              "need to prepare the ingredients and then bake the cake in the oven. The process of making a cake in the oven is \n",
              "similar to that of making a cake in a stand mixer. You will need to combine the ingredients, mix them together, and\n",
              "then bake them in the oven. Once the cake is baked, you can enjoy it!\n",
              "How to Make a Cake in the Microwave?\n",
              "How to Make a Cake in the Microwave? The process of making a cake in the microwave is simple and easy. All you need\n",
              "to do is follow these steps: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Place the cake mix in the microwave and microwave for <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span> seconds. <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Remove the \n",
              "cake from the microwave and let it cool for <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> minutes. <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Spread the icing on the cake and enjoy!\n",
              "How to Make\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wtljntThYnyU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMtIBZjH6mBiUjT3MCsQMn6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ff55f970e7434d0b8bebef5868366642": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_740c8bd2bf1847cd848be53c61edee6e",
              "IPY_MODEL_91c824713abb4b2290978aec9183b52f",
              "IPY_MODEL_4342b1f3d81f4370a7af7b3baf387436"
            ],
            "layout": "IPY_MODEL_23a328543e1b4b58b8b74f8a80fbd3c9"
          }
        },
        "740c8bd2bf1847cd848be53c61edee6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56b5d16b0fb54767b86c3f6788783b69",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3206ffd0927a4117982e69b96154b7a9",
            "value": "config.json:â€‡100%"
          }
        },
        "91c824713abb4b2290978aec9183b52f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5af71d5231a64ca1b93fe18011cac7a6",
            "max": 750,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08114a9e49e946f2a1aa49d69168d8fe",
            "value": 750
          }
        },
        "4342b1f3d81f4370a7af7b3baf387436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed4c5d6deadc42f2bfff19c2e2db5414",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e11b377fb2104ecbb950b7bf08f06df8",
            "value": "â€‡750/750â€‡[00:00&lt;00:00,â€‡65.4kB/s]"
          }
        },
        "23a328543e1b4b58b8b74f8a80fbd3c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56b5d16b0fb54767b86c3f6788783b69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3206ffd0927a4117982e69b96154b7a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5af71d5231a64ca1b93fe18011cac7a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08114a9e49e946f2a1aa49d69168d8fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed4c5d6deadc42f2bfff19c2e2db5414": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e11b377fb2104ecbb950b7bf08f06df8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}