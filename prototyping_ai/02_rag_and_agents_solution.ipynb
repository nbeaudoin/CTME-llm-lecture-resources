{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "![](https://www.dailydoseofds.com/content/images/2024/10/rag.gif)\n",
    "Source: [Daily Dose of DS](https://www.dailydoseofds.com/a-crash-course-on-building-rag-systems-part-1-with-implementations/)\n",
    "\n",
    "Overview:\n",
    "- Introduction to RAG with LlamaIndex\n",
    "- Data ingestion\n",
    "  - PDF\n",
    "  - Web pages\n",
    "  - Code\n",
    "- Data splitting\n",
    "  - Token splitting\n",
    "  - Sentence splitting\n",
    "  - Structured data splitting\n",
    "  - Semantic chunking\n",
    "- Vectorization\n",
    "  - Embeddings\n",
    "  - Vector storage\n",
    "- Retrieval\n",
    "  - Keyword search\n",
    "  - Vector search\n",
    "  - Hybrid search\n",
    "- Advanced methods\n",
    "  - Query rewriting\n",
    "  - Multi-hop retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rich extension is already loaded. To reload it, use:\n",
      "  %reload_ext rich\n"
     ]
    }
   ],
   "source": [
    "%load_ext rich\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to RAG with LlamaIndex\n",
    "\n",
    "LlamaIndex is a library for working with large language models.\n",
    "One of its main strengths is its ability to ingest documents into a vector index and use them to answer questions.\n",
    "This is known as Retrieval Augmented Generation (RAG).\n",
    "\n",
    "To start, we will use a low-code, high-level abstraction to build a basic PDF question-answering system.\n",
    "We will read in PDFs, split them into chunks, embed them, and store them in a vector database.\n",
    "Then, we will use an abstraction known as a `QueryEngine` that implements RAG to answer questions about the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Not in colab - using local environment variables.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Not in colab - using local environment variables.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If we're in colab, use userdata to get the OPENAI_API_KEY\n",
    "import os\n",
    "from rich import print\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    print(\"Colab detected - setting up environment\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
    "    %pip install llama-index \\\n",
    "        llama-index-readers-web \\\n",
    "        thefuzz \\\n",
    "        gradio \\\n",
    "        chromadb \\\n",
    "        llama-index-embeddings-huggingface \\\n",
    "        llama-index-vector-stores-chroma \\\n",
    "        llama-index-retrievers-bm25 \\\n",
    "        llama-index-llms-gemini \\\n",
    "        docling \\\n",
    "        llama-index-readers-docling \\\n",
    "        llama-index-node-parser-docling \\\n",
    "        PyStemmer\n",
    "        \n",
    "except:\n",
    "    print(\"Not in colab - using local environment variables.\")\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv(\"../.env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">PDF already exists at data/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2407.21783</span>.pdf\n",
       "</pre>\n"
      ],
      "text/plain": [
       "PDF already exists at data/\u001b[1;36m2407.21783\u001b[0m.pdf\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "data_dir = \"data\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# Download the PDF file\n",
    "pdf_url = \"https://arxiv.org/pdf/2407.21783\"\n",
    "pdf_path = os.path.join(data_dir, \"2407.21783.pdf\")\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    response = requests.get(pdf_url)\n",
    "    with open(pdf_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded PDF to {pdf_path}\")\n",
    "else:\n",
    "    print(f\"PDF already exists at {pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is to load the data.\n",
    "For general documents like PDFs, LlamaIndex provides a nice abstraction known as a `SimpleDirectoryReader` that can load data from a directory.\n",
    "In the cell below, we use it to load the data from the `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(data_dir).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first document.\n",
    "Most prominently, we get the text of the document.\n",
    "By default, we also get a lot of useful information, like the page number file name, file path, type, size, etc.\n",
    "When we load lots of documents, this type of information becomes important to keep track of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id_</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'d2ffb7ab-de1c-4d12-b2f3-0f28f81ca4a9'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">embedding</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'page_label'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'file_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2407.21783.pdf'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'file_path'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/Users/michaelfrantz/CTME-llm-lecture-resources/prototyping_ai/data/2407.21783.pdf'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'file_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'application/pdf'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'file_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9832488</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'creation_date'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-11-14'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'last_modified_date'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-11-14'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">excluded_embed_metadata_keys</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'file_name'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'file_type'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'file_size'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'creation_date'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'last_modified_date'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'last_accessed_date'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">excluded_llm_metadata_keys</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'file_name'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'file_type'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'file_size'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'creation_date'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'last_modified_date'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'last_accessed_date'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">relationships</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata_template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{key}: {value}'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata_separator</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\\n'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The Llama 3 Herd of Models\\nLlama Team, AI @ Meta 1\\n1A detailed contributor list can be found in the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">appendix of this paper.\\nModern artificial intelligence (AI) systems are powered by foundation models. This paper </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">presents a\\nnew set of foundation models, called Llama 3. It is a herd of language models that natively </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">support\\nmultilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with\\n405B </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">parameters and a context window of up to 128K tokens. This paper presents an extensive\\nempirical evaluation of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Llama 3. We find that Llama 3 delivers comparable quality to leading language\\nmodels such as GPT-4 on a plethora </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of tasks. We publicly release Llama 3, including pre-trained and\\npost-trained versions of the 405B parameter </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language model and our Llama Guard 3 model for input\\nand output safety. The paper also presents the results of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">experiments in which we integrate image,\\nvideo, and speech capabilities into Llama 3 via a compositional approach.</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">We observe this approach\\nperforms competitively with the state-of-the-art on image, video, and speech recognition </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tasks. The\\nresulting models are not yet being broadly released as they are still under development.\\nDate: July </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">23, 2024\\nWebsite: https://llama.meta.com/\\n1 Introduction\\nFoundation models are general models of language, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">vision, speech, and/or other modalities that are designed\\nto support a large variety of AI tasks. They form the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">basis of many modern AI systems.\\nThe development of modern foundation models consists of two main stages:(1) a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pre-training stage in which\\nthe model is trained at massive scale using straightforward tasks such as next-word </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prediction or captioning\\nand (2) a post-training stage in which the model is tuned to follow instructions, align </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">with human preferences,\\nand improve specific capabilities (for example, coding and reasoning).\\nIn this paper, we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">present a new set of foundation models for language, calledLlama 3. The Llama 3 Herd\\nof models natively supports </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">multilinguality, coding, reasoning, and tool usage. Our largest model is dense\\nTransformer with 405B parameters, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">processing information in a context window of up to 128K tokens. Each\\nmember of the herd is listed in Table 1. All</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the results presented in this paper are for the Llama 3.1 models,\\nwhich we will refer to as Llama 3 throughout for</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">brevity.\\nWe believe there are three key levers in the development of high-quality foundation models: data, scale, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and\\nmanaging complexity. We seek to optimize for these three levers in our development process:\\nâ€¢ Data. Compared </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to prior versions of Llama (Touvron et al., 2023a,b), we improved both the quantity </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and\\nqualityofthedataweuseforpre-trainingandpost-training. Theseimprovementsincludethedevelopment\\nof more careful </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pre-processing and curation pipelines for pre-training data and the development of more\\nrigorous quality assurance</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and filtering approaches for post-training data. We pre-train Llama 3 on a\\ncorpus of about 15T multilingual </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tokens, compared to 1.8T tokens for Llama 2.\\nâ€¢ Scale. We train a model at far larger scale than previous Llama </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models: our flagship language model was\\npre-trained using3.8 Ã— 1025 FLOPs, almost50Ã— more than the largest version</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of Llama 2. Specifically,\\nwe pre-trained a flagship model with 405B trainable parameters on 15.6T text tokens. As </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">expected per\\n1\\narXiv:2407.21783v2  [cs.AI]  15 Aug 2024'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">mimetype</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text/plain'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">start_char_idx</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">end_char_idx</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata_seperator</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\\n'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">text_template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{metadata_str}\\n\\n{content}'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mid_\u001b[0m=\u001b[32m'd2ffb7ab-de1c-4d12-b2f3-0f28f81ca4a9'\u001b[0m,\n",
       "    \u001b[33membedding\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'page_label'\u001b[0m: \u001b[32m'1'\u001b[0m,\n",
       "        \u001b[32m'file_name'\u001b[0m: \u001b[32m'2407.21783.pdf'\u001b[0m,\n",
       "        \u001b[32m'file_path'\u001b[0m: \u001b[32m'/Users/michaelfrantz/CTME-llm-lecture-resources/prototyping_ai/data/2407.21783.pdf'\u001b[0m,\n",
       "        \u001b[32m'file_type'\u001b[0m: \u001b[32m'application/pdf'\u001b[0m,\n",
       "        \u001b[32m'file_size'\u001b[0m: \u001b[1;36m9832488\u001b[0m,\n",
       "        \u001b[32m'creation_date'\u001b[0m: \u001b[32m'2024-11-14'\u001b[0m,\n",
       "        \u001b[32m'last_modified_date'\u001b[0m: \u001b[32m'2024-11-14'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mexcluded_embed_metadata_keys\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[32m'file_name'\u001b[0m,\n",
       "        \u001b[32m'file_type'\u001b[0m,\n",
       "        \u001b[32m'file_size'\u001b[0m,\n",
       "        \u001b[32m'creation_date'\u001b[0m,\n",
       "        \u001b[32m'last_modified_date'\u001b[0m,\n",
       "        \u001b[32m'last_accessed_date'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mexcluded_llm_metadata_keys\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[32m'file_name'\u001b[0m,\n",
       "        \u001b[32m'file_type'\u001b[0m,\n",
       "        \u001b[32m'file_size'\u001b[0m,\n",
       "        \u001b[32m'creation_date'\u001b[0m,\n",
       "        \u001b[32m'last_modified_date'\u001b[0m,\n",
       "        \u001b[32m'last_accessed_date'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mrelationships\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mmetadata_template\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32mkey\u001b[0m\u001b[32m}\u001b[0m\u001b[32m: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mvalue\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "    \u001b[33mmetadata_separator\u001b[0m=\u001b[32m'\\n'\u001b[0m,\n",
       "    \u001b[33mtext\u001b[0m=\u001b[32m'The Llama 3 Herd of Models\\nLlama Team, AI @ Meta 1\\n1A detailed contributor list can be found in the \u001b[0m\n",
       "\u001b[32mappendix of this paper.\\nModern artificial intelligence \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAI\u001b[0m\u001b[32m)\u001b[0m\u001b[32m systems are powered by foundation models. This paper \u001b[0m\n",
       "\u001b[32mpresents a\\nnew set of foundation models, called Llama 3. It is a herd of language models that natively \u001b[0m\n",
       "\u001b[32msupport\\nmultilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with\\n405B \u001b[0m\n",
       "\u001b[32mparameters and a context window of up to 128K tokens. This paper presents an extensive\\nempirical evaluation of \u001b[0m\n",
       "\u001b[32mLlama 3. We find that Llama 3 delivers comparable quality to leading language\\nmodels such as GPT-4 on a plethora \u001b[0m\n",
       "\u001b[32mof tasks. We publicly release Llama 3, including pre-trained and\\npost-trained versions of the 405B parameter \u001b[0m\n",
       "\u001b[32mlanguage model and our Llama Guard 3 model for input\\nand output safety. The paper also presents the results of \u001b[0m\n",
       "\u001b[32mexperiments in which we integrate image,\\nvideo, and speech capabilities into Llama 3 via a compositional approach.\u001b[0m\n",
       "\u001b[32mWe observe this approach\\nperforms competitively with the state-of-the-art on image, video, and speech recognition \u001b[0m\n",
       "\u001b[32mtasks. The\\nresulting models are not yet being broadly released as they are still under development.\\nDate: July \u001b[0m\n",
       "\u001b[32m23, 2024\\nWebsite: https://llama.meta.com/\\n1 Introduction\\nFoundation models are general models of language, \u001b[0m\n",
       "\u001b[32mvision, speech, and/or other modalities that are designed\\nto support a large variety of AI tasks. They form the \u001b[0m\n",
       "\u001b[32mbasis of many modern AI systems.\\nThe development of modern foundation models consists of two main stages:\u001b[0m\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m a \u001b[0m\n",
       "\u001b[32mpre-training stage in which\\nthe model is trained at massive scale using straightforward tasks such as next-word \u001b[0m\n",
       "\u001b[32mprediction or captioning\\nand \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m a post-training stage in which the model is tuned to follow instructions, align \u001b[0m\n",
       "\u001b[32mwith human preferences,\\nand improve specific capabilities \u001b[0m\u001b[32m(\u001b[0m\u001b[32mfor example, coding and reasoning\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\nIn this paper, we \u001b[0m\n",
       "\u001b[32mpresent a new set of foundation models for language, calledLlama 3. The Llama 3 Herd\\nof models natively supports \u001b[0m\n",
       "\u001b[32mmultilinguality, coding, reasoning, and tool usage. Our largest model is dense\\nTransformer with 405B parameters, \u001b[0m\n",
       "\u001b[32mprocessing information in a context window of up to 128K tokens. Each\\nmember of the herd is listed in Table 1. All\u001b[0m\n",
       "\u001b[32mthe results presented in this paper are for the Llama 3.1 models,\\nwhich we will refer to as Llama 3 throughout for\u001b[0m\n",
       "\u001b[32mbrevity.\\nWe believe there are three key levers in the development of high-quality foundation models: data, scale, \u001b[0m\n",
       "\u001b[32mand\\nmanaging complexity. We seek to optimize for these three levers in our development process:\\nâ€¢ Data. Compared \u001b[0m\n",
       "\u001b[32mto prior versions of Llama \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTouvron et al., 2023a,b\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, we improved both the quantity \u001b[0m\n",
       "\u001b[32mand\\nqualityofthedataweuseforpre-trainingandpost-training. Theseimprovementsincludethedevelopment\\nof more careful \u001b[0m\n",
       "\u001b[32mpre-processing and curation pipelines for pre-training data and the development of more\\nrigorous quality assurance\u001b[0m\n",
       "\u001b[32mand filtering approaches for post-training data. We pre-train Llama 3 on a\\ncorpus of about 15T multilingual \u001b[0m\n",
       "\u001b[32mtokens, compared to 1.8T tokens for Llama 2.\\nâ€¢ Scale. We train a model at far larger scale than previous Llama \u001b[0m\n",
       "\u001b[32mmodels: our flagship language model was\\npre-trained using3.8 Ã— 1025 FLOPs, almost50Ã— more than the largest version\u001b[0m\n",
       "\u001b[32mof Llama 2. Specifically,\\nwe pre-trained a flagship model with 405B trainable parameters on 15.6T text tokens. As \u001b[0m\n",
       "\u001b[32mexpected per\\n1\\narXiv:2407.21783v2  \u001b[0m\u001b[32m[\u001b[0m\u001b[32mcs.AI\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  15 Aug 2024'\u001b[0m,\n",
       "    \u001b[33mmimetype\u001b[0m=\u001b[32m'text/plain'\u001b[0m,\n",
       "    \u001b[33mstart_char_idx\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mend_char_idx\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mmetadata_seperator\u001b[0m=\u001b[32m'\\n'\u001b[0m,\n",
       "    \u001b[33mtext_template\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32mmetadata_str\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32mcontent\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded the data, we need to vectorize it.\n",
    "We will use a combination of an embedding model and a vector database to store the vectors.\n",
    "In the cell below, we use a HuggingFace embedding model to embed the documents.\n",
    "We also use torch to determine the device to use for the embedding model (`mps` for Mac GPUs, `cuda` for Nvidia GPUs, and `cpu` otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from torch.backends.mps import is_available as is_mps_available\n",
    "from torch.cuda import is_available as is_cuda_available\n",
    "\n",
    "if is_mps_available():\n",
    "    device = \"mps\"\n",
    "elif is_cuda_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\", device=device)\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's set up our RAG query engine.\n",
    "If we want to perform simple question-answering, we can use the `as_query_engine` method.\n",
    "If we want to perform chat with history, we can use the `as_chat_engine` method.\n",
    "We can see both below ðŸ‘‡.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "Settings.llm = llm # set gpt-4o-mini as the default llm\n",
    "\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "chat_engine = index.as_chat_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the query enine works.\n",
    "We start by passing it a question, then it uses the retriever to find the most relevant documents.\n",
    "Finally, it uses the LLM to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"How many new Llama models models are mentioned in the paper?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we see the response text.\n",
    "But there's also some additional information that we can access, including the source nodes.\n",
    "These are the nodes that the retriever used to answer the question.\n",
    "We can see that the retriever found several nodes that are relevant to the question, then the LLM used at least one of them to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The paper mentions a total of three new Llama models: Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> with 8B parameters, Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> with 70B parameters, and \n",
       "Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> with 405B parameters.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The paper mentions a total of three new Llama models: Llama \u001b[1;36m3\u001b[0m with 8B parameters, Llama \u001b[1;36m3\u001b[0m with 70B parameters, and \n",
       "Llama \u001b[1;36m3\u001b[0m with 405B parameters.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the first source node, we can see that it is a node that contains part of the document that is relevant to the question.\n",
    "It also contains the `score`, which is the similarity between the question and the node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">NodeWithScore</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">node</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextNode</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id_</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'7dee73d7-f185-4dd0-84d9-1830c15d286d'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">embedding</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'page_label'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'file_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2407.21783.pdf'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'file_path'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/Users/michaelfrantz/CTME-llm-lecture-resources/prototyping_ai/data/2407.21783.pdf'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'file_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'application/pdf'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'file_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9832488</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creation_date'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-11-14'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'last_modified_date'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-11-14'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">excluded_embed_metadata_keys</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'file_name'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'file_type'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'file_size'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creation_date'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'last_modified_date'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'last_accessed_date'</span>\n",
       "        <span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">excluded_llm_metadata_keys</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'file_name'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'file_type'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'file_size'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creation_date'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'last_modified_date'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'last_accessed_date'</span>\n",
       "        <span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">relationships</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">NodeRelationship.SOURCE:</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'1'</span><span style=\"font-weight: bold\">&gt;</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RelatedNodeInfo</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">node_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'c2aa5e07-1c64-44f6-8701-f3df53873209'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">node_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'4'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'page_label'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'file_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2407.21783.pdf'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'file_path'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'/Users/michaelfrantz/CTME-llm-lecture-resources/prototyping_ai/data/2407.21783.pdf'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'file_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'application/pdf'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'file_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9832488</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'creation_date'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-11-14'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'last_modified_date'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-11-14'</span>\n",
       "                <span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">hash</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2ea90099062b1dd3bee8c1ae15c1a08e55026fc535e90e0d917ed1f6139bfd74'</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata_template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{key}: {value}'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata_separator</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\\n'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Finetuned Multilingual Long context Tool use Release\\nLlama 3 8B âœ— âœ— 1 âœ— âœ— April 2024\\nLlama 3 8B </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Instruct âœ“ âœ— âœ— âœ— April 2024\\nLlama 3 70B âœ— âœ— 1 âœ— âœ— April 2024\\nLlama 3 70B Instruct âœ“ âœ— âœ— âœ— April 2024\\nLlama 3.1 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">8B âœ— âœ“ âœ“ âœ— July 2024\\nLlama 3.1 8B Instruct âœ“ âœ“ âœ“ âœ“ July 2024\\nLlama 3.1 70B âœ— âœ“ âœ“ âœ— July 2024\\nLlama 3.1 70B </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Instruct âœ“ âœ“ âœ“ âœ“ July 2024\\nLlama 3.1 405B âœ— âœ“ âœ“ âœ— July 2024\\nLlama 3.1 405B Instruct âœ“ âœ“ âœ“ âœ“ July 2024\\nTable 1 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Overview of the Llama 3 Herd of models. All results in this paper are for the Llama 3.1 models.\\nscaling laws for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">foundation models, our flagship model outperforms smaller models trained using the\\nsame procedure. While our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">scaling laws suggest our flagship model is an approximately compute-optimal\\nsize for our training budget, we also </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">train our smaller models for much longer than is compute-optimal.\\nThe resulting models perform better than </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">compute-optimal models at the same inference budget. We\\nuse the flagship model to further improve the quality of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">those smaller models during post-training.\\nâ€¢ Managing complexity. We make design choices that seek to maximize our</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ability to scale the model\\ndevelopment process. For example, we opt for a standard dense Transformer model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">architecture (Vaswani\\net al., 2017) with minor adaptations, rather than for a mixture-of-experts model (Shazeer et</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">al., 2017)\\nto maximize training stability. Similarly, we adopt a relatively simple post-training procedure </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">based\\non supervised finetuning (SFT), rejection sampling (RS), and direct preference optimization (DPO;\\nRafailov </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">et al. (2023)) as opposed to more complex reinforcement learning algorithms (Ouyang et al.,\\n2022; Schulman et al.,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">2017) that tend to be less stable and harder to scale.\\nThe result of our work is Llama 3: a herd of three </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">multilingual1 language models with 8B, 70B, and 405B\\nparameters. We evaluate the performance of Llama 3 on a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">plethora of benchmark datasets that span a wide\\nrange of language understanding tasks. In addition, we perform </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extensive human evaluations that compare\\nLlama 3 with competing models. An overview of the performance of the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">flagship Llama 3 model on key\\nbenchmarks is presented in Table 2. Our experimental evaluation suggests that our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">flagship model performs\\non par with leading language models such as GPT-4 (OpenAI, 2023a) across a variety of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tasks, and is close to\\nmatching the state-of-the-art. Our smaller models are best-in-class, outperforming </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">alternative models with\\nsimilar numbers of parameters (Bai et al., 2023; Jiang et al., 2023). Llama 3 also </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">delivers a much better\\nbalance between helpfulness and harmlessness than its predecessor (Touvron et al., 2023b). </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">We present a\\ndetailed analysis of the safety of Llama 3 in Section 5.4.\\nWe are publicly releasing all three Llama</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">3 models under an updated version of the Llama 3 Community License;\\nsee https://llama.meta.com. This includes </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pre-trained and post-trained versions of our 405B parameter\\nlanguage model and a new version of our Llama Guard </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model (Inan et al., 2023) for input and output safety.\\nWe hope that the open release of a flagship model will spur</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a wave of innovation in the research community,\\nand accelerate a responsible path towards the development of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">artificial general intelligence (AGI).\\nAs part of the Llama 3 development process we also develop multimodal </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extensions to the models, enabling\\nimage recognition, video recognition, and speech understanding capabilities. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">These models are still under\\nactive development and not yet ready for release. In addition to our language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">modeling results, the paper\\npresents results of our initial experiments with those multimodal models.\\n1The Llama </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">3 8B and 70B were pre-trained on multilingual data but were intended for use in English at the time.\\n2'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">mimetype</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text/plain'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">start_char_idx</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">end_char_idx</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3740</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata_seperator</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\\n'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">text_template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{metadata_str}\\n\\n{content}'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">score</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7447207372838807</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mNodeWithScore\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mnode\u001b[0m=\u001b[1;35mTextNode\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mid_\u001b[0m=\u001b[32m'7dee73d7-f185-4dd0-84d9-1830c15d286d'\u001b[0m,\n",
       "        \u001b[33membedding\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'page_label'\u001b[0m: \u001b[32m'2'\u001b[0m,\n",
       "            \u001b[32m'file_name'\u001b[0m: \u001b[32m'2407.21783.pdf'\u001b[0m,\n",
       "            \u001b[32m'file_path'\u001b[0m: \u001b[32m'/Users/michaelfrantz/CTME-llm-lecture-resources/prototyping_ai/data/2407.21783.pdf'\u001b[0m,\n",
       "            \u001b[32m'file_type'\u001b[0m: \u001b[32m'application/pdf'\u001b[0m,\n",
       "            \u001b[32m'file_size'\u001b[0m: \u001b[1;36m9832488\u001b[0m,\n",
       "            \u001b[32m'creation_date'\u001b[0m: \u001b[32m'2024-11-14'\u001b[0m,\n",
       "            \u001b[32m'last_modified_date'\u001b[0m: \u001b[32m'2024-11-14'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mexcluded_embed_metadata_keys\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[32m'file_name'\u001b[0m,\n",
       "            \u001b[32m'file_type'\u001b[0m,\n",
       "            \u001b[32m'file_size'\u001b[0m,\n",
       "            \u001b[32m'creation_date'\u001b[0m,\n",
       "            \u001b[32m'last_modified_date'\u001b[0m,\n",
       "            \u001b[32m'last_accessed_date'\u001b[0m\n",
       "        \u001b[1m]\u001b[0m,\n",
       "        \u001b[33mexcluded_llm_metadata_keys\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[32m'file_name'\u001b[0m,\n",
       "            \u001b[32m'file_type'\u001b[0m,\n",
       "            \u001b[32m'file_size'\u001b[0m,\n",
       "            \u001b[32m'creation_date'\u001b[0m,\n",
       "            \u001b[32m'last_modified_date'\u001b[0m,\n",
       "            \u001b[32m'last_accessed_date'\u001b[0m\n",
       "        \u001b[1m]\u001b[0m,\n",
       "        \u001b[33mrelationships\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mNodeRelationship.SOURCE:\u001b[0m\u001b[39m \u001b[0m\u001b[32m'1'\u001b[0m\u001b[1m>\u001b[0m: \u001b[1;35mRelatedNodeInfo\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mnode_id\u001b[0m=\u001b[32m'c2aa5e07-1c64-44f6-8701-f3df53873209'\u001b[0m,\n",
       "                \u001b[33mnode_type\u001b[0m=\u001b[32m'4'\u001b[0m,\n",
       "                \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'page_label'\u001b[0m: \u001b[32m'2'\u001b[0m,\n",
       "                    \u001b[32m'file_name'\u001b[0m: \u001b[32m'2407.21783.pdf'\u001b[0m,\n",
       "                    \u001b[32m'file_path'\u001b[0m: \n",
       "\u001b[32m'/Users/michaelfrantz/CTME-llm-lecture-resources/prototyping_ai/data/2407.21783.pdf'\u001b[0m,\n",
       "                    \u001b[32m'file_type'\u001b[0m: \u001b[32m'application/pdf'\u001b[0m,\n",
       "                    \u001b[32m'file_size'\u001b[0m: \u001b[1;36m9832488\u001b[0m,\n",
       "                    \u001b[32m'creation_date'\u001b[0m: \u001b[32m'2024-11-14'\u001b[0m,\n",
       "                    \u001b[32m'last_modified_date'\u001b[0m: \u001b[32m'2024-11-14'\u001b[0m\n",
       "                \u001b[1m}\u001b[0m,\n",
       "                \u001b[33mhash\u001b[0m=\u001b[32m'2ea90099062b1dd3bee8c1ae15c1a08e55026fc535e90e0d917ed1f6139bfd74'\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mmetadata_template\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32mkey\u001b[0m\u001b[32m}\u001b[0m\u001b[32m: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mvalue\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "        \u001b[33mmetadata_separator\u001b[0m=\u001b[32m'\\n'\u001b[0m,\n",
       "        \u001b[33mtext\u001b[0m=\u001b[32m'Finetuned Multilingual Long context Tool use Release\\nLlama 3 8B âœ— âœ— 1 âœ— âœ— April 2024\\nLlama 3 8B \u001b[0m\n",
       "\u001b[32mInstruct âœ“ âœ— âœ— âœ— April 2024\\nLlama 3 70B âœ— âœ— 1 âœ— âœ— April 2024\\nLlama 3 70B Instruct âœ“ âœ— âœ— âœ— April 2024\\nLlama 3.1 \u001b[0m\n",
       "\u001b[32m8B âœ— âœ“ âœ“ âœ— July 2024\\nLlama 3.1 8B Instruct âœ“ âœ“ âœ“ âœ“ July 2024\\nLlama 3.1 70B âœ— âœ“ âœ“ âœ— July 2024\\nLlama 3.1 70B \u001b[0m\n",
       "\u001b[32mInstruct âœ“ âœ“ âœ“ âœ“ July 2024\\nLlama 3.1 405B âœ— âœ“ âœ“ âœ— July 2024\\nLlama 3.1 405B Instruct âœ“ âœ“ âœ“ âœ“ July 2024\\nTable 1 \u001b[0m\n",
       "\u001b[32mOverview of the Llama 3 Herd of models. All results in this paper are for the Llama 3.1 models.\\nscaling laws for \u001b[0m\n",
       "\u001b[32mfoundation models, our flagship model outperforms smaller models trained using the\\nsame procedure. While our \u001b[0m\n",
       "\u001b[32mscaling laws suggest our flagship model is an approximately compute-optimal\\nsize for our training budget, we also \u001b[0m\n",
       "\u001b[32mtrain our smaller models for much longer than is compute-optimal.\\nThe resulting models perform better than \u001b[0m\n",
       "\u001b[32mcompute-optimal models at the same inference budget. We\\nuse the flagship model to further improve the quality of \u001b[0m\n",
       "\u001b[32mthose smaller models during post-training.\\nâ€¢ Managing complexity. We make design choices that seek to maximize our\u001b[0m\n",
       "\u001b[32mability to scale the model\\ndevelopment process. For example, we opt for a standard dense Transformer model \u001b[0m\n",
       "\u001b[32marchitecture \u001b[0m\u001b[32m(\u001b[0m\u001b[32mVaswani\\net al., 2017\u001b[0m\u001b[32m)\u001b[0m\u001b[32m with minor adaptations, rather than for a mixture-of-experts model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mShazeer et\u001b[0m\n",
       "\u001b[32mal., 2017\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nto maximize training stability. Similarly, we adopt a relatively simple post-training procedure \u001b[0m\n",
       "\u001b[32mbased\\non supervised finetuning \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSFT\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, rejection sampling \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, and direct preference optimization \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDPO;\\nRafailov \u001b[0m\n",
       "\u001b[32met al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as opposed to more complex reinforcement learning algorithms \u001b[0m\u001b[32m(\u001b[0m\u001b[32mOuyang et al.,\\n2022; Schulman et al.,\u001b[0m\n",
       "\u001b[32m2017\u001b[0m\u001b[32m)\u001b[0m\u001b[32m that tend to be less stable and harder to scale.\\nThe result of our work is Llama 3: a herd of three \u001b[0m\n",
       "\u001b[32mmultilingual1 language models with 8B, 70B, and 405B\\nparameters. We evaluate the performance of Llama 3 on a \u001b[0m\n",
       "\u001b[32mplethora of benchmark datasets that span a wide\\nrange of language understanding tasks. In addition, we perform \u001b[0m\n",
       "\u001b[32mextensive human evaluations that compare\\nLlama 3 with competing models. An overview of the performance of the \u001b[0m\n",
       "\u001b[32mflagship Llama 3 model on key\\nbenchmarks is presented in Table 2. Our experimental evaluation suggests that our \u001b[0m\n",
       "\u001b[32mflagship model performs\\non par with leading language models such as GPT-4 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mOpenAI, 2023a\u001b[0m\u001b[32m)\u001b[0m\u001b[32m across a variety of \u001b[0m\n",
       "\u001b[32mtasks, and is close to\\nmatching the state-of-the-art. Our smaller models are best-in-class, outperforming \u001b[0m\n",
       "\u001b[32malternative models with\\nsimilar numbers of parameters \u001b[0m\u001b[32m(\u001b[0m\u001b[32mBai et al., 2023; Jiang et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Llama 3 also \u001b[0m\n",
       "\u001b[32mdelivers a much better\\nbalance between helpfulness and harmlessness than its predecessor \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTouvron et al., 2023b\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. \u001b[0m\n",
       "\u001b[32mWe present a\\ndetailed analysis of the safety of Llama 3 in Section 5.4.\\nWe are publicly releasing all three Llama\u001b[0m\n",
       "\u001b[32m3 models under an updated version of the Llama 3 Community License;\\nsee https://llama.meta.com. This includes \u001b[0m\n",
       "\u001b[32mpre-trained and post-trained versions of our 405B parameter\\nlanguage model and a new version of our Llama Guard \u001b[0m\n",
       "\u001b[32mmodel \u001b[0m\u001b[32m(\u001b[0m\u001b[32mInan et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for input and output safety.\\nWe hope that the open release of a flagship model will spur\u001b[0m\n",
       "\u001b[32ma wave of innovation in the research community,\\nand accelerate a responsible path towards the development of \u001b[0m\n",
       "\u001b[32martificial general intelligence \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAGI\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\nAs part of the Llama 3 development process we also develop multimodal \u001b[0m\n",
       "\u001b[32mextensions to the models, enabling\\nimage recognition, video recognition, and speech understanding capabilities. \u001b[0m\n",
       "\u001b[32mThese models are still under\\nactive development and not yet ready for release. In addition to our language \u001b[0m\n",
       "\u001b[32mmodeling results, the paper\\npresents results of our initial experiments with those multimodal models.\\n1The Llama \u001b[0m\n",
       "\u001b[32m3 8B and 70B were pre-trained on multilingual data but were intended for use in English at the time.\\n2'\u001b[0m,\n",
       "        \u001b[33mmimetype\u001b[0m=\u001b[32m'text/plain'\u001b[0m,\n",
       "        \u001b[33mstart_char_idx\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "        \u001b[33mend_char_idx\u001b[0m=\u001b[1;36m3740\u001b[0m,\n",
       "        \u001b[33mmetadata_seperator\u001b[0m=\u001b[32m'\\n'\u001b[0m,\n",
       "        \u001b[33mtext_template\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32mmetadata_str\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32mcontent\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mscore\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.7447207372838807\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(response.source_nodes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if our chat enigine comes up with the same answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The paper mentions three new Llama models: Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> 8B, Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> 70B, and Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> 405B.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The paper mentions three new Llama models: Llama \u001b[1;36m3\u001b[0m 8B, Llama \u001b[1;36m3\u001b[0m 70B, and Llama \u001b[1;36m3\u001b[0m 405B.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">NodeWithScore</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">node</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextNode</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id_</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'7dee73d7-f185-4dd0-84d9-1830c15d286d'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">embedding</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'page_label'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'file_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2407.21783.pdf'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'file_path'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/Users/michaelfrantz/CTME-llm-lecture-resources/prototyping_ai/data/2407.21783.pdf'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'file_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'application/pdf'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'file_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9832488</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creation_date'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-11-14'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'last_modified_date'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-11-14'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">excluded_embed_metadata_keys</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'file_name'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'file_type'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'file_size'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creation_date'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'last_modified_date'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'last_accessed_date'</span>\n",
       "        <span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">excluded_llm_metadata_keys</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'file_name'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'file_type'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'file_size'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'creation_date'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'last_modified_date'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'last_accessed_date'</span>\n",
       "        <span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">relationships</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">NodeRelationship.SOURCE:</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'1'</span><span style=\"font-weight: bold\">&gt;</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RelatedNodeInfo</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">node_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'c2aa5e07-1c64-44f6-8701-f3df53873209'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">node_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'4'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'page_label'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'file_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2407.21783.pdf'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'file_path'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'/Users/michaelfrantz/CTME-llm-lecture-resources/prototyping_ai/data/2407.21783.pdf'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'file_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'application/pdf'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'file_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9832488</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'creation_date'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-11-14'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'last_modified_date'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2024-11-14'</span>\n",
       "                <span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">hash</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2ea90099062b1dd3bee8c1ae15c1a08e55026fc535e90e0d917ed1f6139bfd74'</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata_template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{key}: {value}'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata_separator</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\\n'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Finetuned Multilingual Long context Tool use Release\\nLlama 3 8B âœ— âœ— 1 âœ— âœ— April 2024\\nLlama 3 8B </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Instruct âœ“ âœ— âœ— âœ— April 2024\\nLlama 3 70B âœ— âœ— 1 âœ— âœ— April 2024\\nLlama 3 70B Instruct âœ“ âœ— âœ— âœ— April 2024\\nLlama 3.1 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">8B âœ— âœ“ âœ“ âœ— July 2024\\nLlama 3.1 8B Instruct âœ“ âœ“ âœ“ âœ“ July 2024\\nLlama 3.1 70B âœ— âœ“ âœ“ âœ— July 2024\\nLlama 3.1 70B </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Instruct âœ“ âœ“ âœ“ âœ“ July 2024\\nLlama 3.1 405B âœ— âœ“ âœ“ âœ— July 2024\\nLlama 3.1 405B Instruct âœ“ âœ“ âœ“ âœ“ July 2024\\nTable 1 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Overview of the Llama 3 Herd of models. All results in this paper are for the Llama 3.1 models.\\nscaling laws for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">foundation models, our flagship model outperforms smaller models trained using the\\nsame procedure. While our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">scaling laws suggest our flagship model is an approximately compute-optimal\\nsize for our training budget, we also </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">train our smaller models for much longer than is compute-optimal.\\nThe resulting models perform better than </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">compute-optimal models at the same inference budget. We\\nuse the flagship model to further improve the quality of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">those smaller models during post-training.\\nâ€¢ Managing complexity. We make design choices that seek to maximize our</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ability to scale the model\\ndevelopment process. For example, we opt for a standard dense Transformer model </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">architecture (Vaswani\\net al., 2017) with minor adaptations, rather than for a mixture-of-experts model (Shazeer et</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">al., 2017)\\nto maximize training stability. Similarly, we adopt a relatively simple post-training procedure </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">based\\non supervised finetuning (SFT), rejection sampling (RS), and direct preference optimization (DPO;\\nRafailov </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">et al. (2023)) as opposed to more complex reinforcement learning algorithms (Ouyang et al.,\\n2022; Schulman et al.,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">2017) that tend to be less stable and harder to scale.\\nThe result of our work is Llama 3: a herd of three </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">multilingual1 language models with 8B, 70B, and 405B\\nparameters. We evaluate the performance of Llama 3 on a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">plethora of benchmark datasets that span a wide\\nrange of language understanding tasks. In addition, we perform </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extensive human evaluations that compare\\nLlama 3 with competing models. An overview of the performance of the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">flagship Llama 3 model on key\\nbenchmarks is presented in Table 2. Our experimental evaluation suggests that our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">flagship model performs\\non par with leading language models such as GPT-4 (OpenAI, 2023a) across a variety of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tasks, and is close to\\nmatching the state-of-the-art. Our smaller models are best-in-class, outperforming </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">alternative models with\\nsimilar numbers of parameters (Bai et al., 2023; Jiang et al., 2023). Llama 3 also </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">delivers a much better\\nbalance between helpfulness and harmlessness than its predecessor (Touvron et al., 2023b). </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">We present a\\ndetailed analysis of the safety of Llama 3 in Section 5.4.\\nWe are publicly releasing all three Llama</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">3 models under an updated version of the Llama 3 Community License;\\nsee https://llama.meta.com. This includes </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pre-trained and post-trained versions of our 405B parameter\\nlanguage model and a new version of our Llama Guard </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model (Inan et al., 2023) for input and output safety.\\nWe hope that the open release of a flagship model will spur</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a wave of innovation in the research community,\\nand accelerate a responsible path towards the development of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">artificial general intelligence (AGI).\\nAs part of the Llama 3 development process we also develop multimodal </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extensions to the models, enabling\\nimage recognition, video recognition, and speech understanding capabilities. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">These models are still under\\nactive development and not yet ready for release. In addition to our language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">modeling results, the paper\\npresents results of our initial experiments with those multimodal models.\\n1The Llama </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">3 8B and 70B were pre-trained on multilingual data but were intended for use in English at the time.\\n2'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">mimetype</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text/plain'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">start_char_idx</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">end_char_idx</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3740</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata_seperator</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\\n'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">text_template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{metadata_str}\\n\\n{content}'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">score</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7443171831086403</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mNodeWithScore\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mnode\u001b[0m=\u001b[1;35mTextNode\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mid_\u001b[0m=\u001b[32m'7dee73d7-f185-4dd0-84d9-1830c15d286d'\u001b[0m,\n",
       "        \u001b[33membedding\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'page_label'\u001b[0m: \u001b[32m'2'\u001b[0m,\n",
       "            \u001b[32m'file_name'\u001b[0m: \u001b[32m'2407.21783.pdf'\u001b[0m,\n",
       "            \u001b[32m'file_path'\u001b[0m: \u001b[32m'/Users/michaelfrantz/CTME-llm-lecture-resources/prototyping_ai/data/2407.21783.pdf'\u001b[0m,\n",
       "            \u001b[32m'file_type'\u001b[0m: \u001b[32m'application/pdf'\u001b[0m,\n",
       "            \u001b[32m'file_size'\u001b[0m: \u001b[1;36m9832488\u001b[0m,\n",
       "            \u001b[32m'creation_date'\u001b[0m: \u001b[32m'2024-11-14'\u001b[0m,\n",
       "            \u001b[32m'last_modified_date'\u001b[0m: \u001b[32m'2024-11-14'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mexcluded_embed_metadata_keys\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[32m'file_name'\u001b[0m,\n",
       "            \u001b[32m'file_type'\u001b[0m,\n",
       "            \u001b[32m'file_size'\u001b[0m,\n",
       "            \u001b[32m'creation_date'\u001b[0m,\n",
       "            \u001b[32m'last_modified_date'\u001b[0m,\n",
       "            \u001b[32m'last_accessed_date'\u001b[0m\n",
       "        \u001b[1m]\u001b[0m,\n",
       "        \u001b[33mexcluded_llm_metadata_keys\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[32m'file_name'\u001b[0m,\n",
       "            \u001b[32m'file_type'\u001b[0m,\n",
       "            \u001b[32m'file_size'\u001b[0m,\n",
       "            \u001b[32m'creation_date'\u001b[0m,\n",
       "            \u001b[32m'last_modified_date'\u001b[0m,\n",
       "            \u001b[32m'last_accessed_date'\u001b[0m\n",
       "        \u001b[1m]\u001b[0m,\n",
       "        \u001b[33mrelationships\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mNodeRelationship.SOURCE:\u001b[0m\u001b[39m \u001b[0m\u001b[32m'1'\u001b[0m\u001b[1m>\u001b[0m: \u001b[1;35mRelatedNodeInfo\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mnode_id\u001b[0m=\u001b[32m'c2aa5e07-1c64-44f6-8701-f3df53873209'\u001b[0m,\n",
       "                \u001b[33mnode_type\u001b[0m=\u001b[32m'4'\u001b[0m,\n",
       "                \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'page_label'\u001b[0m: \u001b[32m'2'\u001b[0m,\n",
       "                    \u001b[32m'file_name'\u001b[0m: \u001b[32m'2407.21783.pdf'\u001b[0m,\n",
       "                    \u001b[32m'file_path'\u001b[0m: \n",
       "\u001b[32m'/Users/michaelfrantz/CTME-llm-lecture-resources/prototyping_ai/data/2407.21783.pdf'\u001b[0m,\n",
       "                    \u001b[32m'file_type'\u001b[0m: \u001b[32m'application/pdf'\u001b[0m,\n",
       "                    \u001b[32m'file_size'\u001b[0m: \u001b[1;36m9832488\u001b[0m,\n",
       "                    \u001b[32m'creation_date'\u001b[0m: \u001b[32m'2024-11-14'\u001b[0m,\n",
       "                    \u001b[32m'last_modified_date'\u001b[0m: \u001b[32m'2024-11-14'\u001b[0m\n",
       "                \u001b[1m}\u001b[0m,\n",
       "                \u001b[33mhash\u001b[0m=\u001b[32m'2ea90099062b1dd3bee8c1ae15c1a08e55026fc535e90e0d917ed1f6139bfd74'\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mmetadata_template\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32mkey\u001b[0m\u001b[32m}\u001b[0m\u001b[32m: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mvalue\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "        \u001b[33mmetadata_separator\u001b[0m=\u001b[32m'\\n'\u001b[0m,\n",
       "        \u001b[33mtext\u001b[0m=\u001b[32m'Finetuned Multilingual Long context Tool use Release\\nLlama 3 8B âœ— âœ— 1 âœ— âœ— April 2024\\nLlama 3 8B \u001b[0m\n",
       "\u001b[32mInstruct âœ“ âœ— âœ— âœ— April 2024\\nLlama 3 70B âœ— âœ— 1 âœ— âœ— April 2024\\nLlama 3 70B Instruct âœ“ âœ— âœ— âœ— April 2024\\nLlama 3.1 \u001b[0m\n",
       "\u001b[32m8B âœ— âœ“ âœ“ âœ— July 2024\\nLlama 3.1 8B Instruct âœ“ âœ“ âœ“ âœ“ July 2024\\nLlama 3.1 70B âœ— âœ“ âœ“ âœ— July 2024\\nLlama 3.1 70B \u001b[0m\n",
       "\u001b[32mInstruct âœ“ âœ“ âœ“ âœ“ July 2024\\nLlama 3.1 405B âœ— âœ“ âœ“ âœ— July 2024\\nLlama 3.1 405B Instruct âœ“ âœ“ âœ“ âœ“ July 2024\\nTable 1 \u001b[0m\n",
       "\u001b[32mOverview of the Llama 3 Herd of models. All results in this paper are for the Llama 3.1 models.\\nscaling laws for \u001b[0m\n",
       "\u001b[32mfoundation models, our flagship model outperforms smaller models trained using the\\nsame procedure. While our \u001b[0m\n",
       "\u001b[32mscaling laws suggest our flagship model is an approximately compute-optimal\\nsize for our training budget, we also \u001b[0m\n",
       "\u001b[32mtrain our smaller models for much longer than is compute-optimal.\\nThe resulting models perform better than \u001b[0m\n",
       "\u001b[32mcompute-optimal models at the same inference budget. We\\nuse the flagship model to further improve the quality of \u001b[0m\n",
       "\u001b[32mthose smaller models during post-training.\\nâ€¢ Managing complexity. We make design choices that seek to maximize our\u001b[0m\n",
       "\u001b[32mability to scale the model\\ndevelopment process. For example, we opt for a standard dense Transformer model \u001b[0m\n",
       "\u001b[32marchitecture \u001b[0m\u001b[32m(\u001b[0m\u001b[32mVaswani\\net al., 2017\u001b[0m\u001b[32m)\u001b[0m\u001b[32m with minor adaptations, rather than for a mixture-of-experts model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mShazeer et\u001b[0m\n",
       "\u001b[32mal., 2017\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nto maximize training stability. Similarly, we adopt a relatively simple post-training procedure \u001b[0m\n",
       "\u001b[32mbased\\non supervised finetuning \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSFT\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, rejection sampling \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRS\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, and direct preference optimization \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDPO;\\nRafailov \u001b[0m\n",
       "\u001b[32met al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as opposed to more complex reinforcement learning algorithms \u001b[0m\u001b[32m(\u001b[0m\u001b[32mOuyang et al.,\\n2022; Schulman et al.,\u001b[0m\n",
       "\u001b[32m2017\u001b[0m\u001b[32m)\u001b[0m\u001b[32m that tend to be less stable and harder to scale.\\nThe result of our work is Llama 3: a herd of three \u001b[0m\n",
       "\u001b[32mmultilingual1 language models with 8B, 70B, and 405B\\nparameters. We evaluate the performance of Llama 3 on a \u001b[0m\n",
       "\u001b[32mplethora of benchmark datasets that span a wide\\nrange of language understanding tasks. In addition, we perform \u001b[0m\n",
       "\u001b[32mextensive human evaluations that compare\\nLlama 3 with competing models. An overview of the performance of the \u001b[0m\n",
       "\u001b[32mflagship Llama 3 model on key\\nbenchmarks is presented in Table 2. Our experimental evaluation suggests that our \u001b[0m\n",
       "\u001b[32mflagship model performs\\non par with leading language models such as GPT-4 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mOpenAI, 2023a\u001b[0m\u001b[32m)\u001b[0m\u001b[32m across a variety of \u001b[0m\n",
       "\u001b[32mtasks, and is close to\\nmatching the state-of-the-art. Our smaller models are best-in-class, outperforming \u001b[0m\n",
       "\u001b[32malternative models with\\nsimilar numbers of parameters \u001b[0m\u001b[32m(\u001b[0m\u001b[32mBai et al., 2023; Jiang et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Llama 3 also \u001b[0m\n",
       "\u001b[32mdelivers a much better\\nbalance between helpfulness and harmlessness than its predecessor \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTouvron et al., 2023b\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. \u001b[0m\n",
       "\u001b[32mWe present a\\ndetailed analysis of the safety of Llama 3 in Section 5.4.\\nWe are publicly releasing all three Llama\u001b[0m\n",
       "\u001b[32m3 models under an updated version of the Llama 3 Community License;\\nsee https://llama.meta.com. This includes \u001b[0m\n",
       "\u001b[32mpre-trained and post-trained versions of our 405B parameter\\nlanguage model and a new version of our Llama Guard \u001b[0m\n",
       "\u001b[32mmodel \u001b[0m\u001b[32m(\u001b[0m\u001b[32mInan et al., 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for input and output safety.\\nWe hope that the open release of a flagship model will spur\u001b[0m\n",
       "\u001b[32ma wave of innovation in the research community,\\nand accelerate a responsible path towards the development of \u001b[0m\n",
       "\u001b[32martificial general intelligence \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAGI\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\nAs part of the Llama 3 development process we also develop multimodal \u001b[0m\n",
       "\u001b[32mextensions to the models, enabling\\nimage recognition, video recognition, and speech understanding capabilities. \u001b[0m\n",
       "\u001b[32mThese models are still under\\nactive development and not yet ready for release. In addition to our language \u001b[0m\n",
       "\u001b[32mmodeling results, the paper\\npresents results of our initial experiments with those multimodal models.\\n1The Llama \u001b[0m\n",
       "\u001b[32m3 8B and 70B were pre-trained on multilingual data but were intended for use in English at the time.\\n2'\u001b[0m,\n",
       "        \u001b[33mmimetype\u001b[0m=\u001b[32m'text/plain'\u001b[0m,\n",
       "        \u001b[33mstart_char_idx\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "        \u001b[33mend_char_idx\u001b[0m=\u001b[1;36m3740\u001b[0m,\n",
       "        \u001b[33mmetadata_seperator\u001b[0m=\u001b[32m'\\n'\u001b[0m,\n",
       "        \u001b[33mtext_template\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32mmetadata_str\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32mcontent\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mscore\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.7443171831086403\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_response = chat_engine.chat(\"How many new Llama models models are mentioned in the paper?\")\n",
    "print(chat_response.response)\n",
    "print(chat_response.source_nodes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data ingestion\n",
    "\n",
    "Data often comes in many different formats.\n",
    "It may come in the form of a PDF, a web page, a code file, etc.\n",
    "We may need some specific processing pipelines to extract the text from these documents, split them correctly, and vectorize them.\n",
    "\n",
    "Luckily, LlamaIndex (and other libraries) provide lots of built-in and add-on tools to help you ingest almost any data type.\n",
    "Instead of loading a PDF, let's load a web page instead.\n",
    "We will use one of the classes provided by [`llama-index-readers-web`](https://llamahub.ai/l/readers/llama-index-readers-web?from=readers) to load data from a web page.\n",
    "\n",
    "In this section, we will:\n",
    "- Load a web page as Markdown\n",
    "- Split it into chunks following the structured format of the Markdown\n",
    "- Embed the chunks\n",
    "- Store the chunks in a vector database\n",
    "- Create a query engine from the vector database and use it to answer a question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_docs = SimpleWebPageReader(html_to_text=True).load_data(['https://en.wikipedia.org/wiki/Wikipedia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chromadb.EphemeralClient().create_collection(\"wikipedia\", get_or_create=True)\n",
    "vector_store = ChromaVectorStore(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        MarkdownNodeParser.from_defaults(),\n",
    "        embed_model,\n",
    "    ], \n",
    "    vector_store=vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pipeline.run(documents=web_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">There are currently <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">339</span> language editions of Wikipedia.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "There are currently \u001b[1;36m339\u001b[0m language editions of Wikipedia.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "response = query_engine.query(\"How many languages there exactly? Quote the exact text as well.\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Quote from source: <span style=\"color: #008000; text-decoration-color: #008000\">'There are currently 339 language editions of Wikipedia (also called _language'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Quote from source: \u001b[32m'There are currently 339 language editions of Wikipedia \u001b[0m\u001b[32m(\u001b[0m\u001b[32malso called _language'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use fuzzywuzzy to find the closest match in the source text\n",
    "from thefuzz import fuzz, process\n",
    "# Get the top matching line of text from the source_text_quote\n",
    "top_match, match_score = process.extractOne(response.response, response.source_nodes[0].text.splitlines(), scorer=fuzz.ratio)\n",
    "assert top_match in response.source_nodes[0].text\n",
    "print(f\"Quote from source: '{top_match}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR\n",
    "\n",
    "There are times where the PDF is not in a format that can be easily read into text.\n",
    "In these cases, we will need to use optical character recognition (OCR) to convert the images to text.\n",
    "There are many libraries and cloud services that can do this, but for this example, we will use the `docling` library since our example document is a PDF.\n",
    "We will then ingest the text into LlamaIndex and use it to answer a question.\n",
    "\n",
    "![](https://ds4sd.github.io/docling/assets/docling_processing.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.docling import DoclingReader\n",
    "from llama_index.node_parser.docling import DoclingNodeParser\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = DoclingReader(export_type=DoclingReader.ExportType.JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m*ERR* --- *ERR*\u001b[0m\n",
      "\u001b[91m*ERR* Table is not square! *ERR*\u001b[0m\n",
      "\u001b[93m*Padding to square...*\u001b[0m\n",
      "\u001b[91m*ERR* --- *ERR*\u001b[0m\n",
      "\u001b[91m*ERR* Table is not square! *ERR*\u001b[0m\n",
      "\u001b[93m*Padding to square...*\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "docling_docs = reader.load_data(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;36m1\u001b[0m"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docling_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to set the text template to \"{content}\" because the default is \"{metadata}\\n\\n{content}\",\n",
    "# and LlamaIndex will try to embed the metadata as well. The metadata is not useful at serach time.\n",
    "docling_docs[0].text_template\n",
    "docling_docs[0].text_template = \"{content}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;36m710\u001b[0m"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docling_node_parser = DoclingNodeParser()\n",
    "docling_nodes = docling_node_parser.get_nodes_from_documents(docling_docs)\n",
    "len(docling_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'schema_name'\u001b[0m: \u001b[32m'docling_core.transforms.chunker.DocMeta'\u001b[0m,\n",
       "    \u001b[32m'version'\u001b[0m: \u001b[32m'1.0.0'\u001b[0m,\n",
       "    \u001b[32m'doc_items'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'self_ref'\u001b[0m: \u001b[32m'#/texts/0'\u001b[0m,\n",
       "            \u001b[32m'parent'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'$ref'\u001b[0m: \u001b[32m'#/body'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'children'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[32m'label'\u001b[0m: \u001b[32m'page_header'\u001b[0m,\n",
       "            \u001b[32m'prov'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "                \u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'page_no'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "                    \u001b[32m'bbox'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                        \u001b[32m'l'\u001b[0m: \u001b[1;36m16.9896183013916\u001b[0m,\n",
       "                        \u001b[32m't'\u001b[0m: \u001b[1;36m578.7445068359375\u001b[0m,\n",
       "                        \u001b[32m'r'\u001b[0m: \u001b[1;36m36.339778900146484\u001b[0m,\n",
       "                        \u001b[32m'b'\u001b[0m: \u001b[1;36m231.99996948242188\u001b[0m,\n",
       "                        \u001b[32m'coord_origin'\u001b[0m: \u001b[32m'BOTTOMLEFT'\u001b[0m\n",
       "                    \u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'charspan'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m38\u001b[0m\u001b[1m]\u001b[0m\n",
       "                \u001b[1m}\u001b[0m\n",
       "            \u001b[1m]\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'origin'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'mimetype'\u001b[0m: \u001b[32m'application/pdf'\u001b[0m,\n",
       "        \u001b[32m'binary_hash'\u001b[0m: \u001b[1;36m7968363826940540892\u001b[0m,\n",
       "        \u001b[32m'filename'\u001b[0m: \u001b[32m'2407.21783.pdf'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docling_nodes[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex(nodes=docling_nodes, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The paper mentions three new Llama models: Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.1</span> 8B, Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.1</span> 70B, and Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.1</span> 405B.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The paper mentions three new Llama models: Llama \u001b[1;36m3.1\u001b[0m 8B, Llama \u001b[1;36m3.1\u001b[0m 70B, and Llama \u001b[1;36m3.1\u001b[0m 405B.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(llm=llm)\n",
    "response = query_engine.query(\"How many new Llama models are mentioned in the paper?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">NodeWithScore</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">node</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextNode</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">id_</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'41152654-030e-4c0b-8e9f-394e0d8705fc'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">embedding</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'schema_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'docling_core.transforms.chunker.DocMeta'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'version'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'1.0.0'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'doc_items'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'self_ref'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'#/tables/0'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'parent'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'$ref'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'#/body'</span><span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'children'</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'label'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'table'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'prov'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "                        <span style=\"font-weight: bold\">{</span>\n",
       "                            <span style=\"color: #008000; text-decoration-color: #008000\">'page_no'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
       "                            <span style=\"color: #008000; text-decoration-color: #008000\">'bbox'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                                <span style=\"color: #008000; text-decoration-color: #008000\">'l'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">102.58776092529297</span>,\n",
       "                                <span style=\"color: #008000; text-decoration-color: #008000\">'t'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">728.816162109375</span>,\n",
       "                                <span style=\"color: #008000; text-decoration-color: #008000\">'r'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">508.96331787109375</span>,\n",
       "                                <span style=\"color: #008000; text-decoration-color: #008000\">'b'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">585.1917724609375</span>,\n",
       "                                <span style=\"color: #008000; text-decoration-color: #008000\">'coord_origin'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'BOTTOMLEFT'</span>\n",
       "                            <span style=\"font-weight: bold\">}</span>,\n",
       "                            <span style=\"color: #008000; text-decoration-color: #008000\">'charspan'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>\n",
       "                        <span style=\"font-weight: bold\">}</span>\n",
       "                    <span style=\"font-weight: bold\">]</span>\n",
       "                <span style=\"font-weight: bold\">}</span>\n",
       "            <span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'headings'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'1 Introduction'</span><span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'captions'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'Table 1 Overview of the Llama 3 Herd of models. All results in this paper are for the Llama 3.1 </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models.'</span>\n",
       "            <span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'origin'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'mimetype'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'application/pdf'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'binary_hash'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7968363826940540892</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'filename'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2407.21783.pdf'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">excluded_embed_metadata_keys</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'schema_name'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'version'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'doc_items'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'origin'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">excluded_llm_metadata_keys</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'schema_name'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'version'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'doc_items'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'origin'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">relationships</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">NodeRelationship.SOURCE:</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'1'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RelatedNodeInfo</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                </span><span style=\"color: #808000; text-decoration-color: #808000\">node_id</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'15648112-c57b-4eee-b43f-f324fa4919b1'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                </span><span style=\"color: #808000; text-decoration-color: #808000\">node_type</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'4'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                </span><span style=\"color: #808000; text-decoration-color: #808000\">metadata</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{}</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                </span><span style=\"color: #808000; text-decoration-color: #808000\">hash</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'235220e28e49c1ec2c9d9d5336b24cc4aace4ed28a06c1fdc262528cc79c4b5b'</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            &lt;NodeRelationship.PREVIOUS: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RelatedNodeInfo</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                </span><span style=\"color: #808000; text-decoration-color: #808000\">node_id</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'9ac0e0d7-79d8-40ff-960f-d9b40b2eacdc'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                </span><span style=\"color: #808000; text-decoration-color: #808000\">node_type</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'1'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                </span><span style=\"color: #808000; text-decoration-color: #808000\">metadata</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">'schema_name'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'docling_core.transforms.chunker.DocMeta'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">'version'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'1.0.0'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">'doc_items'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                        </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                            </span><span style=\"color: #008000; text-decoration-color: #008000\">'self_ref'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'#/texts/17'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                            </span><span style=\"color: #008000; text-decoration-color: #008000\">'parent'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'$ref'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'#/body'</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                            </span><span style=\"color: #008000; text-decoration-color: #008000\">'children'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[]</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                            </span><span style=\"color: #008000; text-decoration-color: #008000\">'label'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'caption'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                            </span><span style=\"color: #008000; text-decoration-color: #008000\">'prov'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                                </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                                    </span><span style=\"color: #008000; text-decoration-color: #008000\">'page_no'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                                    </span><span style=\"color: #008000; text-decoration-color: #008000\">'bbox'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                                        </span><span style=\"color: #008000; text-decoration-color: #008000\">'l'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">69.73268127441406</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                                        </span><span style=\"color: #008000; text-decoration-color: #008000\">'t'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">576.0440673828125</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                                        </span><span style=\"color: #008000; text-decoration-color: #008000\">'r'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">475.39227294921875</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                                        </span><span style=\"color: #008000; text-decoration-color: #008000\">'b'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">565.881103515625</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                                        </span><span style=\"color: #008000; text-decoration-color: #008000\">'coord_origin'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'BOTTOMLEFT'</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                                    </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                                    </span><span style=\"color: #008000; text-decoration-color: #008000\">'charspan'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">103</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                                </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                            </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                        </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                    </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">'headings'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'1 Introduction'</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">'origin'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                        </span><span style=\"color: #008000; text-decoration-color: #008000\">'mimetype'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'application/pdf'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                        </span><span style=\"color: #008000; text-decoration-color: #008000\">'binary_hash'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7968363826940540892</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                        </span><span style=\"color: #008000; text-decoration-color: #008000\">'filename'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2407.21783.pdf'</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                    </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">                </span><span style=\"color: #808000; text-decoration-color: #808000\">hash</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'e07333f44f280efdad25ccf4d7bc70e41202f5174cff25208a56f877f0550c8d'</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            &lt;NodeRelationship.NEXT: </span><span style=\"color: #008000; text-decoration-color: #008000\">'3'</span><span style=\"font-weight: bold\">&gt;</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RelatedNodeInfo</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">node_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'63fafab9-6b6b-4531-8c83-3461f9dcae41'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">node_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'schema_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'docling_core.transforms.chunker.DocMeta'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'version'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'1.0.0'</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'doc_items'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "                        <span style=\"font-weight: bold\">{</span>\n",
       "                            <span style=\"color: #008000; text-decoration-color: #008000\">'self_ref'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'#/texts/18'</span>,\n",
       "                            <span style=\"color: #008000; text-decoration-color: #008000\">'parent'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'$ref'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'#/body'</span><span style=\"font-weight: bold\">}</span>,\n",
       "                            <span style=\"color: #008000; text-decoration-color: #008000\">'children'</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "                            <span style=\"color: #008000; text-decoration-color: #008000\">'label'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'text'</span>,\n",
       "                            <span style=\"color: #008000; text-decoration-color: #008000\">'prov'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "                                <span style=\"font-weight: bold\">{</span>\n",
       "                                    <span style=\"color: #008000; text-decoration-color: #008000\">'page_no'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
       "                                    <span style=\"color: #008000; text-decoration-color: #008000\">'bbox'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                                        <span style=\"color: #008000; text-decoration-color: #008000\">'l'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">94.7854232788086</span>,\n",
       "                                        <span style=\"color: #008000; text-decoration-color: #008000\">'t'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">542.5082397460938</span>,\n",
       "                                        <span style=\"color: #008000; text-decoration-color: #008000\">'r'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">543.0672607421875</span>,\n",
       "                                        <span style=\"color: #008000; text-decoration-color: #008000\">'b'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">484.1476135253906</span>,\n",
       "                                        <span style=\"color: #008000; text-decoration-color: #008000\">'coord_origin'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'BOTTOMLEFT'</span>\n",
       "                                    <span style=\"font-weight: bold\">}</span>,\n",
       "                                    <span style=\"color: #008000; text-decoration-color: #008000\">'charspan'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">502</span><span style=\"font-weight: bold\">]</span>\n",
       "                                <span style=\"font-weight: bold\">}</span>\n",
       "                            <span style=\"font-weight: bold\">]</span>\n",
       "                        <span style=\"font-weight: bold\">}</span>\n",
       "                    <span style=\"font-weight: bold\">]</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'headings'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'1 Introduction'</span><span style=\"font-weight: bold\">]</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'origin'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'mimetype'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'application/pdf'</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'binary_hash'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7968363826940540892</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'filename'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2407.21783.pdf'</span>\n",
       "                    <span style=\"font-weight: bold\">}</span>\n",
       "                <span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">hash</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'69db2970127f298c8fdcb3652696f48deeccffb455fd1b46adcd2489849ac526'</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata_template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{key}: {value}'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata_separator</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\\n'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Llama 3 8B, Finetuned = âœ—. Llama 3 8B, Multilingual = âœ— 1. Llama 3 8B, Long context = âœ—. Llama 3 8B, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Tool use = âœ—. Llama 3 8B, Release = April 2024. Llama 3 8B Instruct, Finetuned = âœ“. Llama 3 8B Instruct, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Multilingual = âœ—. Llama 3 8B Instruct, Long context = âœ—. Llama 3 8B Instruct, Tool use = âœ—. Llama 3 8B Instruct, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Release = April 2024. Llama 3 70B, Finetuned = âœ—. Llama 3 70B, Multilingual = âœ— 1. Llama 3 70B, Long context = âœ—. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Llama 3 70B, Tool use = âœ—. Llama 3 70B, Release = April 2024. Llama 3 70B Instruct, Finetuned = âœ“. Llama 3 70B </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Instruct, Multilingual = âœ—. Llama 3 70B Instruct, Long context = âœ—. Llama 3 70B Instruct, Tool use = âœ—. Llama 3 70B</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Instruct, Release = April 2024. Llama 3.1 8B, Finetuned = âœ—. Llama 3.1 8B, Multilingual = âœ“. Llama 3.1 8B, Long </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">context = âœ“. Llama 3.1 8B, Tool use = âœ—. Llama 3.1 8B, Release = July 2024. Llama 3.1 8B Instruct, Finetuned = âœ“. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Llama 3.1 8B Instruct, Multilingual = âœ“. Llama 3.1 8B Instruct, Long context = âœ“. Llama 3.1 8B Instruct, Tool use =</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“. Llama 3.1 8B Instruct, Release = July 2024. Llama 3.1 70B, Finetuned = âœ—. Llama 3.1 70B, Multilingual = âœ“. Llama</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">3.1 70B, Long context = âœ“. Llama 3.1 70B, Tool use = âœ—. Llama 3.1 70B, Release = July 2024. Llama 3.1 70B Instruct,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Finetuned = âœ“. Llama 3.1 70B Instruct, Multilingual = âœ“. Llama 3.1 70B Instruct, Long context = âœ“. Llama 3.1 70B </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Instruct, Tool use = âœ“. Llama 3.1 70B Instruct, Release = July 2024. Llama 3.1 405B, Finetuned = âœ—. Llama 3.1 405B,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Multilingual = âœ“. Llama 3.1 405B, Long context = âœ“. Llama 3.1 405B, Tool use = âœ—. Llama 3.1 405B, Release = July </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">2024. Llama 3.1 405B Instruct, Finetuned = âœ“. Llama 3.1 405B Instruct, Multilingual = âœ“. Llama 3.1 405B Instruct, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Long context = âœ“. Llama 3.1 405B Instruct, Tool use = âœ“. Llama 3.1 405B Instruct, Release = July 2024'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">mimetype</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text/plain'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">start_char_idx</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">end_char_idx</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metadata_seperator</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\\n'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">text_template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{metadata_str}\\n\\n{content}'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">score</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7679427012175201</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mNodeWithScore\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mnode\u001b[0m=\u001b[1;35mTextNode\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mid_\u001b[0m=\u001b[32m'41152654-030e-4c0b-8e9f-394e0d8705fc'\u001b[0m,\n",
       "        \u001b[33membedding\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'schema_name'\u001b[0m: \u001b[32m'docling_core.transforms.chunker.DocMeta'\u001b[0m,\n",
       "            \u001b[32m'version'\u001b[0m: \u001b[32m'1.0.0'\u001b[0m,\n",
       "            \u001b[32m'doc_items'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "                \u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'self_ref'\u001b[0m: \u001b[32m'#/tables/0'\u001b[0m,\n",
       "                    \u001b[32m'parent'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'$ref'\u001b[0m: \u001b[32m'#/body'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'children'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                    \u001b[32m'label'\u001b[0m: \u001b[32m'table'\u001b[0m,\n",
       "                    \u001b[32m'prov'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "                        \u001b[1m{\u001b[0m\n",
       "                            \u001b[32m'page_no'\u001b[0m: \u001b[1;36m2\u001b[0m,\n",
       "                            \u001b[32m'bbox'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                                \u001b[32m'l'\u001b[0m: \u001b[1;36m102.58776092529297\u001b[0m,\n",
       "                                \u001b[32m't'\u001b[0m: \u001b[1;36m728.816162109375\u001b[0m,\n",
       "                                \u001b[32m'r'\u001b[0m: \u001b[1;36m508.96331787109375\u001b[0m,\n",
       "                                \u001b[32m'b'\u001b[0m: \u001b[1;36m585.1917724609375\u001b[0m,\n",
       "                                \u001b[32m'coord_origin'\u001b[0m: \u001b[32m'BOTTOMLEFT'\u001b[0m\n",
       "                            \u001b[1m}\u001b[0m,\n",
       "                            \u001b[32m'charspan'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\n",
       "                        \u001b[1m}\u001b[0m\n",
       "                    \u001b[1m]\u001b[0m\n",
       "                \u001b[1m}\u001b[0m\n",
       "            \u001b[1m]\u001b[0m,\n",
       "            \u001b[32m'headings'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'1 Introduction'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[32m'captions'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "                \u001b[32m'Table 1 Overview of the Llama 3 Herd of models. All results in this paper are for the Llama 3.1 \u001b[0m\n",
       "\u001b[32mmodels.'\u001b[0m\n",
       "            \u001b[1m]\u001b[0m,\n",
       "            \u001b[32m'origin'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'mimetype'\u001b[0m: \u001b[32m'application/pdf'\u001b[0m,\n",
       "                \u001b[32m'binary_hash'\u001b[0m: \u001b[1;36m7968363826940540892\u001b[0m,\n",
       "                \u001b[32m'filename'\u001b[0m: \u001b[32m'2407.21783.pdf'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mexcluded_embed_metadata_keys\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'schema_name'\u001b[0m, \u001b[32m'version'\u001b[0m, \u001b[32m'doc_items'\u001b[0m, \u001b[32m'origin'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33mexcluded_llm_metadata_keys\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'schema_name'\u001b[0m, \u001b[32m'version'\u001b[0m, \u001b[32m'doc_items'\u001b[0m, \u001b[32m'origin'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33mrelationships\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mNodeRelationship.SOURCE:\u001b[0m\u001b[39m \u001b[0m\u001b[32m'1'\u001b[0m\u001b[39m>: \u001b[0m\u001b[1;35mRelatedNodeInfo\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mnode_id\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'15648112-c57b-4eee-b43f-f324fa4919b1'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mnode_type\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'4'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mmetadata\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mhash\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'235220e28e49c1ec2c9d9d5336b24cc4aace4ed28a06c1fdc262528cc79c4b5b'\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            <NodeRelationship.PREVIOUS: \u001b[0m\u001b[32m'2'\u001b[0m\u001b[39m>: \u001b[0m\u001b[1;35mRelatedNodeInfo\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mnode_id\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'9ac0e0d7-79d8-40ff-960f-d9b40b2eacdc'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mnode_type\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'1'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mmetadata\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[32m'schema_name'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'docling_core.transforms.chunker.DocMeta'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[32m'version'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'1.0.0'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[32m'doc_items'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m                            \u001b[0m\u001b[32m'self_ref'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'#/texts/17'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                            \u001b[0m\u001b[32m'parent'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m{\u001b[0m\u001b[32m'$ref'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'#/body'\u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                            \u001b[0m\u001b[32m'children'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                            \u001b[0m\u001b[32m'label'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'caption'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                            \u001b[0m\u001b[32m'prov'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\n",
       "\u001b[39m                                \u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m                                    \u001b[0m\u001b[32m'page_no'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                                    \u001b[0m\u001b[32m'bbox'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m                                        \u001b[0m\u001b[32m'l'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m69.73268127441406\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                                        \u001b[0m\u001b[32m't'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m576.0440673828125\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                                        \u001b[0m\u001b[32m'r'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m475.39227294921875\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                                        \u001b[0m\u001b[32m'b'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m565.881103515625\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                                        \u001b[0m\u001b[32m'coord_origin'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'BOTTOMLEFT'\u001b[0m\n",
       "\u001b[39m                                    \u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                                    \u001b[0m\u001b[32m'charspan'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[39m, \u001b[0m\u001b[1;36m103\u001b[0m\u001b[1;39m]\u001b[0m\n",
       "\u001b[39m                                \u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[39m                            \u001b[0m\u001b[1;39m]\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[32m'headings'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\u001b[32m'1 Introduction'\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[32m'origin'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[32m'mimetype'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'application/pdf'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[32m'binary_hash'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m7968363826940540892\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[32m'filename'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'2407.21783.pdf'\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mhash\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'e07333f44f280efdad25ccf4d7bc70e41202f5174cff25208a56f877f0550c8d'\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            <NodeRelationship.NEXT: \u001b[0m\u001b[32m'3'\u001b[0m\u001b[1m>\u001b[0m: \u001b[1;35mRelatedNodeInfo\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mnode_id\u001b[0m=\u001b[32m'63fafab9-6b6b-4531-8c83-3461f9dcae41'\u001b[0m,\n",
       "                \u001b[33mnode_type\u001b[0m=\u001b[32m'1'\u001b[0m,\n",
       "                \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'schema_name'\u001b[0m: \u001b[32m'docling_core.transforms.chunker.DocMeta'\u001b[0m,\n",
       "                    \u001b[32m'version'\u001b[0m: \u001b[32m'1.0.0'\u001b[0m,\n",
       "                    \u001b[32m'doc_items'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "                        \u001b[1m{\u001b[0m\n",
       "                            \u001b[32m'self_ref'\u001b[0m: \u001b[32m'#/texts/18'\u001b[0m,\n",
       "                            \u001b[32m'parent'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'$ref'\u001b[0m: \u001b[32m'#/body'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                            \u001b[32m'children'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                            \u001b[32m'label'\u001b[0m: \u001b[32m'text'\u001b[0m,\n",
       "                            \u001b[32m'prov'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "                                \u001b[1m{\u001b[0m\n",
       "                                    \u001b[32m'page_no'\u001b[0m: \u001b[1;36m2\u001b[0m,\n",
       "                                    \u001b[32m'bbox'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                                        \u001b[32m'l'\u001b[0m: \u001b[1;36m94.7854232788086\u001b[0m,\n",
       "                                        \u001b[32m't'\u001b[0m: \u001b[1;36m542.5082397460938\u001b[0m,\n",
       "                                        \u001b[32m'r'\u001b[0m: \u001b[1;36m543.0672607421875\u001b[0m,\n",
       "                                        \u001b[32m'b'\u001b[0m: \u001b[1;36m484.1476135253906\u001b[0m,\n",
       "                                        \u001b[32m'coord_origin'\u001b[0m: \u001b[32m'BOTTOMLEFT'\u001b[0m\n",
       "                                    \u001b[1m}\u001b[0m,\n",
       "                                    \u001b[32m'charspan'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m502\u001b[0m\u001b[1m]\u001b[0m\n",
       "                                \u001b[1m}\u001b[0m\n",
       "                            \u001b[1m]\u001b[0m\n",
       "                        \u001b[1m}\u001b[0m\n",
       "                    \u001b[1m]\u001b[0m,\n",
       "                    \u001b[32m'headings'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'1 Introduction'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                    \u001b[32m'origin'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                        \u001b[32m'mimetype'\u001b[0m: \u001b[32m'application/pdf'\u001b[0m,\n",
       "                        \u001b[32m'binary_hash'\u001b[0m: \u001b[1;36m7968363826940540892\u001b[0m,\n",
       "                        \u001b[32m'filename'\u001b[0m: \u001b[32m'2407.21783.pdf'\u001b[0m\n",
       "                    \u001b[1m}\u001b[0m\n",
       "                \u001b[1m}\u001b[0m,\n",
       "                \u001b[33mhash\u001b[0m=\u001b[32m'69db2970127f298c8fdcb3652696f48deeccffb455fd1b46adcd2489849ac526'\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mmetadata_template\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32mkey\u001b[0m\u001b[32m}\u001b[0m\u001b[32m: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mvalue\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "        \u001b[33mmetadata_separator\u001b[0m=\u001b[32m'\\n'\u001b[0m,\n",
       "        \u001b[33mtext\u001b[0m=\u001b[32m'Llama 3 8B, Finetuned = âœ—. Llama 3 8B, Multilingual = âœ— 1. Llama 3 8B, Long context = âœ—. Llama 3 8B, \u001b[0m\n",
       "\u001b[32mTool use = âœ—. Llama 3 8B, Release = April 2024. Llama 3 8B Instruct, Finetuned = âœ“. Llama 3 8B Instruct, \u001b[0m\n",
       "\u001b[32mMultilingual = âœ—. Llama 3 8B Instruct, Long context = âœ—. Llama 3 8B Instruct, Tool use = âœ—. Llama 3 8B Instruct, \u001b[0m\n",
       "\u001b[32mRelease = April 2024. Llama 3 70B, Finetuned = âœ—. Llama 3 70B, Multilingual = âœ— 1. Llama 3 70B, Long context = âœ—. \u001b[0m\n",
       "\u001b[32mLlama 3 70B, Tool use = âœ—. Llama 3 70B, Release = April 2024. Llama 3 70B Instruct, Finetuned = âœ“. Llama 3 70B \u001b[0m\n",
       "\u001b[32mInstruct, Multilingual = âœ—. Llama 3 70B Instruct, Long context = âœ—. Llama 3 70B Instruct, Tool use = âœ—. Llama 3 70B\u001b[0m\n",
       "\u001b[32mInstruct, Release = April 2024. Llama 3.1 8B, Finetuned = âœ—. Llama 3.1 8B, Multilingual = âœ“. Llama 3.1 8B, Long \u001b[0m\n",
       "\u001b[32mcontext = âœ“. Llama 3.1 8B, Tool use = âœ—. Llama 3.1 8B, Release = July 2024. Llama 3.1 8B Instruct, Finetuned = âœ“. \u001b[0m\n",
       "\u001b[32mLlama 3.1 8B Instruct, Multilingual = âœ“. Llama 3.1 8B Instruct, Long context = âœ“. Llama 3.1 8B Instruct, Tool use =\u001b[0m\n",
       "\u001b[32mâœ“. Llama 3.1 8B Instruct, Release = July 2024. Llama 3.1 70B, Finetuned = âœ—. Llama 3.1 70B, Multilingual = âœ“. Llama\u001b[0m\n",
       "\u001b[32m3.1 70B, Long context = âœ“. Llama 3.1 70B, Tool use = âœ—. Llama 3.1 70B, Release = July 2024. Llama 3.1 70B Instruct,\u001b[0m\n",
       "\u001b[32mFinetuned = âœ“. Llama 3.1 70B Instruct, Multilingual = âœ“. Llama 3.1 70B Instruct, Long context = âœ“. Llama 3.1 70B \u001b[0m\n",
       "\u001b[32mInstruct, Tool use = âœ“. Llama 3.1 70B Instruct, Release = July 2024. Llama 3.1 405B, Finetuned = âœ—. Llama 3.1 405B,\u001b[0m\n",
       "\u001b[32mMultilingual = âœ“. Llama 3.1 405B, Long context = âœ“. Llama 3.1 405B, Tool use = âœ—. Llama 3.1 405B, Release = July \u001b[0m\n",
       "\u001b[32m2024. Llama 3.1 405B Instruct, Finetuned = âœ“. Llama 3.1 405B Instruct, Multilingual = âœ“. Llama 3.1 405B Instruct, \u001b[0m\n",
       "\u001b[32mLong context = âœ“. Llama 3.1 405B Instruct, Tool use = âœ“. Llama 3.1 405B Instruct, Release = July 2024'\u001b[0m,\n",
       "        \u001b[33mmimetype\u001b[0m=\u001b[32m'text/plain'\u001b[0m,\n",
       "        \u001b[33mstart_char_idx\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mend_char_idx\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mmetadata_seperator\u001b[0m=\u001b[32m'\\n'\u001b[0m,\n",
       "        \u001b[33mtext_template\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32mmetadata_str\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32mcontent\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mscore\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.7679427012175201\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(response.source_nodes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data splitting\n",
    "\n",
    "Many times, it's impractical to embed the entire document, and expensive to feed the entire document to the LLM.\n",
    "Instead, we can split the document into chunks, embed the chunks, and use a retrieval method to find the most relevant chunks.\n",
    "There are naÃ¯ve methods that split texts into chunks of a specific length with some overlap;\n",
    "there are methods that use the structure of the document to split it into sections (e.g. sections, figures, tables);\n",
    "and there are more advanced methods that use semantic similarity to group the text into chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the OCR'd text is just one very long Markdown string, we need to split it into chunks.\n",
    "One nice way to do that is use the inherent structure of Markdown to split it into sections.\n",
    "We do that here with LlamaIndex's `MarkdownNodeParser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36809685196e403d9c27d27893da5a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m*ERR* --- *ERR*\u001b[0m\n",
      "\u001b[91m*ERR* Table is not square! *ERR*\u001b[0m\n",
      "\u001b[93m*Padding to square...*\u001b[0m\n",
      "\u001b[91m*ERR* --- *ERR*\u001b[0m\n",
      "\u001b[91m*ERR* Table is not square! *ERR*\u001b[0m\n",
      "\u001b[93m*Padding to square...*\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "docling_md_docs = DoclingReader(export_type=DoclingReader.ExportType.MARKDOWN).load_data(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_nodes = MarkdownNodeParser.from_defaults().get_nodes_from_documents(documents=docling_md_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;36m105\u001b[0m"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## 3.2 Model Architecture\n",
       "\n",
       "Llama 3 uses a standard, dense Transformer architecture (Vaswani et al., 2017). It does not deviate significantly from Llama and Llama 2 (Touvron et al., 2023a,b) in terms of model architecture; our performance gains are primarily driven by improvements in data quality and diversity as well as by increased training scale.\n",
       "\n",
       "We make a few small modifications compared to Llama 2:\n",
       "\n",
       "- Â· We use grouped query attention (GQA; Ainslie et al. (2023)) with 8 key-value heads to improve inference speed and to reduce the size of key-value caches during decoding.\n",
       "- Â· We use an attention mask that prevents self-attention between different documents within the same sequence. We find that this change had limited impact during in standard pre-training, but find it to be important in continued pre-training on very long sequences.\n",
       "\n",
       "Table 3 Overview of the key hyperparameters of Llama 3. We display settings for 8B, 70B, and 405B language models.\n",
       "\n",
       "|                                                        | 8B             | 70B                   | 405B       |\n",
       "|--------------------------------------------------------|----------------|-----------------------|------------|\n",
       "| Layers                                                 | 32             | 80                    | 126        |\n",
       "| Model Dimension                                        | 4,096          | 8192                  | 16,384     |\n",
       "| FFN Dimension                                          | 14,336         | 28,672                | 53,248     |\n",
       "| Attention Heads                                        | 32             | 64                    | 128        |\n",
       "| Key/Value Heads                                        | 8              | 8                     | 8          |\n",
       "| Peak Learning Rate Activation Function Vocabulary Size | 3 Ã— 10 - 4     | 1 . 5 Ã— 10 - 4 SwiGLU | 8 Ã— 10 - 5 |\n",
       "| Positional Embeddings                                  | 128,000 RoPE ( | Î¸ = 500 ,             | 000 )      |\n",
       "\n",
       "- Â· We use a vocabulary with 128K tokens. Our token vocabulary combines 100K tokens from the tiktoken 3 tokenizer with 28K additional tokens to better support non-English languages. Compared to the Llama 2 tokenizer, our new tokenizer improves compression rates on a sample of English data from 3.17 to 3.94 characters per token. This enables the model to \"read\" more text for the same amount of training compute. We also found that adding 28K tokens from select non-English languages improved both compression ratios and downstream performance, with no impact on English tokenization.\n",
       "- Â· We increase the RoPE base frequency hyperparameter to 500,000. This enables us to better support longer contexts; Xiong et al. (2023) showed this value to be effective for context lengths up to 32,768.\n",
       "\n",
       "Llama 3 405B uses an architecture with 126 layers, a token representation dimension of 16,384, and 128 attention heads; see Table 3 for details. This leads to a model size that is approximately compute-optimal according to scaling laws on our data for our training budget of 3 . 8 Ã— 10 25 FLOPs."
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mIPython.core.display.Markdown\u001b[0m\u001b[39m object\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(md_nodes[9].text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">There are multiple Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> models, including at least the 8B, 70B, and 405B versions.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "There are multiple Llama \u001b[1;36m3\u001b[0m models, including at least the 8B, 70B, and 405B versions.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">## <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.1</span> Language\n",
       "\n",
       "Scale. Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> follows the enduring trend of applying straightforward methods at ever increasing scales in \n",
       "foundation models. Improvements are driven by increased compute and improved data, with the 405B model using almost\n",
       "fifty times the pre-training compute budget of Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> 70B. Despite containing 405B parameters, our largest Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "in fact contains fewer parameters than earlier and much less performant models such as PALM <span style=\"font-weight: bold\">(</span>Chowdhery et al., \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"font-weight: bold\">)</span>, due to better understanding of scaling laws <span style=\"font-weight: bold\">(</span>Kaplan et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2020</span>; Hoffmann et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span><span style=\"font-weight: bold\">)</span>. Little is publicly\n",
       "known about the size of other frontier models, such as Claude <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> or GPT <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> <span style=\"font-weight: bold\">(</span>OpenAI, 2023a<span style=\"font-weight: bold\">)</span>, but overall performance \n",
       "is compareable.\n",
       "\n",
       "Small models. Developments in smaller models have paralleled those in large models. Models with fewer parameters \n",
       "can dramatically improve inference cost and simplify deployment <span style=\"font-weight: bold\">(</span>Mehta et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>; Team et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span><span style=\"font-weight: bold\">)</span>. The \n",
       "smaller Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> models achieve this by training far beyond the point of compute optimal training, effectively \n",
       "trading training compute for inference efficiency. An alternative path is to distill larger models into smaller \n",
       "ones, as in Phi <span style=\"font-weight: bold\">(</span>Abdin et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span><span style=\"font-weight: bold\">)</span>.\n",
       "\n",
       "Architectures. While Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> makes minimal architectural modifiations to compared to Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, other recent \n",
       "foundation models have explored other designs. Most notably, mixture of experts architectures <span style=\"font-weight: bold\">(</span>Shazeer et al., \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span>; Lewis et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2021</span>; Fedus et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span>; Zhou et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span><span style=\"font-weight: bold\">)</span> can be used as an efficient way to increase the \n",
       "capacity of a models, such as in Mixtral <span style=\"font-weight: bold\">(</span>Jiang et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span><span style=\"font-weight: bold\">)</span> and Arctic <span style=\"font-weight: bold\">(</span>Snowflake, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span><span style=\"font-weight: bold\">)</span>. Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> outperforms \n",
       "these models, suggesting that dense architectures are not the limiting factor, but there remain numerous trade offs\n",
       "in terms of training and inference efficiency, and model stability at scale.\n",
       "\n",
       "Open source. Open weights foundation models have rapidly improved over the last year, with Llama3-405B now \n",
       "competitive with the current closed weight state-of-the-art. Numerous model families have recently been developed, \n",
       "including Mistral <span style=\"font-weight: bold\">(</span>Jiang et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"font-weight: bold\">)</span>, Falcon <span style=\"font-weight: bold\">(</span>Almazrouei et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"font-weight: bold\">)</span>, MPT <span style=\"font-weight: bold\">(</span>Databricks, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span><span style=\"font-weight: bold\">)</span>, Pythia <span style=\"font-weight: bold\">(</span>Biderman \n",
       "et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"font-weight: bold\">)</span>, Arctic <span style=\"font-weight: bold\">(</span>Snowflake, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span><span style=\"font-weight: bold\">)</span>, OpenELM <span style=\"font-weight: bold\">(</span>Mehta et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span><span style=\"font-weight: bold\">)</span>, OLMo <span style=\"font-weight: bold\">(</span>Groeneveld et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span><span style=\"font-weight: bold\">)</span>, StableLM \n",
       "<span style=\"font-weight: bold\">(</span>Bellagente et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span><span style=\"font-weight: bold\">)</span>, OpenLLaMA <span style=\"font-weight: bold\">(</span>Geng and Liu, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"font-weight: bold\">)</span>, Qwen <span style=\"font-weight: bold\">(</span>Bai et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"font-weight: bold\">)</span>, Gemma <span style=\"font-weight: bold\">(</span>Team et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span><span style=\"font-weight: bold\">)</span>, Grok\n",
       "<span style=\"font-weight: bold\">(</span>XAI, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span><span style=\"font-weight: bold\">)</span>, and Phi <span style=\"font-weight: bold\">(</span>Abdin et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span><span style=\"font-weight: bold\">)</span>.\n",
       "\n",
       "Post-training. Post-training Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> follows the established strategy of instruction tuning <span style=\"font-weight: bold\">(</span>Chung et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span>; \n",
       "Ouyang et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span><span style=\"font-weight: bold\">)</span> followed by alignment with human feedback <span style=\"font-weight: bold\">(</span>Kaufmann et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"font-weight: bold\">)</span>. While some studies have \n",
       "shown the surprising effectiveness of lightweight alignment procedures <span style=\"font-weight: bold\">(</span>Zhou et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span><span style=\"font-weight: bold\">)</span>, Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> uses millions \n",
       "of human instructions and preference judgments to improve the pre-trained model, including\n",
       "\n",
       "techniques such as rejection sampling <span style=\"font-weight: bold\">(</span>Bai et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span><span style=\"font-weight: bold\">)</span>, supervised finetuning <span style=\"font-weight: bold\">(</span>Sanh et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span><span style=\"font-weight: bold\">)</span>, and Direct \n",
       "Preference Optimization <span style=\"font-weight: bold\">(</span>Rafailov et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"font-weight: bold\">)</span>. In order to curate these instruction and preference examples, we \n",
       "deploy earlier versions of Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> to filter <span style=\"font-weight: bold\">(</span>Liu et al., 2024c<span style=\"font-weight: bold\">)</span>, re-write <span style=\"font-weight: bold\">(</span>Pan et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span><span style=\"font-weight: bold\">)</span>, or generate prompts \n",
       "and responses <span style=\"font-weight: bold\">(</span>Liu et al., 2024b<span style=\"font-weight: bold\">)</span> and apply these techniques through multiple rounds of post-training.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "## \u001b[1;36m9.1\u001b[0m Language\n",
       "\n",
       "Scale. Llama \u001b[1;36m3\u001b[0m follows the enduring trend of applying straightforward methods at ever increasing scales in \n",
       "foundation models. Improvements are driven by increased compute and improved data, with the 405B model using almost\n",
       "fifty times the pre-training compute budget of Llama \u001b[1;36m2\u001b[0m 70B. Despite containing 405B parameters, our largest Llama \u001b[1;36m3\u001b[0m\n",
       "in fact contains fewer parameters than earlier and much less performant models such as PALM \u001b[1m(\u001b[0mChowdhery et al., \n",
       "\u001b[1;36m2023\u001b[0m\u001b[1m)\u001b[0m, due to better understanding of scaling laws \u001b[1m(\u001b[0mKaplan et al., \u001b[1;36m2020\u001b[0m; Hoffmann et al., \u001b[1;36m2022\u001b[0m\u001b[1m)\u001b[0m. Little is publicly\n",
       "known about the size of other frontier models, such as Claude \u001b[1;36m3\u001b[0m or GPT \u001b[1;36m4\u001b[0m \u001b[1m(\u001b[0mOpenAI, 2023a\u001b[1m)\u001b[0m, but overall performance \n",
       "is compareable.\n",
       "\n",
       "Small models. Developments in smaller models have paralleled those in large models. Models with fewer parameters \n",
       "can dramatically improve inference cost and simplify deployment \u001b[1m(\u001b[0mMehta et al., \u001b[1;36m2024\u001b[0m; Team et al., \u001b[1;36m2024\u001b[0m\u001b[1m)\u001b[0m. The \n",
       "smaller Llama \u001b[1;36m3\u001b[0m models achieve this by training far beyond the point of compute optimal training, effectively \n",
       "trading training compute for inference efficiency. An alternative path is to distill larger models into smaller \n",
       "ones, as in Phi \u001b[1m(\u001b[0mAbdin et al., \u001b[1;36m2024\u001b[0m\u001b[1m)\u001b[0m.\n",
       "\n",
       "Architectures. While Llama \u001b[1;36m3\u001b[0m makes minimal architectural modifiations to compared to Llama \u001b[1;36m2\u001b[0m, other recent \n",
       "foundation models have explored other designs. Most notably, mixture of experts architectures \u001b[1m(\u001b[0mShazeer et al., \n",
       "\u001b[1;36m2017\u001b[0m; Lewis et al., \u001b[1;36m2021\u001b[0m; Fedus et al., \u001b[1;36m2022\u001b[0m; Zhou et al., \u001b[1;36m2022\u001b[0m\u001b[1m)\u001b[0m can be used as an efficient way to increase the \n",
       "capacity of a models, such as in Mixtral \u001b[1m(\u001b[0mJiang et al., \u001b[1;36m2024\u001b[0m\u001b[1m)\u001b[0m and Arctic \u001b[1m(\u001b[0mSnowflake, \u001b[1;36m2024\u001b[0m\u001b[1m)\u001b[0m. Llama \u001b[1;36m3\u001b[0m outperforms \n",
       "these models, suggesting that dense architectures are not the limiting factor, but there remain numerous trade offs\n",
       "in terms of training and inference efficiency, and model stability at scale.\n",
       "\n",
       "Open source. Open weights foundation models have rapidly improved over the last year, with Llama3-405B now \n",
       "competitive with the current closed weight state-of-the-art. Numerous model families have recently been developed, \n",
       "including Mistral \u001b[1m(\u001b[0mJiang et al., \u001b[1;36m2023\u001b[0m\u001b[1m)\u001b[0m, Falcon \u001b[1m(\u001b[0mAlmazrouei et al., \u001b[1;36m2023\u001b[0m\u001b[1m)\u001b[0m, MPT \u001b[1m(\u001b[0mDatabricks, \u001b[1;36m2024\u001b[0m\u001b[1m)\u001b[0m, Pythia \u001b[1m(\u001b[0mBiderman \n",
       "et al., \u001b[1;36m2023\u001b[0m\u001b[1m)\u001b[0m, Arctic \u001b[1m(\u001b[0mSnowflake, \u001b[1;36m2024\u001b[0m\u001b[1m)\u001b[0m, OpenELM \u001b[1m(\u001b[0mMehta et al., \u001b[1;36m2024\u001b[0m\u001b[1m)\u001b[0m, OLMo \u001b[1m(\u001b[0mGroeneveld et al., \u001b[1;36m2024\u001b[0m\u001b[1m)\u001b[0m, StableLM \n",
       "\u001b[1m(\u001b[0mBellagente et al., \u001b[1;36m2024\u001b[0m\u001b[1m)\u001b[0m, OpenLLaMA \u001b[1m(\u001b[0mGeng and Liu, \u001b[1;36m2023\u001b[0m\u001b[1m)\u001b[0m, Qwen \u001b[1m(\u001b[0mBai et al., \u001b[1;36m2023\u001b[0m\u001b[1m)\u001b[0m, Gemma \u001b[1m(\u001b[0mTeam et al., \u001b[1;36m2024\u001b[0m\u001b[1m)\u001b[0m, Grok\n",
       "\u001b[1m(\u001b[0mXAI, \u001b[1;36m2024\u001b[0m\u001b[1m)\u001b[0m, and Phi \u001b[1m(\u001b[0mAbdin et al., \u001b[1;36m2024\u001b[0m\u001b[1m)\u001b[0m.\n",
       "\n",
       "Post-training. Post-training Llama \u001b[1;36m3\u001b[0m follows the established strategy of instruction tuning \u001b[1m(\u001b[0mChung et al., \u001b[1;36m2022\u001b[0m; \n",
       "Ouyang et al., \u001b[1;36m2022\u001b[0m\u001b[1m)\u001b[0m followed by alignment with human feedback \u001b[1m(\u001b[0mKaufmann et al., \u001b[1;36m2023\u001b[0m\u001b[1m)\u001b[0m. While some studies have \n",
       "shown the surprising effectiveness of lightweight alignment procedures \u001b[1m(\u001b[0mZhou et al., \u001b[1;36m2024\u001b[0m\u001b[1m)\u001b[0m, Llama \u001b[1;36m3\u001b[0m uses millions \n",
       "of human instructions and preference judgments to improve the pre-trained model, including\n",
       "\n",
       "techniques such as rejection sampling \u001b[1m(\u001b[0mBai et al., \u001b[1;36m2022\u001b[0m\u001b[1m)\u001b[0m, supervised finetuning \u001b[1m(\u001b[0mSanh et al., \u001b[1;36m2022\u001b[0m\u001b[1m)\u001b[0m, and Direct \n",
       "Preference Optimization \u001b[1m(\u001b[0mRafailov et al., \u001b[1;36m2023\u001b[0m\u001b[1m)\u001b[0m. In order to curate these instruction and preference examples, we \n",
       "deploy earlier versions of Llama \u001b[1;36m3\u001b[0m to filter \u001b[1m(\u001b[0mLiu et al., 2024c\u001b[1m)\u001b[0m, re-write \u001b[1m(\u001b[0mPan et al., \u001b[1;36m2024\u001b[0m\u001b[1m)\u001b[0m, or generate prompts \n",
       "and responses \u001b[1m(\u001b[0mLiu et al., 2024b\u001b[1m)\u001b[0m and apply these techniques through multiple rounds of post-training.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "md_index = VectorStoreIndex(nodes=md_nodes, embed_model=embed_model)\n",
    "md_query_engine = md_index.as_query_engine(llm=llm)\n",
    "response = md_query_engine.query(\"How many llama3 models are there?\")\n",
    "print(response.response)\n",
    "print(response.source_nodes[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting simple: bm25\n",
    "\n",
    "BM25 is a simple retrieval method that uses the BM25 algorithm to score the relevance of each document to the query.\n",
    "The BM25 algorithm is a probabilistic retrieval model that uses the term frequency and inverse document frequency of the query terms to score the relevance of each document.\n",
    "You should be familiar with the basic idea of tf-idf from your NLP class - you can think of BM25 as a generalization of tf-idf that takes into account more factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Retriever.from_defaults(nodes=md_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.4940</span>\n",
       "Text:\n",
       "## <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.4</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> Safety Results\n",
       "\n",
       "We first highlight Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>'s general behavior along various axes and then describe results for each specific new \n",
       "capability and our effectiveness at mitigating the safety risks.\n",
       "\n",
       "Overall performance. A comparison of Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>'s final violation and false refusal rates with similar models can be \n",
       "found in Figures <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span> and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>. These results focus on our largest parameter size Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> 405B model, compared to \n",
       "relevant competitors. Two of the competitors are end-to-end systems acce<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Score: \u001b[1;36m2.4940\u001b[0m\n",
       "Text:\n",
       "## \u001b[1;36m5.4\u001b[0m.\u001b[1;36m4\u001b[0m Safety Results\n",
       "\n",
       "We first highlight Llama \u001b[1;36m3\u001b[0m's general behavior along various axes and then describe results for each specific new \n",
       "capability and our effectiveness at mitigating the safety risks.\n",
       "\n",
       "Overall performance. A comparison of Llama \u001b[1;36m3\u001b[0m's final violation and false refusal rates with similar models can be \n",
       "found in Figures \u001b[1;36m19\u001b[0m and \u001b[1;36m20\u001b[0m. These results focus on our largest parameter size Llama \u001b[1;36m3\u001b[0m 405B model, compared to \n",
       "relevant competitors. Two of the competitors are end-to-end systems acce\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.5804</span>\n",
       "Text:\n",
       "## <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.1</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> Contamination Analysis\n",
       "\n",
       "We conduct a contamination analysis to estimate to what extent benchmark scores may be influenced by contamination \n",
       "of the evaluation data in the pre-training corpus. In previous work, several different contamination methods have \n",
       "been used, with various different hyperparameters - we refer to Singh et al. <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span><span style=\"font-weight: bold\">)</span> for an overview. Any of these \n",
       "methods can suffer from false positives and negatives, and how to best run contamination analyses is currently \n",
       "still an open<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Score: \u001b[1;36m1.5804\u001b[0m\n",
       "Text:\n",
       "## \u001b[1;36m5.1\u001b[0m.\u001b[1;36m4\u001b[0m Contamination Analysis\n",
       "\n",
       "We conduct a contamination analysis to estimate to what extent benchmark scores may be influenced by contamination \n",
       "of the evaluation data in the pre-training corpus. In previous work, several different contamination methods have \n",
       "been used, with various different hyperparameters - we refer to Singh et al. \u001b[1m(\u001b[0m\u001b[1;36m2024\u001b[0m\u001b[1m)\u001b[0m for an overview. Any of these \n",
       "methods can suffer from false positives and negatives, and how to best run contamination analyses is currently \n",
       "still an open\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for node in bm25.retrieve(\"How many llama3 models are there?\"):\n",
    "    print(f\"Score: {node.score:.4f}\\nText:\\n{node.node.text[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense retrieval (vector search)\n",
    "\n",
    "BM25 is a simple and fast method that depends on word matching.\n",
    "But if we want to do more complex retrieval, we can use dense retrieval.\n",
    "We represent both our query and documents as vectors and use a similarity metric to find the most relevant documents.\n",
    "This is what's been going on under the hood in the previous examples using `VectorStoreIndex`.\n",
    "\n",
    "Since most of the mechanics are taken care for us under the hood, let's examine what goes on under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_documents = [\n",
    "    \"My favorite type of dog is a golden retriever.\",\n",
    "    \"I like to eat pizza with my friends.\",\n",
    "    \"I like to go to the gym in the morning.\",\n",
    "    \"I like to play basketball with my friends.\",\n",
    "]\n",
    "\n",
    "embeddings = np.array(embed_model.get_text_embedding_batch(sample_documents))\n",
    "\n",
    "query = \"What do I like to do with friends?\"\n",
    "query_embedding = np.array(embed_model.get_text_embedding(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0.40551496\u001b[0m, \u001b[1;36m0.65599524\u001b[0m, \u001b[1;36m0.45809953\u001b[0m, \u001b[1;36m0.64565545\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(query_embedding.reshape(1, -1), embeddings).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a `VectorStoreIndex`, we can use the `as_retriever` method to get a retriever object.\n",
    "This uses dense retrieval under the hood, since it already has an embedding model as a part of the class.\n",
    "Here, we use the `similarity_top_k` parameter to limit the number of results to 2 and show the cosine similarity score along with the beginning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7720</span>\n",
       "Text:\n",
       "## <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.1</span> Language\n",
       "\n",
       "Scale. Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> follows the enduring trend of applying straightforward methods at ever increasing scales in \n",
       "foundation models. Improvements are driven by increased compute and improved data, with the 405B model using almost\n",
       "fifty times the pre-training compute budget of Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> 70B. Despite containing 405B parameters, our largest Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "in fact contains fewer parameters than earlier and much less performant models such as PALM <span style=\"font-weight: bold\">(</span>Chowdhery et al., \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"font-weight: bold\">)</span>, due to better understandin<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Score: \u001b[1;36m0.7720\u001b[0m\n",
       "Text:\n",
       "## \u001b[1;36m9.1\u001b[0m Language\n",
       "\n",
       "Scale. Llama \u001b[1;36m3\u001b[0m follows the enduring trend of applying straightforward methods at ever increasing scales in \n",
       "foundation models. Improvements are driven by increased compute and improved data, with the 405B model using almost\n",
       "fifty times the pre-training compute budget of Llama \u001b[1;36m2\u001b[0m 70B. Despite containing 405B parameters, our largest Llama \u001b[1;36m3\u001b[0m\n",
       "in fact contains fewer parameters than earlier and much less performant models such as PALM \u001b[1m(\u001b[0mChowdhery et al., \n",
       "\u001b[1;36m2023\u001b[0m\u001b[1m)\u001b[0m, due to better understandin\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7673</span>\n",
       "Text:\n",
       "## <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.2</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> Math and Reasoning Benchmarks\n",
       "\n",
       "Our math and reasoning benchmark results are presented in Table <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> 8B model outperforms other models of \n",
       "similar sizes on GSM8K, MATH, and GPQA. Our 70B model performs significantly better than other models in its class \n",
       "on all the benchmarks. Finally, Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> 405B model is the best in its category\n",
       "\n",
       "Table <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> Multilingual benchmarks . For MGSM <span style=\"font-weight: bold\">(</span>Shi et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span><span style=\"font-weight: bold\">)</span>, we report <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>-shot CoT results for our Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> \n",
       "models. Multilingual MMLU is an internal ben<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Score: \u001b[1;36m0.7673\u001b[0m\n",
       "Text:\n",
       "## \u001b[1;36m5.2\u001b[0m.\u001b[1;36m5\u001b[0m Math and Reasoning Benchmarks\n",
       "\n",
       "Our math and reasoning benchmark results are presented in Table \u001b[1;36m2\u001b[0m. Llama \u001b[1;36m3\u001b[0m 8B model outperforms other models of \n",
       "similar sizes on GSM8K, MATH, and GPQA. Our 70B model performs significantly better than other models in its class \n",
       "on all the benchmarks. Finally, Llama \u001b[1;36m3\u001b[0m 405B model is the best in its category\n",
       "\n",
       "Table \u001b[1;36m20\u001b[0m Multilingual benchmarks . For MGSM \u001b[1m(\u001b[0mShi et al., \u001b[1;36m2022\u001b[0m\u001b[1m)\u001b[0m, we report \u001b[1;36m0\u001b[0m-shot CoT results for our Llama \u001b[1;36m3\u001b[0m \n",
       "models. Multilingual MMLU is an internal ben\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"How many llama3 models are there?\"\n",
    "top_results = md_index.as_retriever(similarity_top_k=2).retrieve(query)\n",
    "for result in top_results:\n",
    "    print(f\"Score: {result.score:.4f}\\nText:\\n{result.node.text[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid search: query rewriting and reciprocal ranking\n",
    "\n",
    "Sometimes, you may want several methods of searching over your data, then combining the results.\n",
    "This is known as hybrid search.\n",
    "\n",
    "Haveing multiple retrievers may not mean having separate objects - we may just have multiple queries.\n",
    "In this example, we'll use an LLM to rewrite our query into multiple queries, then use a dense retriever to find the most relevant documents.\n",
    "Finally, we'll use reciprocal ranking to re-rank the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_retriever = md_index.as_retriever(similarity_top_k=5)\n",
    "hybrid_retriever = QueryFusionRetriever(\n",
    "    [dense_retriever],\n",
    "    num_queries=3,\n",
    "    use_async=False,\n",
    "    mode='reciprocal_rerank',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "What are the different versions of the Llama3 model?\n",
      "How many variants of the Llama3 model exist?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0497</span>\n",
       "Text:\n",
       "## <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.1</span> Language\n",
       "\n",
       "Scale. Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> follows the enduring trend of applying straightforward methods at ever increasing scales in \n",
       "foundation models. Improvements are driven by increased compute and improved data, with the 405B model using almost\n",
       "fifty times the pre-training compute budget of Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> 70B. Despite containing 405B parameters, our largest Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "in fact contains fewer parameters than earlier and much less performant models such as PALM <span style=\"font-weight: bold\">(</span>Chowdhery et al., \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"font-weight: bold\">)</span>, due to better understandin<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Score: \u001b[1;36m0.0497\u001b[0m\n",
       "Text:\n",
       "## \u001b[1;36m9.1\u001b[0m Language\n",
       "\n",
       "Scale. Llama \u001b[1;36m3\u001b[0m follows the enduring trend of applying straightforward methods at ever increasing scales in \n",
       "foundation models. Improvements are driven by increased compute and improved data, with the 405B model using almost\n",
       "fifty times the pre-training compute budget of Llama \u001b[1;36m2\u001b[0m 70B. Despite containing 405B parameters, our largest Llama \u001b[1;36m3\u001b[0m\n",
       "in fact contains fewer parameters than earlier and much less performant models such as PALM \u001b[1m(\u001b[0mChowdhery et al., \n",
       "\u001b[1;36m2023\u001b[0m\u001b[1m)\u001b[0m, due to better understandin\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Score: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0489</span>\n",
       "Text:\n",
       "## <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.2</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> Math and Reasoning Benchmarks\n",
       "\n",
       "Our math and reasoning benchmark results are presented in Table <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> 8B model outperforms other models of \n",
       "similar sizes on GSM8K, MATH, and GPQA. Our 70B model performs significantly better than other models in its class \n",
       "on all the benchmarks. Finally, Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> 405B model is the best in its category\n",
       "\n",
       "Table <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span> Multilingual benchmarks . For MGSM <span style=\"font-weight: bold\">(</span>Shi et al., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span><span style=\"font-weight: bold\">)</span>, we report <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>-shot CoT results for our Llama <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> \n",
       "models. Multilingual MMLU is an internal ben<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Score: \u001b[1;36m0.0489\u001b[0m\n",
       "Text:\n",
       "## \u001b[1;36m5.2\u001b[0m.\u001b[1;36m5\u001b[0m Math and Reasoning Benchmarks\n",
       "\n",
       "Our math and reasoning benchmark results are presented in Table \u001b[1;36m2\u001b[0m. Llama \u001b[1;36m3\u001b[0m 8B model outperforms other models of \n",
       "similar sizes on GSM8K, MATH, and GPQA. Our 70B model performs significantly better than other models in its class \n",
       "on all the benchmarks. Finally, Llama \u001b[1;36m3\u001b[0m 405B model is the best in its category\n",
       "\n",
       "Table \u001b[1;36m20\u001b[0m Multilingual benchmarks . For MGSM \u001b[1m(\u001b[0mShi et al., \u001b[1;36m2022\u001b[0m\u001b[1m)\u001b[0m, we report \u001b[1;36m0\u001b[0m-shot CoT results for our Llama \u001b[1;36m3\u001b[0m \n",
       "models. Multilingual MMLU is an internal ben\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = hybrid_retriever.retrieve(\"How many llama3 models are there?\")\n",
    "for result in results:\n",
    "    print(f\"Score: {result.score:.4f}\\nText:\\n{result.node.text[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking\n",
    "\n",
    "Often, retrieval only gives us a good first pass at finding relevant documents.\n",
    "We can use re-ranking to improve the results.\n",
    "There are several re-ranking methods, but in this case we'll use a cross-encoder to re-rank the results.\n",
    "We'll also introduce a more low-level method for organizing LlamaIndex flows called workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    Context,\n",
    "    step,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Event\n",
    ")\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk throught the code below step by step.\n",
    "First, we define the events that will be passed between the steps.\n",
    "This includes a retrieval result, a re-ranking result, and a prompt for the LLM.\n",
    "The start and stop are already taken care of for us by `StartEvent` and `StopEvent`.\n",
    "\n",
    "Next, we define our `RAGWithReRank` workflow.\n",
    "We initialize it with an index and LLM, which we will use for retrieval and answering and the cross-encoder for re-ranking.\n",
    "We also define exactly what to do for each step of the workflow.\n",
    "It knows what to do because of the `@step` decorator and the types annotations of each method.\n",
    "\n",
    "Finally, we can run the workflow with a query.\n",
    "The workflow will automatically handle passing the events between the steps, so we don't have to worry about that.\n",
    "We can observe that the results are just what we expect as we saw from the previous examples.\n",
    "One advantage of this method is that it's much more flexible - you can include arbitrary code, loops, conditionals, etc.\n",
    "\n",
    "For more detailed documentation on workflows, see [here](https://docs.llamaindex.ai/en/stable/understanding/workflows/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the events that will be passed between the steps\n",
    "class RetrievalResult(Event):\n",
    "    results: List[NodeWithScore]\n",
    "\n",
    "class ReRankResult(Event):\n",
    "    results: List[NodeWithScore]\n",
    "\n",
    "class RagPrompt(Event):\n",
    "    prompt: str\n",
    "\n",
    "# Define the workflow\n",
    "class RAGWithReRank(Workflow):\n",
    "    # Initialize the workflow with an index and LLM\n",
    "    def __init__(self, index: VectorStoreIndex = md_index, llm: OpenAI = OpenAI(model=\"gpt-4o-mini\")):\n",
    "        super().__init__(timeout=10, verbose=False)\n",
    "        self.index = index\n",
    "        self.llm = llm\n",
    "        self.reranker = SentenceTransformerRerank(similarity_top_k=5)\n",
    "\n",
    "    # The start method is called when the workflow is run.\n",
    "    # This is the first step of the workflow that takes in a query and returns a retrieval result.\n",
    "    # Since we want to use a course retriever, we use a top k of 20.\n",
    "    @step\n",
    "    async def start(self, ctx: Context, ev: StartEvent) -> RetrievalResult:\n",
    "        query = ev.query\n",
    "        await ctx.set('query', query)\n",
    "        results = self.index.as_retriever(similarity_top_k=20).retrieve(query)\n",
    "        return RetrievalResult(results=results)\n",
    "    \n",
    "    # This step takes the retrieval results and re-ranks them.\n",
    "    # Notice that the similarity top k is set to 5, so we only keep the top 5 results.\n",
    "    # This is smaller than the retrieval step since this is about refining a smaller number of results.\n",
    "    @step\n",
    "    async def rerank(self, ctx: Context, ev: RetrievalResult) -> ReRankResult:\n",
    "        results = ev.results\n",
    "        query = await ctx.get('query')\n",
    "        reranked = self.reranker.postprocess_nodes(nodes=results, query_str=query)\n",
    "        return ReRankResult(results=reranked)\n",
    "    \n",
    "    # This step creates a prompt for the LLM.\n",
    "    # It does this by joining the re-ranked results into a single string and formatting it with the query.\n",
    "    @step\n",
    "    async def create_prompt(self, ctx: Context, ev: ReRankResult) -> RagPrompt:\n",
    "        reranked_results = ev.results\n",
    "        await ctx.set('reranked_results', reranked_results)\n",
    "        query = await ctx.get('query')\n",
    "        reranked_str = '\\n\\n'.join(i.text for i in reranked_results)\n",
    "        prompt = f\"\"\"\\\n",
    "Here is some relevant context:\n",
    "--------------------------------\n",
    "{reranked_str}\n",
    "--------------------------------\n",
    "Based on the above information and not prior knowledge, please answer the question.\n",
    "Question:\n",
    "{query}\n",
    "\"\"\"\n",
    "        return RagPrompt(prompt=prompt)\n",
    "\n",
    "    # This step takes the prompt and uses the LLM to answer the question.\n",
    "    # It also takes the re-ranked results and attaches them to the response in case we want to see the nodes.\n",
    "    @step\n",
    "    async def answer(self, ctx: Context, ev: RagPrompt) -> StopEvent:\n",
    "        prompt = ev.prompt\n",
    "        ranked_results = await ctx.get('reranked_results')\n",
    "        messages = [\n",
    "            ChatMessage.from_str(prompt)\n",
    "        ]\n",
    "        answer = await self.llm.achat(messages)\n",
    "        result = {\n",
    "            \"response\": answer,\n",
    "            \"source_nodes\": ranked_results\n",
    "        }\n",
    "        return StopEvent(result=result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = RAGWithReRank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'response'\u001b[0m: \u001b[1;35mChatResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmessage\u001b[0m=\u001b[1;35mChatMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mrole\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mMessageRole.ASSISTANT:\u001b[0m\u001b[39m \u001b[0m\u001b[32m'assistant'\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mcontent\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'Based on the provided information, there are three Llama 3 models mentioned: Llama 3 405B, Llama 3 70B, and Llama 3 8B.'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33madditional_kwargs\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[33mraw\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mChatCompletion\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mid\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'chatcmpl-AaG8rHJuwOFEmokijOKLTp1Fb7OLz'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mchoices\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m[\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[1;35mChoice\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[33mfinish_reason\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'stop'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[33mindex\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[33mlogprobs\u001b[0m\u001b[39m=\u001b[0m\u001b[3;35mNone\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[33mmessage\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mChatCompletionMessage\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mcontent\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'Based on the provided information, there are three Llama 3 models mentioned: Llama 3 405B, Llama 3 70B, and Llama 3 8B.'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mrefusal\u001b[0m\u001b[39m=\u001b[0m\u001b[3;35mNone\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mrole\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'assistant'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33maudio\u001b[0m\u001b[39m=\u001b[0m\u001b[3;35mNone\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mfunction_call\u001b[0m\u001b[39m=\u001b[0m\u001b[3;35mNone\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mtool_calls\u001b[0m\u001b[39m=\u001b[0m\u001b[3;35mNone\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mcreated\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m1733205221\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mmodel\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'gpt-4o-mini-2024-07-18'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mobject\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'chat.completion'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mservice_tier\u001b[0m\u001b[39m=\u001b[0m\u001b[3;35mNone\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33msystem_fingerprint\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'fp_0705bf87c0'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33musage\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mCompletionUsage\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mcompletion_tokens\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m41\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mprompt_tokens\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m476\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mtotal_tokens\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m517\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mcompletion_tokens_details\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mCompletionTokensDetails\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[33maccepted_prediction_tokens\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[33maudio_tokens\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[33mreasoning_tokens\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[33mrejected_prediction_tokens\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m0\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mprompt_tokens_details\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mPromptTokensDetails\u001b[0m\u001b[1;39m(\u001b[0m\u001b[33maudio_tokens\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[39m, \u001b[0m\u001b[33mcached_tokens\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[33mdelta\u001b[0m\u001b[39m=\u001b[0m\u001b[3;35mNone\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[33mlogprobs\u001b[0m\u001b[39m=\u001b[0m\u001b[3;35mNone\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[33madditional_kwargs\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[32m'prompt_tokens'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m476\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'completion_tokens'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m41\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'total_tokens'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m517\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[32m'source_nodes'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;39m[\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;35mNodeWithScore\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mnode\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mTextNode\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mid_\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'3c3f9069-0bc7-418e-a459-97e33db9c484'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33membedding\u001b[0m\u001b[39m=\u001b[0m\u001b[3;35mNone\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mmetadata\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[32m'header_path'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'/The Llama 3 Herd of Models/'\u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mexcluded_embed_metadata_keys\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mexcluded_llm_metadata_keys\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mrelationships\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m                    <NodeRelationship.SOURCE: \u001b[0m\u001b[32m'1'\u001b[0m\u001b[39m>: \u001b[0m\u001b[1;35mRelatedNodeInfo\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mnode_id\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'4ace5d67-75b5-4b0d-89ab-750543ed4ab7'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mnode_type\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'4'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mmetadata\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mhash\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'6580628c19eadc016add8c7cfe05a7a0156b41500dce909861246ea20ca828b5'\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                    <NodeRelationship.PREVIOUS: \u001b[0m\u001b[32m'2'\u001b[0m\u001b[39m>: \u001b[0m\u001b[1;35mRelatedNodeInfo\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mnode_id\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'bedf1b31-e019-4100-a894-47188fb67ef5'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mnode_type\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'1'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mmetadata\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[32m'header_path'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'/The Llama 3 Herd of Models/'\u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mhash\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'c4ee18ed68d45edbb5b9bdda28884635f25c9af1a3e33cbcfc6529fa56052f76'\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                    <NodeRelationship.NEXT: \u001b[0m\u001b[32m'3'\u001b[0m\u001b[39m>: \u001b[0m\u001b[1;35mRelatedNodeInfo\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mnode_id\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'a3552214-5773-4600-9007-ac44eb159207'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mnode_type\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'1'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mmetadata\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[32m'header_path'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'/The Llama 3 Herd of Models/'\u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mhash\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'7229924e8da6cbb402c501a188dc1f23eec5d60b4f0e30d3b09e99d01b06afcf'\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mmetadata_template\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32mkey\u001b[0m\u001b[32m}\u001b[0m\u001b[32m: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mvalue\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mmetadata_separator\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'\\n'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mtext\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'## 9 Related Work\\n\\nThe development of Llama 3 builds on a large body of prior work studying foundation models for language, images, videos, and speech. A comprehensive overview of that work is outside the scope of this paper; we refer the reader to Bordes et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m; Madan et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m; Zhao et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2023a\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for such overviews. Below, we briefly outline seminal works that directly influenced the development of Llama 3.'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mmimetype\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'text/plain'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mstart_char_idx\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m277447\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mend_char_idx\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m277871\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mmetadata_seperator\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'\\n'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mtext_template\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32mmetadata_str\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32mcontent\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mscore\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;36m.5473752\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;35mNodeWithScore\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mnode\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mTextNode\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mid_\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'384080fc-0834-4825-b720-b34b803cf146'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33membedding\u001b[0m\u001b[39m=\u001b[0m\u001b[3;35mNone\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mmetadata\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[32m'header_path'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'/The Llama 3 Herd of Models/'\u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mexcluded_embed_metadata_keys\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mexcluded_llm_metadata_keys\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                \u001b[0m\u001b[33mrelationships\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\n",
       "\u001b[39m                    <NodeRelationship.SOURCE: \u001b[0m\u001b[32m'1'\u001b[0m\u001b[39m>: \u001b[0m\u001b[1;35mRelatedNodeInfo\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mnode_id\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'4ace5d67-75b5-4b0d-89ab-750543ed4ab7'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mnode_type\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'4'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mmetadata\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mhash\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'6580628c19eadc016add8c7cfe05a7a0156b41500dce909861246ea20ca828b5'\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                    <NodeRelationship.PREVIOUS: \u001b[0m\u001b[32m'2'\u001b[0m\u001b[39m>: \u001b[0m\u001b[1;35mRelatedNodeInfo\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mnode_id\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'a993cf25-c35e-4e43-a027-0b466c8dcc4e'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mnode_type\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'1'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mmetadata\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\u001b[32m'header_path'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'/The Llama 3 Herd of Models/'\u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                        \u001b[0m\u001b[33mhash\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'506b003dcf96adb4cdd4a8013436ae1548f57ff1470daef51d1f89067cdf2417'\u001b[0m\n",
       "\u001b[39m                    \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m                    <NodeRelationship.NEXT: \u001b[0m\u001b[32m'3'\u001b[0m\u001b[1m>\u001b[0m: \u001b[1;35mRelatedNodeInfo\u001b[0m\u001b[1m(\u001b[0m\n",
       "                        \u001b[33mnode_id\u001b[0m=\u001b[32m'6ca676d2-8881-4c73-bbad-91b0f2dd4237'\u001b[0m,\n",
       "                        \u001b[33mnode_type\u001b[0m=\u001b[32m'1'\u001b[0m,\n",
       "                        \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'header_path'\u001b[0m: \u001b[32m'/The Llama 3 Herd of Models/'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                        \u001b[33mhash\u001b[0m=\u001b[32m'1a81dd1953bdc6e1a0ba1f0e31ad14b4708980f28361b896c36c3067a8cf7592'\u001b[0m\n",
       "                    \u001b[1m)\u001b[0m\n",
       "                \u001b[1m}\u001b[0m,\n",
       "                \u001b[33mmetadata_template\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32mkey\u001b[0m\u001b[32m}\u001b[0m\u001b[32m: \u001b[0m\u001b[32m{\u001b[0m\u001b[32mvalue\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[33mmetadata_separator\u001b[0m=\u001b[32m'\\n'\u001b[0m,\n",
       "                \u001b[33mtext\u001b[0m=\u001b[32m'## 5.2.4 Multilingual Benchmarks\\n\\nLlama 3 supports 8 languages - English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai, although the underlying foundation model has been trained on a broader collection of languages. 9 In Table 20, we show results from evaluating Llama 3 on the multilingual MMLU \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHendrycks et al., 2021a\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and Multilingual Grade School Math \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMGSM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m(\u001b[0m\u001b[32mShi et al., 2022\u001b[0m\u001b[32m)\u001b[0m\u001b[32m benchmarks.\\n\\nMultilingual MMLU. We translate MMLU questions, few-shot examples, and answers using Google Translate. We leave the task instructions in English and perform the evaluation in a 5-shot setting. In Table 20, we report average results across German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\\n\\nMGSM \u001b[0m\u001b[32m(\u001b[0m\u001b[32mShi et al., 2022\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. We use the same native prompts as in simple-evals \u001b[0m\u001b[32m(\u001b[0m\u001b[32mOpenAI, 2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for testing our models in a 0-shot CoT setting. In Table 20, we report averge results across languages covered in MGSM benchmark.\\n\\nWe find that Llama 3 405B outperforms most other models on MGSM, achieving an average of 91.6%. On MMLU, in line with English MMLU results shown above, Llama 3 405B falls behind GPT-4o by 2%. On the other hand, both Llama 3 70B and 8B models demonstrate strong performance, leading among competitors with a wide margin on both tasks.'\u001b[0m,\n",
       "                \u001b[33mmimetype\u001b[0m=\u001b[32m'text/plain'\u001b[0m,\n",
       "                \u001b[33mstart_char_idx\u001b[0m=\u001b[1;36m150351\u001b[0m,\n",
       "                \u001b[33mend_char_idx\u001b[0m=\u001b[1;36m151621\u001b[0m,\n",
       "                \u001b[33mmetadata_seperator\u001b[0m=\u001b[32m'\\n'\u001b[0m,\n",
       "                \u001b[33mtext_template\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32mmetadata_str\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32mcontent\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33mscore\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.544649\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await rag.run(query=\"How many llama3 models are there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Hello, world!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Hello, world!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MyWorkflow(Workflow):\n",
    "    @step\n",
    "    async def my_step(self, ev: StartEvent) -> StopEvent:\n",
    "        # do something here\n",
    "        return StopEvent(result=\"Hello, world!\")\n",
    "\n",
    "\n",
    "w = MyWorkflow(timeout=10, verbose=False)\n",
    "result = await w.run()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>1 <span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold; text-decoration: underline\">from</span><span style=\"font-weight: bold; text-decoration: underline\"> </span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">llama_index.utils.workflow</span><span style=\"font-weight: bold; text-decoration: underline\"> </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold; text-decoration: underline\">import</span><span style=\"font-weight: bold; text-decoration: underline\"> draw_all_possible_flows</span>                               <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">IPython.display</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> HTML                                                             <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>draw_all_possible_flows(MyWorkflow, filename=<span style=\"color: #808000; text-decoration-color: #808000\">\"flow.html\"</span>)                                    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ModuleNotFoundError: </span>No module named <span style=\"color: #008000; text-decoration-color: #008000\">'llama_index.utils'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31mâ•­â”€\u001b[0m\u001b[31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[31mâ”€â•®\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m1 \u001b[1;4;94mfrom\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mllama_index\u001b[0m\u001b[1;4;96m.\u001b[0m\u001b[1;4;96mutils\u001b[0m\u001b[1;4;96m.\u001b[0m\u001b[1;4;96mworkflow\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;94mimport\u001b[0m\u001b[1;4m draw_all_possible_flows\u001b[0m                               \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m2 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mIPython\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mdisplay\u001b[0m \u001b[94mimport\u001b[0m HTML                                                             \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m3 \u001b[0m                                                                                             \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ”‚\u001b[0m   \u001b[2m4 \u001b[0mdraw_all_possible_flows(MyWorkflow, filename=\u001b[33m\"\u001b[0m\u001b[33mflow.html\u001b[0m\u001b[33m\"\u001b[0m)                                    \u001b[31mâ”‚\u001b[0m\n",
       "\u001b[31mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
       "\u001b[1;91mModuleNotFoundError: \u001b[0mNo module named \u001b[32m'llama_index.utils'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "from IPython.display import HTML\n",
    "\n",
    "draw_all_possible_flows(MyWorkflow, filename=\"flow.html\")\n",
    "\n",
    "with open(\"flow.html\", \"r\") as f:\n",
    "    display(HTML(f.read()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Chat with PDF (30 minutes)\n",
    "\n",
    "Your goal in this exercise is to create a Gradio interface for a question-answering system.\n",
    "Your application should:\n",
    "- Use the query engine created above to answer questions about the uploaded PDF\n",
    "- Display the question and answer in the UI. If using QueryEngine, use a question and answer format. If using ChatEngine, use a chat format.\n",
    "\n",
    "If you need a challenge:\n",
    "- Use the `gr.File` component to allow the user to upload ANY pdf and ask question about it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from tempfile import TemporaryDirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the ingest_documents function:\n",
    "def ingest_documents(file_obj):\n",
    "    \"\"\"\n",
    "    Process an uploaded PDF file and create a query engine for it.\n",
    "    \n",
    "    Args:\n",
    "        file_obj: Gradio file upload object containing the PDF\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (chat_engine, cleared_file_upload, filename)\n",
    "    \"\"\"\n",
    "    if file_obj is None:\n",
    "        return None, None, \"\"\n",
    "        \n",
    "    with TemporaryDirectory() as temp_dir:\n",
    "        file_path = os.path.join(temp_dir, 'tmp.pdf')\n",
    "        # Get the file bytes from the Gradio upload object\n",
    "        file_bytes = open(file_obj, \"rb\").read()\n",
    "        \n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(file_bytes)\n",
    "            \n",
    "        documents = SimpleDirectoryReader(temp_dir).load_data()\n",
    "        llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "        index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "        chat_engine = index.as_chat_engine(llm=llm)\n",
    "        \n",
    "    return chat_engine, None, Path(file_obj.name).name, []\n",
    "\n",
    "def chat(message, history, chat_engine):\n",
    "    history.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        }\n",
    "    )\n",
    "    response = chat_engine.chat(message)\n",
    "    history.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": response.response\n",
    "    })\n",
    "    return history, None\n",
    "\n",
    "# In the Blocks interface:\n",
    "with gr.Blocks() as demo:\n",
    "    chat_engine_state = gr.State()  # Renamed for clarity\n",
    "    gr.Markdown(\"## RAG Demo: Question answering with a PDF\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):   \n",
    "            file_upload = gr.File(label=\"Upload a PDF file\", file_types=[\".pdf\"], file_count=\"single\")\n",
    "            submit_button = gr.Button(\"Submit\")\n",
    "            pdf_name = gr.Textbox(label=\"You are asking about...\")\n",
    "        with gr.Column(scale=5):\n",
    "            chatbot = gr.Chatbot(label=\"Chat with the uploaded PDF\", type='messages')\n",
    "            input = gr.Textbox(label=\"Enter a question\", interactive=True)\n",
    "\n",
    "    # Update to only store the chat_engine in the State\n",
    "    submit_button.click(\n",
    "        fn=ingest_documents, \n",
    "        inputs=file_upload, \n",
    "        outputs=[chat_engine_state, file_upload, pdf_name, chatbot]\n",
    "    )\n",
    "    input.submit(\n",
    "        fn=chat, \n",
    "        inputs=[input, chatbot, chat_engine_state], \n",
    "        outputs=[chatbot, input]\n",
    "    )\n",
    "\n",
    "demo.launch(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
