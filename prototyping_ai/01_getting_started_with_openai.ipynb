{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4HDOGmN6OmF0",
        "uZDWgZfZQGcU",
        "XfqCJ9bmLDG2",
        "e-_7WDTwPyZo",
        "JTQ-yAmFQVOM",
        "-bHAfnK9YizZ"
      ],
      "authorship_tag": "ABX9TyN4sEMBeyunEgNWmQG7a8bt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgfrantz/CTME-llm-lecture-resources/blob/main/prototyping_ai/01_getting_started_with_openai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making our first LLM API call\n",
        "\n"
      ],
      "metadata": {
        "id": "4HDOGmN6OmF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "uZDWgZfZQGcU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYQyB8QUIhOm",
        "outputId": "c2ea058b-05b4-4a65-d780-b2c0943bf41b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/387.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m387.1/387.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -Uqqqq openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from rich import print"
      ],
      "metadata": {
        "id": "z7bXGANBPZk-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting our `OPENAI_API_KEY` environment variable\n",
        "\n",
        "When we use any LLM provider like OpenAI, Anthropic, or Google, we need some way to tell them who we are making the request.\n",
        "Today, we'll be using OpenAI.\n",
        "The most straightforward way to provide this key is through the environment variable `OPENAI_API_KEY`.\n",
        "The OpenAI python client looks for this environment variable to use in authentication.\n",
        "\n",
        "In the cell below, we load it from the Google Colab secrets manager on the left ðŸ‘ˆ.\n",
        "Before runnign this code, make sure your API key is set as shown below:\n",
        "\n",
        "![](https://github.com/mgfrantz/CTME-llm-lecture-resources/blob/main/images/colabSecrets.png?raw=true)"
      ],
      "metadata": {
        "id": "XfqCJ9bmLDG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the OPENAI_API_KEY environment variable\n",
        "from google.colab import userdata # import the environment variables from secrets\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY') # Set the OPENAI_API_KEY environmnet variable"
      ],
      "metadata": {
        "id": "N2nPVwzrIpPQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Under the hood: `curl`\n",
        "\n",
        "Almost all of the interactions we will have with LLMs are through API calls.\n",
        "Below is one of the most low-level ways we can call an LLM, using the `curl` command.\n",
        "This command gives us a lot of information about how the API request is structured.\n",
        "We pass a JSON with an authorization header containing our `OPENAI_API_KEY`.\n",
        "We also pass the model we want to call, the chat messages, and hyperparameters such as `temperature` that help control how text is generated."
      ],
      "metadata": {
        "id": "e-_7WDTwPyZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the API call to OpenAI and store the response in test.json\n",
        "!curl https://api.openai.com/v1/chat/completions \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "  -d '{ \\\n",
        "     \"model\": \"gpt-4o-mini\", \\\n",
        "     \"messages\": [{\"role\": \"user\", \"content\": \"Say: This is a test!\"}], \\\n",
        "     \"temperature\": 0.7 \\\n",
        "    }' > test.json\n",
        "# Show the output of test.json formatted nicely\n",
        "!cat test.json | python -m json.tool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s47r5FtcOyLG",
        "outputId": "bcbbac2a-6768-4638-8098-5d4960df74e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   773  100   637  100   136    595    127  0:00:01  0:00:01 --:--:--   723\n",
            "{\n",
            "    \"id\": \"chatcmpl-APM8UarCUDDZ74saPyS5JP1XUGrVt\",\n",
            "    \"object\": \"chat.completion\",\n",
            "    \"created\": 1730606654,\n",
            "    \"model\": \"gpt-4o-mini-2024-07-18\",\n",
            "    \"choices\": [\n",
            "        {\n",
            "            \"index\": 0,\n",
            "            \"message\": {\n",
            "                \"role\": \"assistant\",\n",
            "                \"content\": \"This is a test!\",\n",
            "                \"refusal\": null\n",
            "            },\n",
            "            \"logprobs\": null,\n",
            "            \"finish_reason\": \"stop\"\n",
            "        }\n",
            "    ],\n",
            "    \"usage\": {\n",
            "        \"prompt_tokens\": 14,\n",
            "        \"completion_tokens\": 5,\n",
            "        \"total_tokens\": 19,\n",
            "        \"prompt_tokens_details\": {\n",
            "            \"cached_tokens\": 0\n",
            "        },\n",
            "        \"completion_tokens_details\": {\n",
            "            \"reasoning_tokens\": 0\n",
            "        }\n",
            "    },\n",
            "    \"system_fingerprint\": \"fp_0ba0d124f1\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the OpenAI Python client\n",
        "\n",
        "While the `curl` command shows us how the API call is made, it's not something that we can easily use in more complex applications.\n",
        "One thing we can use is the OpenAI python client.\n",
        "We can do the exact same thing, but the API call is a bit more abstracted from the developer.\n",
        "Let's see how to perform the exact same API call using the OpenAI clinet ðŸ‘‡:"
      ],
      "metadata": {
        "id": "DPRgn2_IP3w6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI # import OpenAI\n",
        "client = OpenAI() # Create the OpenAI client"
      ],
      "metadata": {
        "id": "fPRvearuIz86"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the messages (same as above)\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Say: This is a test!\"},\n",
        "]\n",
        "\n",
        "# Make the API call\n",
        "chat_completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=messages,\n",
        ")"
      ],
      "metadata": {
        "id": "vYbGuKC8I3na"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the output\n",
        "print(chat_completion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "2gd5AitgI9kU",
        "outputId": "d74a31a8-9066-488e-b973-3920bf2dfa62"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mChatCompletion\u001b[0m\u001b[1m(\u001b[0m\n",
              "    \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-APMY43Lrt1pBUyosVycuSaoNdqA6p'\u001b[0m,\n",
              "    \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\n",
              "        \u001b[1;35mChoice\u001b[0m\u001b[1m(\u001b[0m\n",
              "            \u001b[33mfinish_reason\u001b[0m=\u001b[32m'stop'\u001b[0m,\n",
              "            \u001b[33mindex\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
              "            \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "            \u001b[33mmessage\u001b[0m=\u001b[1;35mChatCompletionMessage\u001b[0m\u001b[1m(\u001b[0m\n",
              "                \u001b[33mcontent\u001b[0m=\u001b[32m'This is a test!'\u001b[0m,\n",
              "                \u001b[33mrefusal\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "                \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
              "                \u001b[33maudio\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "                \u001b[33mfunction_call\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "                \u001b[33mtool_calls\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
              "            \u001b[1m)\u001b[0m\n",
              "        \u001b[1m)\u001b[0m\n",
              "    \u001b[1m]\u001b[0m,\n",
              "    \u001b[33mcreated\u001b[0m=\u001b[1;36m1730608240\u001b[0m,\n",
              "    \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini-2024-07-18'\u001b[0m,\n",
              "    \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,\n",
              "    \u001b[33mservice_tier\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
              "    \u001b[33msystem_fingerprint\u001b[0m=\u001b[32m'fp_0ba0d124f1'\u001b[0m,\n",
              "    \u001b[33musage\u001b[0m=\u001b[1;35mCompletionUsage\u001b[0m\u001b[1m(\u001b[0m\n",
              "        \u001b[33mcompletion_tokens\u001b[0m=\u001b[1;36m5\u001b[0m,\n",
              "        \u001b[33mprompt_tokens\u001b[0m=\u001b[1;36m14\u001b[0m,\n",
              "        \u001b[33mtotal_tokens\u001b[0m=\u001b[1;36m19\u001b[0m,\n",
              "        \u001b[33mcompletion_tokens_details\u001b[0m=\u001b[1;35mCompletionTokensDetails\u001b[0m\u001b[1m(\u001b[0m\u001b[33maudio_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m, \u001b[33mreasoning_tokens\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m,\n",
              "        \u001b[33mprompt_tokens_details\u001b[0m=\u001b[1;35mPromptTokensDetails\u001b[0m\u001b[1m(\u001b[0m\u001b[33maudio_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m, \u001b[33mcached_tokens\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\n",
              "    \u001b[1m)\u001b[0m\n",
              "\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletion</span><span style=\"font-weight: bold\">(</span>\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-APMY43Lrt1pBUyosVycuSaoNdqA6p'</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
              "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Choice</span><span style=\"font-weight: bold\">(</span>\n",
              "            <span style=\"color: #808000; text-decoration-color: #808000\">finish_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
              "            <span style=\"color: #808000; text-decoration-color: #808000\">index</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
              "            <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "            <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionMessage</span><span style=\"font-weight: bold\">(</span>\n",
              "                <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'This is a test!'</span>,\n",
              "                <span style=\"color: #808000; text-decoration-color: #808000\">refusal</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "                <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
              "                <span style=\"color: #808000; text-decoration-color: #808000\">audio</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "                <span style=\"color: #808000; text-decoration-color: #808000\">function_call</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "                <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
              "            <span style=\"font-weight: bold\">)</span>\n",
              "        <span style=\"font-weight: bold\">)</span>\n",
              "    <span style=\"font-weight: bold\">]</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1730608240</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini-2024-07-18'</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">service_tier</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'fp_0ba0d124f1'</span>,\n",
              "    <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionUsage</span><span style=\"font-weight: bold\">(</span>\n",
              "        <span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,\n",
              "        <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>,\n",
              "        <span style=\"color: #808000; text-decoration-color: #808000\">total_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>,\n",
              "        <span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionTokensDetails</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">audio_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reasoning_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>,\n",
              "        <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTokensDetails</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">audio_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #808000; text-decoration-color: #808000\">cached_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>\n",
              "    <span style=\"font-weight: bold\">)</span>\n",
              "<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build our first chatbots\n",
        "\n",
        "In this section, we'll go through several demos.\n",
        "By the end of this section, you should be able to:\n",
        "- Build a basic chatbot with the popular `gradio` Python library\n",
        "- Understand key hyperparameters like `temperature`, `top_p`, and `top_k`\n",
        "- Build an advanced chatbot with hyperparameter controls"
      ],
      "metadata": {
        "id": "yJ0qYsS0Y4DZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a basic chatbot with `gradio`"
      ],
      "metadata": {
        "id": "JTQ-yAmFQVOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqqqq gradio"
      ],
      "metadata": {
        "id": "qOq34PZ-QjYO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "4_tJtXNMRHA6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(message, history):\n",
        "    if not history:\n",
        "        history = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant who speaks like a pirate.\"},\n",
        "        ]\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=history,\n",
        "        stream=True,\n",
        "    )\n",
        "    partial_message = \"\"\n",
        "    for chunk in response:\n",
        "        if chunk.choices[0].delta.content is not None:\n",
        "              partial_message = partial_message + chunk.choices[0].delta.content\n",
        "              yield partial_message"
      ],
      "metadata": {
        "id": "DL3d7elhRkj3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.ChatInterface(chat, type='messages').launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "zy5lPNy9R7ur",
        "outputId": "25f0bd46-47ca-4f8f-f2a8-2640c50f8ec0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://744a14ac940306dcab.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://744a14ac940306dcab.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7863 <> https://744a14ac940306dcab.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text generation hyperparameters"
      ],
      "metadata": {
        "id": "-bHAfnK9YizZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `temperature`"
      ],
      "metadata": {
        "id": "pibw6nM-YncC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `top_k`"
      ],
      "metadata": {
        "id": "WxkYh5ZkYrb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `top_p`"
      ],
      "metadata": {
        "id": "d-x9f33bYvPN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced chatbot with hyperparameter controls"
      ],
      "metadata": {
        "id": "ktB3q7OrYW72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def user(message:str, history:list, system_message:str):\n",
        "    if not history and system_message:\n",
        "        history.append(\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "        )\n",
        "    # Append the user's message to the conversation history\n",
        "    history.append({\"role\": \"user\", \"content\": message})\n",
        "    return \"\", history\n",
        "\n",
        "def bot(history, temperature, top_p):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=history,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        stream=True,\n",
        "    )\n",
        "    history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
        "    for chunk in response:\n",
        "        if chunk.choices[0].delta.content is not None:\n",
        "              history[-1]['content'] += chunk.choices[0].delta.content\n",
        "              yield history\n",
        "\n",
        "def clear():\n",
        "    return \"\", []"
      ],
      "metadata": {
        "id": "ozv16NX5WLzT"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            temperature = gr.Slider(minimum=0, maximum=1, value=0.7, label=\"Temperature\")\n",
        "            top_p = gr.Slider(minimum=0, maximum=1, value=1, label=\"Top p\")\n",
        "            system_message = gr.Textbox(label=\"System message\", value=\"You are a helpful assistant who speaks like a pirate.\")\n",
        "            clear_button = gr.Button(\"Clear messages\")\n",
        "        with gr.Column(scale=3):\n",
        "            chatbot = gr.Chatbot(type='messages')\n",
        "            input = gr.Textbox(label=\"Message\", value=\"\")\n",
        "            input.submit(user, [input, chatbot, system_message], [input, chatbot]).then(\n",
        "                bot, [chatbot, temperature, top_p], chatbot\n",
        "            )\n",
        "            clear_button.click(clear, [], [input, chatbot])\n",
        "\n",
        "demo.launch()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "S_qTM-_-UkG6",
        "outputId": "23c4960a-de48-4f90-f622-3d02cd81d24c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://63ee18136999bbd902.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://63ee18136999bbd902.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "3Uj86BPRVHLm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3EY-BXuHYfzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt engineering 1: few-shot promting"
      ],
      "metadata": {
        "id": "0vnpPQUOZpPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings and vector stores"
      ],
      "metadata": {
        "id": "tfedNl_UZyWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt engineering 2: dynamic few-shot prompting"
      ],
      "metadata": {
        "id": "whSrknBwZ1hA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cdM9igtyZ4ZA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}